{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336a6880",
   "metadata": {},
   "source": [
    "# Lecture 11 - Dimension Reduction for Data Visualization\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    ":depth: 1\n",
    "```\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Understand **unsupervised learning** vs **supervised learning** in chemistry.\n",
    "- Explain the intuition and math of **PCA** and read **loadings**, **scores**, and **explained variance**.\n",
    "- Use **t-SNE** and **UMAP** to embed high dimensional chemical features to 2D for visualization.\n",
    "\n",
    "\n",
    "[![Colab](https://img.shields.io/badge/Open-Colab-orange)](https://colab.research.google.com/drive/15nKl8LM8bkO7e4o4JjQ-hLQHxpW37kVh?usp=sharing)\n",
    "\n",
    "\n",
    "## 1. Setup and data\n",
    "\n",
    "We will reuse the C-H oxidation dataset and compute a small set of descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a37613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from umap-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from umap-learn) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from umap-learn) (1.7.1)\n",
      "Collecting numba>=0.51.2 (from umap-learn)\n",
      "  Downloading numba-0.62.0-cp313-cp313-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from umap-learn) (4.67.1)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.2->umap-learn)\n",
      "  Downloading llvmlite-0.45.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=1.6->umap-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->umap-learn) (0.4.6)\n",
      "Downloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "Downloading numba-0.62.0-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.8/2.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.45.0-cp313-cp313-win_amd64.whl (37.9 MB)\n",
      "   ---------------------------------------- 0.0/37.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.8/37.9 MB 8.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 4.2/37.9 MB 10.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.3/37.9 MB 12.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.3/37.9 MB 14.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 16.3/37.9 MB 16.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 21.2/37.9 MB 17.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 27.8/37.9 MB 19.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.6/37.9 MB 21.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 37.9/37.9 MB 21.2 MB/s eta 0:00:00\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: llvmlite, numba, pynndescent, umap-learn\n",
      "\n",
      "   ---------------------------------------- 0/4 [llvmlite]\n",
      "   ---------------------------------------- 0/4 [llvmlite]\n",
      "   ---------------------------------------- 0/4 [llvmlite]\n",
      "   ---------------------------------------- 0/4 [llvmlite]\n",
      "   ---------------------------------------- 0/4 [llvmlite]\n",
      "   ---------------------------------------- 0/4 [llvmlite]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   ---------- ----------------------------- 1/4 [numba]\n",
      "   -------------------- ------------------- 2/4 [pynndescent]\n",
      "   -------------------- ------------------- 2/4 [pynndescent]\n",
      "   ------------------------------ --------- 3/4 [umap-learn]\n",
      "   ---------------------------------------- 4/4 [umap-learn]\n",
      "\n",
      "Successfully installed llvmlite-0.45.0 numba-0.62.0 pynndescent-0.5.13 umap-learn-0.5.9.post2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, pairwise_distances\n",
    "\n",
    "try:\n",
    "  import umap.umap_ as umap\n",
    "except:\n",
    "  %pip install umap-learn\n",
    "  import umap.umap_ as umap\n",
    "\n",
    "# Utils\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors\n",
    "    RD = True\n",
    "except Exception:\n",
    "    try:\n",
    "      %pip install rdkit\n",
    "      from rdkit import Chem\n",
    "      from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors\n",
    "      RD = True\n",
    "    except:\n",
    "      RD = False\n",
    "      Chem = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/C_H_oxidation_dataset.csv\"\n",
    "df_raw = pd.read_csv(url)\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ffe37",
   "metadata": {},
   "source": [
    "We compute four quick 4 descriptors for everyone. Different from we we did previously, we only have another function compute 10 descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c938208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original function (4 descriptors)\n",
    "def calc_descriptors(smiles: str):\n",
    "    if smiles is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\": Crippen.MolLogP(m),\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(m),\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(m),\n",
    "    })\n",
    "\n",
    "\n",
    "# New function (10 descriptors)\n",
    "def calc_descriptors10(smiles: str):\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\": Crippen.MolLogP(m),\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(m),\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(m),\n",
    "        \"NumHAcceptors\": rdMolDescriptors.CalcNumHBA(m),\n",
    "        \"NumHDonors\": rdMolDescriptors.CalcNumHBD(m),\n",
    "        \"NumRotatableBonds\": rdMolDescriptors.CalcNumRotatableBonds(m),\n",
    "        \"HeavyAtomCount\": Descriptors.HeavyAtomCount(m),   # <-- fixed\n",
    "        \"FractionCSP3\": rdMolDescriptors.CalcFractionCSP3(m),\n",
    "        \"NumAromaticRings\": rdMolDescriptors.CalcNumAromaticRings(m)\n",
    "    })\n",
    "\n",
    "# Example usage (choose which function you want to apply)\n",
    "desc4 = df_raw[\"SMILES\"].apply(calc_descriptors)      # 4 descriptors\n",
    "desc10 = df_raw[\"SMILES\"].apply(calc_descriptors10)   # 10 descriptors\n",
    "\n",
    "df4 = pd.concat([df_raw, desc4], axis=1)\n",
    "df10 = pd.concat([df_raw, desc10], axis=1)\n",
    "\n",
    "print(\"Rows x Cols (4 desc):\", df4.shape)\n",
    "print(\"Rows x Cols (10 desc):\", df10.shape)\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df10.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f37477",
   "metadata": {},
   "source": [
    "Now we build a 64-bit Morgan fingerprint ($r=2$). We keep both a **small descriptor table** and a **high dimensional fingerprint** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee490ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit import DataStructs\n",
    "\n",
    "# --- Morgan fingerprint function (compact string) ---\n",
    "def morgan_bits(smiles: str, n_bits: int = 64, radius: int = 2):\n",
    "    if smiles is None:\n",
    "        return np.nan\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return np.nan\n",
    "\n",
    "    gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "    fp = gen.GetFingerprint(m)\n",
    "    arr = np.zeros((n_bits,), dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "\n",
    "    # join into one string of \"0101...\"\n",
    "    return \"\".join(map(str, arr))\n",
    "\n",
    "#1024bit\n",
    "df_morgan_1024 = df_raw.copy()\n",
    "df_morgan_1024[\"Fingerprint\"] = df_morgan_1024[\"SMILES\"].apply(lambda s: morgan_bits(s, n_bits=1024, radius=2))\n",
    "\n",
    "#64 bit\n",
    "df_morgan = df_raw.copy()\n",
    "df_morgan[\"Fingerprint\"] = df_morgan[\"SMILES\"].apply(lambda s: morgan_bits(s, n_bits=64, radius=2))\n",
    "\n",
    "\n",
    "print(\"Rows x Cols:\", df_morgan.shape)\n",
    "df_morgan.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f61b6",
   "metadata": {},
   "source": [
    "```{admonition} Data choices\n",
    "> - **X_small** uses 4 descriptors. Good for first PCA stories.\n",
    "> - **X_fp** has 64 bits. Good for t-SNE or UMAP since it is very high dimensional.\n",
    "```\n",
    "\n",
    "Now, let's compare their 4D, 10D descriptors with 64D fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# pick indices you want\n",
    "sample_indices = [1, 5, 7, 15, 20, 80, 100]\n",
    "\n",
    "# make sure you have all descriptor sets ready\n",
    "desc4 = df_raw[\"SMILES\"].apply(calc_descriptors)\n",
    "desc10 = df_raw[\"SMILES\"].apply(calc_descriptors10)\n",
    "df_all = pd.concat([df_raw, desc4, desc10.add_prefix(\"d10_\"), df_morgan[\"Fingerprint\"]], axis=1)\n",
    "\n",
    "# loop through chosen indices\n",
    "for idx in sample_indices:\n",
    "    row = df_all.iloc[idx]\n",
    "    mol = Chem.MolFromSmiles(row[\"SMILES\"])\n",
    "    img = Draw.MolToImage(mol, size=(200, 200), legend=row[\"Compound Name\"])\n",
    "    display(img)\n",
    "\n",
    "    print(f\"=== {row['Compound Name']} ===\")\n",
    "    print(\"SMILES:\", row[\"SMILES\"])\n",
    "\n",
    "    # 4-descriptor set\n",
    "    d4 = np.round([row[\"MolWt\"], row[\"LogP\"], row[\"TPSA\"], row[\"NumRings\"]], 2)\n",
    "    print(\"Descriptors [MolWt, LogP, TPSA, NumRings]:\", d4)\n",
    "\n",
    "    # 10-descriptor set\n",
    "    d10_keys = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"NumHAcceptors\",\"NumHDonors\",\n",
    "                \"NumRotatableBonds\",\"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]\n",
    "    d10 = np.round([row[\"d10_\"+k] for k in d10_keys], 2)\n",
    "    print(\"10-Descriptor Vector:\", d10)\n",
    "    print(\"Fingerprint:\", row[\"Fingerprint\"][:32])\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef0aa7",
   "metadata": {},
   "source": [
    "- The 4-descriptor table is easy to standardize and to interpret.\n",
    "- The 1024-bit fingerprint captures substructure presence or absence and is useful for neighborhood maps.\n",
    "\n",
    "With 4-desciptor, it's easier to visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40174646",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# pick specific indices\n",
    "df_sel = df_all.iloc[sample_indices]   # df_all includes your descriptors\n",
    "\n",
    "# --- 3D scatter plot ---\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "sc = ax.scatter(\n",
    "    df_sel[\"MolWt\"], df_sel[\"LogP\"], df_sel[\"TPSA\"],\n",
    "    c=df_sel[\"NumRings\"], cmap=\"viridis\", s=100\n",
    ")\n",
    "\n",
    "# add labels to points\n",
    "for i, row in df_sel.iterrows():\n",
    "    ax.text(\n",
    "        row[\"MolWt\"]+10 ,   # shift value so no overlap\n",
    "        row[\"LogP\"]-0.21,\n",
    "        row[\"TPSA\"]+4.5,\n",
    "        row[\"Compound Name\"],\n",
    "        ha=\"right\",   # align text to the right\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"MolWt\")\n",
    "ax.set_ylabel(\"LogP\")\n",
    "ax.set_zlabel(\"TPSA\")\n",
    "plt.colorbar(sc, label=\"NumRings\")\n",
    "plt.title(\"3D Descriptor Visualization (Selected Compounds)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d635b",
   "metadata": {},
   "source": [
    "Now we can apply this to the entire C-H oxidation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bded37f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "sc = ax.scatter(\n",
    "    df4[\"MolWt\"],\n",
    "    df4[\"LogP\"],\n",
    "    df4[\"TPSA\"],\n",
    "    c=df4[\"NumRings\"], cmap=\"viridis\", s=10, alpha=0.5\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"MolWt\")\n",
    "ax.set_ylabel(\"LogP\")\n",
    "ax.set_zlabel(\"TPSA\")\n",
    "\n",
    "plt.colorbar(sc, label=\"NumRings\")  # 4th dimension\n",
    "plt.title(\"4D Descriptor Visualization (MolWt, LogP, TPSA, NumRings)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327de9e",
   "metadata": {},
   "source": [
    "And depending which three descriptors you used for XYZ, you can have different plottings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00e4ba",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# All 4 descriptors\n",
    "descs = [\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "\n",
    "for i, color_dim in enumerate(descs, 1):\n",
    "    # the other three go on axes\n",
    "    axes_dims = [d for d in descs if d != color_dim]\n",
    "    xcol, ycol, zcol = axes_dims\n",
    "\n",
    "    ax = fig.add_subplot(1, 4, i, projection=\"3d\")\n",
    "\n",
    "    sc = ax.scatter(\n",
    "        df4[xcol], df4[ycol], df4[zcol],\n",
    "        c=df4[color_dim], cmap=\"cividis\", s=10, alpha=0.7\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(xcol)\n",
    "    ax.set_ylabel(ycol)\n",
    "    ax.set_zlabel(zcol)\n",
    "    ax.set_title(f\"Color = {color_dim}\")\n",
    "\n",
    "    # horizontal colorbar under each subplot\n",
    "    cbar = plt.colorbar(sc, ax=ax, orientation=\"horizontal\", pad=0.1, shrink=0.7)\n",
    "    cbar.set_label(color_dim)\n",
    "\n",
    "plt.suptitle(\"4D Descriptor Visualization (All Permutations)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447924d",
   "metadata": {},
   "source": [
    "But when it comes to 10-dimensional descriptors or even something like a 1024-dimensional Morgan fingerprint, it is nearly impossible to directly visualize them. It becomes even harder to know the \"neigbors\" to a specific molecule.\n",
    "\n",
    "Humans can only intuitively grasp two or three axes at once. Once you go beyond that, you can only look at pairs of dimensions at a time, and the relationships quickly become hard to characterize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b02ff",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vars = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "        \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "        \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(vars), 1, figsize=(6, 20), sharex=False)\n",
    "\n",
    "for i, v in enumerate(vars):\n",
    "    sns.scatterplot(\n",
    "        data=df10,\n",
    "        x=\"MolWt\", y=v,   # fix one descriptor on x-axis (e.g., MolWt)\n",
    "        ax=axes[i], s=20, alpha=0.7\n",
    "    )\n",
    "    axes[i].set_ylabel(v)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15dec5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# expand \"0101...\" strings into arrays of integers\n",
    "fp_matrix = df_morgan[\"Fingerprint\"].apply(lambda x: [int(ch) for ch in x]).tolist()\n",
    "fp_df = pd.DataFrame(fp_matrix, index=df_morgan[\"Compound Name\"])\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(fp_df, cmap=\"Greys\", cbar=False)\n",
    "plt.title(\"Morgan Fingerprint\")\n",
    "plt.xlabel(\"Bit Index\")\n",
    "plt.ylabel(\"Molecule\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ed033",
   "metadata": {},
   "source": [
    "That’s why cheminformatics usually applies **dimension reduction** methods (PCA, t-SNE, UMAP) to compress high-dimensional data into a visualizable 2D or 3D form. We will get to these points today!\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Supervised vs unsupervised\n",
    "\n",
    "In Lectures 6 to 8 we learned supervised models that map $x \\to y$ with labeled targets. Today we switch to **unsupervised learning**.\n",
    "\n",
    "- **Dimension reduction**: summarize high dimensional $x \\in \\mathbb{R}^p$ to a few coordinates $z \\in \\mathbb{R}^k$ with $k \\ll p$. You do not use $y$ during the fit.\n",
    "- **Clustering**: group samples into clusters based on a similarity or distance rule. Again no labels during the fit.\n",
    "\n",
    "We will color plots using toxicity when available. That is only to **interpret** the embedding or clusters. It is not used in the algorithms.\n",
    "\n",
    "```{admonition} ⏰ Exercise 2\n",
    "\n",
    "State whether each task is supervised or unsupervised:\n",
    "\n",
    "- Predict melting point from descriptors.\n",
    "- Group molecules by scaffold similarity.\n",
    "- Map 1024-bit fingerprints to 2D for plotting.\n",
    "- Predict toxicity from descriptors.\n",
    "```\n",
    "### 2.1 Principal Component Analysis (PCA)\n",
    "\n",
    "Feature choices will drive everything downstream. You can use:\n",
    "\n",
    "- Scalar descriptors like MolWt, LogP, TPSA, ring counts.\n",
    "- Fingerprints like Morgan bits or MACCS keys.\n",
    "- Learned embeddings from a graph neural network. We will not use those here, but the same methods apply.\n",
    "\n",
    "If two students use different features, their dimension reduction plots will look different. That is expected.\n",
    "\n",
    "> So, how do we start with dimension reduction?\n",
    "Let's first try **Principal Component Analysis (PCA)**\n",
    "In the previous section, when we move from **4 descriptors** (MolWt, LogP, TPSA, NumRings) to **10 descriptors** or even **64-bit Morgan fingerprints**, direct visualization becomes impossible. You can only plot a few dimensions at a time, and the relationships across all features become hard to interpret.\n",
    "\n",
    "PCA addresses this challenge by finding **new axes (principal components)** that summarize the directions of greatest variance in the data. These axes are:\n",
    "\n",
    "- **Orthogonal** (uncorrelated).\n",
    "- **Ranked by importance** (PC1 explains the most variance, PC2 the second most, and so on).\n",
    "- **Linear combinations** of the original features.\n",
    "\n",
    "Thus, PCA compresses high-dimensional molecular representations into just **2 or 3 coordinates** that can be plotted.\n",
    "\n",
    "\n",
    "\n",
    "Below is the mathematical pieces:\n",
    "\n",
    "1. Start with standardized data $\\tilde X$ (mean 0, variance 1 across features).\n",
    "2. Compute the covariance matrix:\n",
    "   $\n",
    "   S = \\frac{1}{n-1}\\tilde X^\\top \\tilde X\n",
    "   $\n",
    "3. Either:\n",
    "   - Eigen-decompose $S$ → eigenvectors are principal directions.\n",
    "   - Or compute SVD:\n",
    "     $\n",
    "     \\tilde X = U \\Sigma V^\\top\n",
    "     $\n",
    "4. The top $k$ eigenvectors (or columns of $V$) form the principal directions.\n",
    "5. The new coordinates are:\n",
    "   $\n",
    "   Z = \\tilde X V_k\n",
    "   $\n",
    "   where $Z$ are the principal component scores.\n",
    "\n",
    "---\n",
    "What this means for our chemistry dataset (`df10`)\n",
    "\n",
    "- `df10` contains **10 molecular descriptors** for each compound:\n",
    "  - MolWt, LogP, TPSA, NumRings,\n",
    "  - NumHAcceptors, NumHDonors,\n",
    "  - NumRotatableBonds, HeavyAtomCount,\n",
    "  - FractionCSP3, NumAromaticRings.\n",
    "\n",
    "- These form a data matrix $X$ of shape **(n molecules × 10 features)**.\n",
    "\n",
    "- PCA standardizes each column (feature) so they are comparable on the same scale.  \n",
    "  For example, MolWt ranges in the hundreds, but FractionCSP3 is between 0 and 1. Standardization makes both contribute fairly.\n",
    "\n",
    "- Then PCA computes **principal directions** as linear combinations of these 10 descriptors.  \n",
    "\n",
    "- The result is a new score matrix $Z$ of shape **(n molecules × k components)**, usually with $k=2$ or $3$ for visualization.  \n",
    "  This means each molecule is now represented by just 2–3 numbers (PC1, PC2, PC3) instead of 10 descriptors.\n",
    "\n",
    "- The same approach works for fingerprints: instead of 10 columns, you may have 64, 1024, or 2048. PCA reduces that down to 2–3 interpretable axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee19afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# ---- prepare matrices ----\n",
    "X_desc = df10[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "               \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "               \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]].values\n",
    "\n",
    "X_morgan = np.array([[int(ch) for ch in fp] for fp in df_morgan[\"Fingerprint\"]])  # 64 bits\n",
    "\n",
    "# ---- standardize ----\n",
    "X_desc_std = StandardScaler().fit_transform(X_desc)\n",
    "X_morgan_std = StandardScaler().fit_transform(X_morgan)\n",
    "\n",
    "# ---- PCA fit ----\n",
    "pca_desc = PCA().fit(X_desc_std)\n",
    "pca_morgan = PCA().fit(X_morgan_std)\n",
    "\n",
    "# ---- scree plots ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "axes[0].plot(np.cumsum(pca_desc.explained_variance_ratio_[:10]), marker=\"o\")\n",
    "axes[0].set_title(\"Explained Variance (10D descriptors)\")\n",
    "axes[0].set_xlabel(\"Number of PCs\")\n",
    "axes[0].set_ylabel(\"Cumulative variance\")\n",
    "\n",
    "axes[1].plot(np.cumsum(pca_morgan.explained_variance_ratio_[:20]), marker=\"o\")\n",
    "axes[1].set_title(\"Explained Variance (64D Morgan)\")\n",
    "axes[1].set_xlabel(\"Number of PCs\")\n",
    "axes[1].set_ylabel(\"Cumulative variance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 2D PCA projections ----\n",
    "desc_2d = pca_desc.transform(X_desc_std)[:, :2]\n",
    "morgan_2d = pca_morgan.transform(X_morgan_std)[:, :2]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "axes[0].scatter(desc_2d[:,0], desc_2d[:,1], c=\"blue\", alpha=0.7)\n",
    "axes[0].set_title(\"PCA 2D projection (10 descriptors)\")\n",
    "axes[0].set_xlabel(\"PC1\")\n",
    "axes[0].set_ylabel(\"PC2\")\n",
    "\n",
    "axes[1].scatter(morgan_2d[:,0], morgan_2d[:,1], c=\"green\", alpha=0.7)\n",
    "axes[1].set_title(\"PCA 2D projection (64-bit Morgan)\")\n",
    "axes[1].set_xlabel(\"PC1\")\n",
    "axes[1].set_ylabel(\"PC2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 3D PCA projections ----\n",
    "desc_3d = pca_desc.transform(X_desc_std)[:, :3]\n",
    "morgan_3d = pca_morgan.transform(X_morgan_std)[:, :3]\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.scatter(desc_3d[:,0], desc_3d[:,1], desc_3d[:,2],\n",
    "            c=\"blue\", alpha=0.7, s=50)\n",
    "ax1.set_title(\"PCA 3D projection (10 descriptors)\")\n",
    "ax1.set_xlabel(\"PC1\"); ax1.set_ylabel(\"PC2\"); ax1.set_zlabel(\"PC3\")\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.scatter(morgan_3d[:,0], morgan_3d[:,1], morgan_3d[:,2],\n",
    "            c=\"green\", alpha=0.7, s=50)\n",
    "ax2.set_title(\"PCA 3D projection (64-bit Morgan)\")\n",
    "ax2.set_xlabel(\"PC1\"); ax2.set_ylabel(\"PC2\"); ax2.set_zlabel(\"PC3\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba42196",
   "metadata": {},
   "source": [
    "Now you can see, the 10D vector is converted to 2D or 3D vector, which become much easier to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533df278",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccfb81a",
   "metadata": {},
   "source": [
    "In essence, PCA gives us coordinates for each molecule in a reduced 2D space.  \n",
    "To make this more concrete, we can select specific molecules and compare them:\n",
    "\n",
    "- A **manual pair** that we choose (indices 0 and 5).  \n",
    "- The **closest pair** in PCA space (most similar according to the descriptors).  \n",
    "- The **farthest pair** in PCA space (most dissimilar).  \n",
    "\n",
    "We plot all molecules in grey, highlight the pairs with different colors, and draw a line connecting each pair.  \n",
    "For the selected molecules we also show their **structure, compound name, and PCA coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87fcfa7",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "\n",
    "# ---- prepare feature matrix ----\n",
    "X_desc = df10[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "               \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "               \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]].values\n",
    "\n",
    "# ---- standardize and PCA ----\n",
    "X_desc_std = StandardScaler().fit_transform(X_desc)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_desc_std)\n",
    "\n",
    "# ---- distance matrix to find closest/farthest ----\n",
    "pairs = list(combinations(range(len(X_pca)), 2))\n",
    "distances = [np.linalg.norm(X_pca[i]-X_pca[j]) for i,j in pairs]\n",
    "farthest = pairs[np.argmax(distances)]\n",
    "closest = pairs[np.argmin(distances)]\n",
    "\n",
    "# ---- chosen pairs ----\n",
    "manual_pair = (225,35)\n",
    "pairs_to_show = [(\"Manual pair\", manual_pair),\n",
    "                 (\"Closest pair\", closest),\n",
    "                 (\"Farthest pair\", farthest)]\n",
    "\n",
    "# ---- scatter plot ----\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=\"grey\", alpha=0.4)\n",
    "\n",
    "colors = [\"red\",\"blue\",\"green\"]\n",
    "for (label,(i,j)),c in zip(pairs_to_show, colors):\n",
    "    plt.scatter(X_pca[i,0], X_pca[i,1], c=c, s=80, edgecolor=\"black\")\n",
    "    plt.scatter(X_pca[j,0], X_pca[j,1], c=c, s=80, edgecolor=\"black\")\n",
    "    plt.plot([X_pca[i,0],X_pca[j,0]],[X_pca[i,1],X_pca[j,1]],c=c,label=label)\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.title(\"PCA of 10 Descriptors with Selected Pairs\")\n",
    "plt.show()\n",
    "\n",
    "# ---- show structures + coordinates ----\n",
    "for label, (i,j) in pairs_to_show:\n",
    "    print(f\"=== {label} ===\")\n",
    "    for idx in (i,j):\n",
    "        name = df10.iloc[idx][\"Compound Name\"]\n",
    "        smiles = df10.iloc[idx][\"SMILES\"]\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        img = Draw.MolToImage(mol, size=(200,200), legend=name)\n",
    "        display(img)\n",
    "        print(f\"{name} (index {idx})\")\n",
    "        print(f\"  PC1 = {X_pca[idx,0]:.3f}, PC2 = {X_pca[idx,1]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989541f5",
   "metadata": {},
   "source": [
    "We can also make this plot to be interactive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5bd05",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ---- prepare feature matrix ----\n",
    "desc_cols = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "             \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "             \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]\n",
    "\n",
    "X_desc = df10[desc_cols].values\n",
    "\n",
    "# ---- standardize and PCA ----\n",
    "X_desc_std = StandardScaler().fit_transform(X_desc)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_desc_std)\n",
    "\n",
    "# ---- build dataframe with PCA scores ----\n",
    "df_pca = df10.copy()\n",
    "df_pca[\"PC1\"] = X_pca[:,0]\n",
    "df_pca[\"PC2\"] = X_pca[:,1]\n",
    "\n",
    "# ---- interactive scatter with custom size ----\n",
    "fig = px.scatter(\n",
    "    df_pca,\n",
    "    x=\"PC1\", y=\"PC2\",\n",
    "    color=\"NumRings\",\n",
    "    hover_data=[\"Compound Name\"] + desc_cols,\n",
    "    title=\"Interactive PCA of 10 Descriptors\",\n",
    "    width=1200, height=600   # <-- 12 wide, 6 tall\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d193e",
   "metadata": {},
   "source": [
    "### 2.2 Data leakage in unsupervised settings\n",
    "\n",
    "Even in unsupervised workflows, leakage can happen.  \n",
    "A common mistake is to **fit the scaler or PCA on the full dataset** before splitting.  \n",
    "This means the test set has already influenced the scaling or component directions, which is a subtle form of information leak.  \n",
    "\n",
    "**Safer pattern**:  \n",
    "1. Split the dataset into train and test.  \n",
    "2. Fit the scaler only on the training subset.  \n",
    "3. Transform both train and test using the fitted scaler.  \n",
    "4. Fit PCA (or clustering) on the training data only, then apply the transform to the test set.  \n",
    "\n",
    "This way the test set truly remains unseen during model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf7044",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- features ----\n",
    "desc_cols = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "             \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "             \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]\n",
    "\n",
    "X = df10[desc_cols].values\n",
    "\n",
    "# ---- split first ----\n",
    "X_train, X_test = train_test_split(X, test_size=0.3, random_state=42)\n",
    "\n",
    "# ---- fit scaler on train only ----\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# ---- fit PCA on train only ----\n",
    "pca = PCA(n_components=2).fit(X_train_std)\n",
    "X_train_pca = pca.transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "print(\"Train shape:\", X_train_pca.shape)\n",
    "print(\"Test shape:\", X_test_pca.shape)\n",
    "\n",
    "# ---- visualize ----\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c=\"purple\", label=\"Train\", alpha=0.6)\n",
    "plt.scatter(X_test_pca[:,0], X_test_pca[:,1], c=\"orange\", label=\"Test\", alpha=0.3, marker=\"o\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Leakage-safe PCA (Train vs Test)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b9fbe",
   "metadata": {},
   "source": [
    "If the test set overlaps reasonably with the distribution of the train set, that’s a sign your train-derived scaling and PCA directions generalize.\n",
    "\n",
    "If the test points lie far outside, that could mean:\n",
    "\n",
    "- Train and test distributions are very different.\n",
    "- Or PCA didn’t capture the structure of test molecules well.\n",
    "Note:\n",
    "\n",
    "- In pure visualization use cases, the risk is smaller since we are not optimizing a predictive model.\n",
    "- For cluster analysis that will be compared to labels afterward, keep the split discipline.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Standardization and distance\n",
    "\n",
    "Unsupervised methods depend on a distance or similarity rule. The choice of distance and the way you scale features can completely change neighborhoods, clusters, and embeddings. This section covers why we standardize, which distance to use for scalar descriptors vs fingerprints, and how to compute and visualize distance in a leakage safe way.\n",
    "\n",
    "\n",
    "### 3.1 Why standardize\n",
    "\n",
    "Descriptor columns live on different scales. `MolWt` can be in the hundreds. `FractionCSP3` is in `[0, 1]`. If you compute Euclidean distance on the raw matrix, large scale columns dominate. Standardization fixes this by centering and scaling each column so that all features contribute fairly.\n",
    "\n",
    "Common scalers:\n",
    "- **StandardScaler**: zero mean, unit variance. Good default.\n",
    "- **MinMaxScaler**: rescales to [0, 1]. Useful when you want bounded ranges.\n",
    "- **RobustScaler**: uses medians and IQR. Useful with outliers.\n",
    "\n",
    "\n",
    "\n",
    "Below is an example for `StandardScaler()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa58597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pick two columns\n",
    "cols2 = [\"MolWt\", \"TPSA\"]\n",
    "X2 = df10[cols2].to_numpy()\n",
    "\n",
    "# standardize both columns\n",
    "scaler = StandardScaler().fit(X2)\n",
    "X2_std = scaler.transform(X2)\n",
    "\n",
    "# show means and std before vs after\n",
    "print(\"Raw means:\", np.round(X2.mean(axis=0), 3))\n",
    "print(\"Raw stds :\", np.round(X2.std(axis=0, ddof=0), 3))\n",
    "print(\"Std means:\", np.round(X2_std.mean(axis=0), 3))\n",
    "print(\"Std stds :\", np.round(X2_std.std(axis=0, ddof=0), 3))\n",
    "\n",
    "# quick scatter before vs after\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "ax[0].scatter(X2[:,0], X2[:,1], s=10, alpha=0.6)\n",
    "ax[0].set_xlabel(\"MolWt\"); ax[0].set_ylabel(\"TPSA\"); ax[0].set_title(\"Raw\")\n",
    "\n",
    "ax[1].scatter(X2_std[:,0], X2_std[:,1], s=10, alpha=0.6)\n",
    "ax[1].set_xlabel(\"MolWt (z)\"); ax[1].set_ylabel(\"TPSA (z)\"); ax[1].set_title(\"Standardized\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7003b5",
   "metadata": {},
   "source": [
    "### 3.2 Which distance to use\n",
    "\n",
    "- **Scalar descriptors** (10D):\n",
    "  - **Euclidean** on standardized data is a good default.\n",
    "  - **Cosine** focuses on direction rather than magnitude. Try it when overall size varies a lot.\n",
    "- **Fingerprints** (64D or 1024D binary bits):\n",
    "  - **Tanimoto similarity** is the chem-informatics standard. Tanimoto distance = 1 − Tanimoto similarity.\n",
    "  - You can also compute cosine, but Tanimoto aligns with bitset overlap and is more common for substructure style features.\n",
    "\n",
    "\n",
    "Below are equations:\n",
    "\n",
    "- **Euclidean distance** between rows $x_i, x_j \\in \\mathbb{R}^d$  \n",
    "  $\n",
    "  d_{\\text{Euc}}(i,j)=\\left\\|x_i-x_j\\right\\|_2=\\sqrt{\\sum_{k=1}^{d}\\left(x_{ik}-x_{jk}\\right)^2}\n",
    "  $\n",
    "- **Cosine distance** (1 minus cosine similarity)  \n",
    "  $\n",
    "  d_{\\text{Cos}}(i,j)=1-\\frac{x_i^\\top x_j}{\\lVert x_i\\rVert_2\\,\\lVert x_j\\rVert_2}\n",
    "  $\n",
    "- **Tanimoto distance** for bit vectors $b_i,b_j\\in\\{0,1\\}^m$ with on-bit sets $A,B$  \n",
    "  $\n",
    "  s_{\\text{Tan}}(i,j)=\\frac{|A\\cap B|}{|A|+|B|-|A\\cap B|},\\qquad\n",
    "  d_{\\text{Tan}}(i,j)=1-s_{\\text{Tan}}(i,j)\n",
    "  $\n",
    "\n",
    "\n",
    "Below we show Euclidean vs Cosine on two standardized columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd943d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# two columns for a small, clear demo\n",
    "cols2 = [\"MolWt\", \"TPSA\"]\n",
    "X2 = df10[cols2].to_numpy()\n",
    "\n",
    "# standardize\n",
    "X2_std = StandardScaler().fit_transform(X2)\n",
    "\n",
    "# take a tiny subset to keep visuals readable\n",
    "Xsmall = X2_std[:30]\n",
    "\n",
    "# pairwise distances\n",
    "D_eu = pairwise_distances(Xsmall, metric=\"euclidean\")\n",
    "D_co = pairwise_distances(Xsmall, metric=\"cosine\")\n",
    "\n",
    "# heatmaps side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "im0 = ax[0].imshow(D_eu, aspect=\"auto\")\n",
    "ax[0].set_title(\"Euclidean distance\")\n",
    "ax[0].set_xlabel(\"sample\"); ax[0].set_ylabel(\"sample\")\n",
    "fig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im1 = ax[1].imshow(D_co, aspect=\"auto\")\n",
    "ax[1].set_title(\"Cosine distance\")\n",
    "ax[1].set_xlabel(\"sample\"); ax[1].set_ylabel(\"sample\")\n",
    "fig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# highlight how neighbors can change across metrics\n",
    "q = 10\n",
    "eu_nn = np.argsort(D_eu[q])[1:6]\n",
    "co_nn = np.argsort(D_co[q])[1:6]\n",
    "print(\"Euclidean Nearest Neighbor (rows) for sample #10:\", eu_nn.tolist())\n",
    "print(\"Cosine Nearest Neighbor (rows) for sample #10:\", co_nn.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8dcb1",
   "metadata": {},
   "source": [
    "```{admonition} Note\n",
    "Idea is that, the same query point can have different nearest neighbors under Euclidean vs Cosine because one metric cares about absolute offsets while the other cares about direction.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c86aa6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# take first 30 molecules to keep plot compact\n",
    "fp_strings = df_morgan[\"Fingerprint\"].iloc[:30].tolist()\n",
    "\n",
    "def fp_string_to_bitvect(fp_str):\n",
    "    arr = [int(ch) for ch in fp_str]\n",
    "    bv = DataStructs.ExplicitBitVect(len(arr))\n",
    "    for k, b in enumerate(arr):\n",
    "        if b: bv.SetBit(k)\n",
    "    return bv\n",
    "\n",
    "fps = [fp_string_to_bitvect(s) for s in fp_strings]\n",
    "\n",
    "# similarities and distances\n",
    "n = len(fps)\n",
    "S = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    S[i, :] = DataStructs.BulkTanimotoSimilarity(fps[i], fps)\n",
    "\n",
    "D_tan = 1.0 - S\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(5,4))\n",
    "im = plt.imshow(D_tan, aspect=\"auto\")\n",
    "plt.title(\"Tanimoto distance (Morgan bits)\")\n",
    "plt.xlabel(\"sample\"); plt.ylabel(\"sample\")\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# nearest neighbors under Tanimoto\n",
    "q = 10\n",
    "nn_tan = np.argsort(D_tan[q])[1:6]\n",
    "print(\"Tanimoto NN (rows) for query 10:\", nn_tan.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e15e85",
   "metadata": {},
   "source": [
    "```{admonition} Note\n",
    "Idea is that, small Tanimoto distance means strong bit overlap, which often reflects shared substructures. This is why Tanimoto is standard for fingerprint similarity.\n",
    "```\n",
    "Below we add a simple 2D plot to **show how nearest neighbors can differ** under different metrics in the same fingerprint space.  \n",
    "We compute a 2D embedding once (PCA on the 0/1 bit matrix) and then draw neighbor connections for all three kinds of distance calculation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d0fdc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from rdkit import DataStructs\n",
    "\n",
    "# --- build matrices on the FULL set ---\n",
    "fp_strings = df_morgan[\"Fingerprint\"].tolist()\n",
    "n = len(fp_strings)\n",
    "m = len(fp_strings[0])\n",
    "\n",
    "def fp_string_to_bitvect(fp_str):\n",
    "    arr = [int(ch) for ch in fp_str]\n",
    "    bv = DataStructs.ExplicitBitVect(len(arr))\n",
    "    for k, b in enumerate(arr):\n",
    "        if b: bv.SetBit(k)\n",
    "    return bv\n",
    "\n",
    "# RDKit bitvecs for Tanimoto\n",
    "fp_bvs = [fp_string_to_bitvect(s) for s in fp_strings]\n",
    "\n",
    "# 0/1 matrix for cosine and Euclidean\n",
    "Xfp = np.array([[int(ch) for ch in s] for s in fp_strings], dtype=float)  # shape (n, m)\n",
    "\n",
    "# --- pairwise distances in ORIGINAL space ---\n",
    "# Tanimoto\n",
    "S_tani = np.zeros((n, n), dtype=float)\n",
    "for i in range(n):\n",
    "    S_tani[i, :] = DataStructs.BulkTanimotoSimilarity(fp_bvs[i], fp_bvs)\n",
    "D_tani = 1.0 - S_tani\n",
    "\n",
    "# Cosine and Euclidean on 0/1 bits\n",
    "D_cos = pairwise_distances(Xfp, metric=\"cosine\")\n",
    "D_eu  = pairwise_distances(Xfp, metric=\"euclidean\")  # for binary, relates to Hamming\n",
    "\n",
    "# --- 2D coordinates for plotting ONLY ---\n",
    "Z = PCA(n_components=2).fit_transform(Xfp)\n",
    "\n",
    "# --- helper for stable top-k excluding self ---\n",
    "def topk_neighbors(D, q, k=5):\n",
    "    order = np.argsort(D[q], kind=\"mergesort\")  # stable\n",
    "    order = order[order != q]\n",
    "    return order[:k]\n",
    "\n",
    "q = 196   # your example index\n",
    "k = 5\n",
    "\n",
    "nn_tani = topk_neighbors(D_tani, q, k)\n",
    "nn_cos  = topk_neighbors(D_cos,  q, k)\n",
    "nn_eu   = topk_neighbors(D_eu,   q, k)\n",
    "\n",
    "print(\"Tanimoto NN for query\", q, \"->\", nn_tani.tolist())\n",
    "print(\"Cosine   NN for query\", q, \"->\", nn_cos.tolist())\n",
    "print(\"Euclidean NN for query\", q, \"->\", nn_eu.tolist())\n",
    "\n",
    "# --- three-panel plot: same coords, different neighbor sets ---\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for a in ax:\n",
    "    a.scatter(Z[:,0], Z[:,1], c=\"lightgrey\", s=16, alpha=0.6, label=\"All points\")\n",
    "    a.scatter(Z[q,0], Z[q,1], c=\"black\", s=80, edgecolor=\"white\", label=\"Query\")\n",
    "    a.set_xlabel(\"PC1\"); a.set_ylabel(\"PC2\")\n",
    "\n",
    "# panel 1: Tanimoto\n",
    "ax[0].scatter(Z[nn_tani,0], Z[nn_tani,1], c=\"tab:blue\", s=40, label=\"kNN\")\n",
    "for j in nn_tani:\n",
    "    ax[0].plot([Z[q,0], Z[j,0]], [Z[q,1], Z[j,1]], c=\"tab:blue\", alpha=0.8)\n",
    "ax[0].set_title(\"Neighbors by Tanimoto\")\n",
    "\n",
    "# panel 2: Cosine\n",
    "ax[1].scatter(Z[nn_cos,0], Z[nn_cos,1], c=\"tab:green\", s=40, label=\"kNN\")\n",
    "for j in nn_cos:\n",
    "    ax[1].plot([Z[q,0], Z[j,0]], [Z[q,1], Z[j,1]], c=\"tab:green\", alpha=0.8)\n",
    "ax[1].set_title(\"Neighbors by Cosine\")\n",
    "\n",
    "# panel 3: Euclidean\n",
    "ax[2].scatter(Z[nn_eu,0], Z[nn_eu,1], c=\"tab:red\", s=40, label=\"kNN\")\n",
    "for j in nn_eu:\n",
    "    ax[2].plot([Z[q,0], Z[j,0]], [Z[q,1], Z[j,1]], c=\"tab:red\", alpha=0.8)\n",
    "ax[2].set_title(\"Neighbors by Euclidean\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204042be",
   "metadata": {},
   "source": [
    "We can plot both the query molecule and the found neighbors to take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21704f7e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 1) Draw the query molecule\n",
    "q_name = df10.iloc[q][\"Compound Name\"]\n",
    "q_smiles = df10.iloc[q][\"SMILES\"]\n",
    "q_mol = Chem.MolFromSmiles(q_smiles)\n",
    "print(f\"Query #{q}: {q_name}\")\n",
    "display(Draw.MolToImage(q_mol, size=(250, 250), legend=f\"Query: {q_name}\"))\n",
    "\n",
    "# 2) Build a 5×3 grid of neighbor structures\n",
    "#    Row r: [Tanimoto_r, Cosine_r, Euclidean_r]\n",
    "mols = []\n",
    "legends = []\n",
    "\n",
    "def add_neighbors(metric_name, nn_idxs, dists):\n",
    "    col_mols, col_legs = [], []\n",
    "    for j in nn_idxs:\n",
    "        name = df10.iloc[j][\"Compound Name\"]\n",
    "        smi  = df10.iloc[j][\"SMILES\"]\n",
    "        mol  = Chem.MolFromSmiles(smi)\n",
    "        col_mols.append(mol)\n",
    "        col_legs.append(f\"{metric_name}\\n{name}\\nidx={j}  d={dists[q, j]:.3f}\")\n",
    "    return col_mols, col_legs\n",
    "\n",
    "# Prepare the three columns (5 neighbors each)\n",
    "m_tani, l_tani = add_neighbors(\"Tanimoto\", nn_tani, D_tani)\n",
    "m_cos,  l_cos  = add_neighbors(\"Cosine\",   nn_cos,  D_cos)\n",
    "m_eu,   l_eu   = add_neighbors(\"Euclidean\",nn_eu,   D_eu)\n",
    "\n",
    "# Interleave as rows: (Tani[r], Cos[r], Euclid[r]) for r=0..4\n",
    "for r in range(5):\n",
    "    mols.extend([m_tani[r], m_cos[r], m_eu[r]])\n",
    "    legends.extend([l_tani[r], l_cos[r], l_eu[r]])\n",
    "\n",
    "# Draw 5 rows × 3 columns (molsPerRow=3)\n",
    "grid = Draw.MolsToGridImage(\n",
    "    mols,\n",
    "    molsPerRow=3,\n",
    "    subImgSize=(220, 220),\n",
    "    legends=legends,\n",
    ")\n",
    "\n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdc29e",
   "metadata": {},
   "source": [
    "## 4. Nonlinear embeddings: t-SNE and UMAP\n",
    "\n",
    "Linear PCA gives a fast global summary. For curved manifolds or cluster-heavy data, **t-SNE** and **UMAP** often reveal structure that PCA compresses. Both start from a notion of neighborhood in the original feature space and then optimize a 2D or 3D layout that tries to keep neighbors close.\n",
    "\n",
    "We will use two feature sets:\n",
    "- **10D scalar descriptors** from `df10[desc_cols]`\n",
    "- **64-bit Morgan fingerprints** from `df_morgan[\"Fingerprint\"]`\n",
    "\n",
    "\n",
    "### 4.1 t-SNE\n",
    "\n",
    "**Idea:** Build probabilities that say which points are neighbors in high-dim, then find a low-dim map whose neighbor probabilities match.\n",
    "\n",
    "**Math:**\n",
    "- Convert distances to conditional probabilities with a Gaussian kernel per point:\n",
    "  $\n",
    "  p_{j\\mid i}=\\frac{\\exp\\!\\left(-\\frac{\\lVert x_i-x_j\\rVert^2}{2\\sigma_i^2}\\right)}{\\sum_{k\\ne i}\\exp\\!\\left(-\\frac{\\lVert x_i-x_k\\rVert^2}{2\\sigma_i^2}\\right)},\\quad p_{i\\mid i}=0\n",
    "  $\n",
    "  $\\sigma_i$ is chosen so that the **perplexity** matches a user target (roughly the effective number of neighbors).\n",
    "- Symmetrize:\n",
    "  $\n",
    "  P_{ij}=\\frac{p_{j\\mid i}+p_{i\\mid j}}{2n}\n",
    "  $\n",
    "- In 2D, use a heavy-tailed kernel to define\n",
    "  $\n",
    "  q_{ij}=\\frac{\\left(1+\\lVert y_i-y_j\\rVert^2\\right)^{-1}}{\\sum_{k\\ne \\ell}\\left(1+\\lVert y_k-y_\\ell\\rVert^2\\right)^{-1}},\\quad q_{ii}=0\n",
    "  $\n",
    "- Optimize by minimizing the Kullback–Leibler divergence\n",
    "  $\n",
    "  \\operatorname{KL}(P\\parallel Q)=\\sum_{i\\ne j}P_{ij}\\log\\frac{P_{ij}}{Q_{ij}}.\n",
    "  $/\n",
    "\n",
    "**Practical tips:**\n",
    "- Choose `perplexity` about 5 to 50. For ~500 molecules, start at 30.\n",
    "- Standardize descriptor matrices. Fingerprints are binary. You can feed them as 0/1, or first reduce to 50 PCs for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ----- features -----\n",
    "desc_cols = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "             \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "             \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]\n",
    "\n",
    "# 10D descriptors: standardize\n",
    "X_desc = df10[desc_cols].to_numpy()\n",
    "X_desc_std = StandardScaler().fit_transform(X_desc)\n",
    "\n",
    "# 64-bit Morgan: turn \"0101...\" into 0/1 matrix\n",
    "X_fp = np.array([[int(ch) for ch in s] for s in df_morgan[\"Fingerprint\"]], dtype=float)\n",
    "\n",
    "# optional: speed-up via PCA(50) before t-SNE on fingerprints\n",
    "#X_fp_50 = PCA(n_components=min(50, X_fp.shape[1], X_fp.shape[0]-1), random_state=0).fit_transform(X_fp)\n",
    "\n",
    "# ----- t-SNE fits -----\n",
    "tsne_desc = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\",\n",
    "                 init=\"pca\", n_iter=1000, random_state=0)\n",
    "Y_desc = tsne_desc.fit_transform(X_desc_std)\n",
    "\n",
    "tsne_fp = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\",\n",
    "               init=\"pca\", n_iter=1000, random_state=0)\n",
    "Y_fp = tsne_fp.fit_transform(X_fp)\n",
    "\n",
    "# ----- plots -----\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].scatter(Y_desc[:,0], Y_desc[:,1], s=18, alpha=0.8)\n",
    "ax[0].set_title(\"t-SNE on 10D descriptors\")\n",
    "ax[0].set_xlabel(\"tSNE-1\"); ax[0].set_ylabel(\"tSNE-2\")\n",
    "\n",
    "ax[1].scatter(Y_fp[:,0], Y_fp[:,1], s=18, alpha=0.8)\n",
    "ax[1].set_title(\"t-SNE on 64-bit Morgan\")\n",
    "ax[1].set_xlabel(\"tSNE-1\"); ax[1].set_ylabel(\"tSNE-2\")\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53caf2f2",
   "metadata": {},
   "source": [
    "### 4.2 UMAP\n",
    "\n",
    "**Idea:** Build a weighted kNN graph in the original space, interpret it as a fuzzy set of edges, then find low dimensional coordinates that preserve this fuzzy structure.\n",
    "\n",
    "**Math:**\n",
    "\n",
    "- For each point \\(i\\), choose a local connectivity scale so that exactly \\(k\\) neighbors have significant membership. Convert distances to directed fuzzy memberships $\\mu_{i\\to j}\\in[0,1]$.\n",
    "\n",
    "- Symmetrize with fuzzy union\n",
    "  $\n",
    "  \\mu_{ij} = \\mu_{i\\to j} + \\mu_{j\\to i} - \\mu_{i\\to j}\\mu_{j\\to i}\n",
    "  $\n",
    "\n",
    "- In low dimension, define a differentiable edge likelihood controlled by **spread** and **min\\_dist**. Optimize a cross entropy between the high dimensional and low dimensional fuzzy graphs with negative sampling.\n",
    "\n",
    "**Practice:**\n",
    "\n",
    "- `n_neighbors` sets the balance of local vs global. Small values focus on fine detail, larger values keep more global structure. Start at 15 to 50.\n",
    "- `min_dist` controls cluster tightness. Smaller values allow tighter blobs, larger values spread points.\n",
    "- UMAP supports `.transform`, so you can fit on train then place test without leakage. For fingerprints, `metric=\"jaccard\"` matches Tanimoto on 0 or 1 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894cbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP\n",
    "\n",
    "# ----- features -----\n",
    "desc_cols = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "             \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "             \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]\n",
    "X_desc = df10[desc_cols].to_numpy()\n",
    "\n",
    "X_fp = np.array([[int(ch) for ch in s] for s in df_morgan[\"Fingerprint\"]], dtype=float)\n",
    "\n",
    "# ----- split first (leakage safe) -----\n",
    "X_desc_tr, X_desc_te = train_test_split(X_desc, test_size=0.3, random_state=42)\n",
    "X_fp_tr,   X_fp_te   = train_test_split(X_fp,   test_size=0.3, random_state=42)\n",
    "\n",
    "# ----- standardize descriptors using train only -----\n",
    "scaler = StandardScaler().fit(X_desc_tr)\n",
    "X_desc_tr_std = scaler.transform(X_desc_tr)\n",
    "X_desc_te_std = scaler.transform(X_desc_te)\n",
    "\n",
    "# ----- UMAP fits (separate models per feature family) -----\n",
    "umap_desc = UMAP(n_neighbors=30, min_dist=0.1, metric=\"euclidean\", random_state=0)\n",
    "Z_desc_tr = umap_desc.fit_transform(X_desc_tr_std)\n",
    "Z_desc_te = umap_desc.transform(X_desc_te_std)  # out-of-sample\n",
    "\n",
    "umap_fp = UMAP(n_neighbors=30, min_dist=0.1, metric=\"cosine\", random_state=0)\n",
    "Z_fp_tr = umap_fp.fit_transform(X_fp_tr)\n",
    "Z_fp_te = umap_fp.transform(X_fp_te)\n",
    "\n",
    "# ----- plots -----\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "ax[0].scatter(Z_desc_tr[:,0], Z_desc_tr[:,1], s=14, c=\"tab:blue\", alpha=0.7, label=\"train\")\n",
    "ax[0].scatter(Z_desc_te[:,0], Z_desc_te[:,1], s=14, c=\"tab:red\", alpha=0.7, label=\"test\", marker=\"x\")\n",
    "ax[0].set_title(\"UMAP on 10D descriptors\")\n",
    "ax[0].set_xlabel(\"UMAP-1\"); ax[0].set_ylabel(\"UMAP-2\"); ax[0].legend()\n",
    "\n",
    "ax[1].scatter(Z_fp_tr[:,0], Z_fp_tr[:,1], s=14, c=\"tab:blue\", alpha=0.7, label=\"train\")\n",
    "ax[1].scatter(Z_fp_te[:,0], Z_fp_te[:,1], s=14, c=\"tab:red\", alpha=0.7, label=\"test\", marker=\"x\")\n",
    "ax[1].set_title(\"UMAP on 64-bit Morgan\")\n",
    "ax[1].set_xlabel(\"UMAP-1\"); ax[1].set_ylabel(\"UMAP-2\"); ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fa203",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP\n",
    "\n",
    "# -----------------------------\n",
    "# Data prep\n",
    "# -----------------------------\n",
    "desc_cols = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "             \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "             \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]\n",
    "X_desc = df10[desc_cols].to_numpy()\n",
    "\n",
    "# Morgan 64-bit (string of 0/1 per row)\n",
    "X_fp64 = np.array([[int(ch) for ch in s] for s in df_morgan[\"Fingerprint\"]], dtype=float)\n",
    "\n",
    "# Morgan 1024-bit\n",
    "X_fp1024 = np.array([[int(ch) for ch in s] for s in df_morgan_1024[\"Fingerprint\"]], dtype=float)\n",
    "\n",
    "# single split to keep row alignment across all feature sets\n",
    "n = X_desc.shape[0]\n",
    "idx = np.arange(n)\n",
    "idx_tr, idx_te = train_test_split(idx, test_size=0.30, random_state=0, shuffle=True)\n",
    "\n",
    "X_desc_tr,   X_desc_te   = X_desc[idx_tr],   X_desc[idx_te]\n",
    "X_fp64_tr,   X_fp64_te   = X_fp64[idx_tr],   X_fp64[idx_te]\n",
    "X_fp1024_tr, X_fp1024_te = X_fp1024[idx_tr], X_fp1024[idx_te]\n",
    "\n",
    "# scale descriptors using train only\n",
    "scaler = StandardScaler().fit(X_desc_tr)\n",
    "X_desc_tr_std = scaler.transform(X_desc_tr)\n",
    "X_desc_te_std = scaler.transform(X_desc_te)\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def tanimoto_distance(u, v):\n",
    "    # extended Tanimoto for real or binary vectors\n",
    "    uv = float(np.dot(u, v))\n",
    "    uu = float(np.dot(u, u))\n",
    "    vv = float(np.dot(v, v))\n",
    "    denom = uu + vv - uv\n",
    "    if denom == 0.0:\n",
    "        # both zero vectors -> distance 0\n",
    "        return 0.0\n",
    "    return 1.0 - (uv / denom)\n",
    "\n",
    "metric_map = {\n",
    "    \"Euclidean\": \"euclidean\",\n",
    "    \"Cosine\": \"cosine\",\n",
    "    \"Tanimoto\": tanimoto_distance,\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# UMAP and plotting\n",
    "# -----------------------------\n",
    "def fit_umap_and_transform(Xtr, Xte, metric, n_neighbors=30, min_dist=0.1, random_state=0):\n",
    "    reducer = UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    Ztr = reducer.fit_transform(Xtr)\n",
    "    Zte = reducer.transform(Xte)\n",
    "    return Ztr, Zte\n",
    "\n",
    "feature_sets = [\n",
    "    (\"10D descriptors\", X_desc_tr_std, X_desc_te_std),\n",
    "    (\"Morgan 64-bit\",   X_fp64_tr,     X_fp64_te),\n",
    "    (\"Morgan 1024-bit\", X_fp1024_tr,   X_fp1024_te),\n",
    "]\n",
    "\n",
    "metrics_order = [\"Euclidean\", \"Cosine\", \"Tanimoto\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "for i, (feat_name, Xtr, Xte) in enumerate(feature_sets):\n",
    "    for j, mname in enumerate(metrics_order):\n",
    "        metric = metric_map[mname]\n",
    "        Ztr, Zte = fit_umap_and_transform(Xtr, Xte, metric=metric)\n",
    "\n",
    "        ax = axes[i, j]\n",
    "        ax.scatter(Ztr[:, 0], Ztr[:, 1], s=12, c=\"tab:blue\", alpha=0.7, label=\"train\")\n",
    "        ax.scatter(Zte[:, 0], Zte[:, 1], s=12, c=\"tab:red\",  alpha=0.7, label=\"test\", marker=\"x\")\n",
    "        ax.set_title(f\"{feat_name} • {mname}\")\n",
    "        ax.set_xlabel(\"UMAP-1\")\n",
    "        ax.set_ylabel(\"UMAP-2\")\n",
    "        ax.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f7337",
   "metadata": {},
   "source": [
    "SO how these compare in chemistry?\n",
    "\n",
    "**In terms of global shape:**\n",
    "- PCA preserves coarse directions of variance and is easy to interpret through loadings.\n",
    "-UMAP can keep some global relations if `n_neighbors` is set higher.\n",
    "-t-SNE focuses on local neighborhoods and often distorts large scale distances.\n",
    "\n",
    "\n",
    "**For clusters:**\n",
    "- t-SNE and UMAP usually separate clusters more clearly than PCA.\n",
    "- UMAP tends to preserve relative distances between clusters better than t-SNE.\n",
    "\n",
    "\n",
    "**Workflow tips.**\n",
    "1. Decide the feature family. For scalar descriptors, standardize first. For fingerprints, keep bits and pick a metric that matches chemistry.\n",
    "2. If you plan any evaluation, split first. Fit scalers and embedding models on train only. Transform test afterward.\n",
    "3. Read maps with labels only for interpretation. Do not feed labels during fitting in unsupervised sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d4a73",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = StandardScaler().fit_transform(df10[desc_cols].to_numpy())\n",
    "\n",
    "Z_pca  = PCA(n_components=2, random_state=0).fit_transform(X)\n",
    "Z_tsne = TSNE(n_components=2, perplexity=30, init=\"pca\", learning_rate=\"auto\",\n",
    "              n_iter=1000, random_state=0).fit_transform(X)\n",
    "Z_umap = UMAP(n_neighbors=30, min_dist=0.1, metric=\"euclidean\", random_state=0).fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,4))\n",
    "ax[0].scatter(Z_pca[:,0],  Z_pca[:,1],  s=14, alpha=0.8); ax[0].set_title(\"PCA (2D)\")\n",
    "ax[1].scatter(Z_tsne[:,0], Z_tsne[:,1], s=14, alpha=0.8); ax[1].set_title(\"t-SNE (2D)\")\n",
    "ax[2].scatter(Z_umap[:,0], Z_umap[:,1], s=14, alpha=0.8); ax[2].set_title(\"UMAP (2D)\")\n",
    "for a in ax: a.set_xlabel(\"comp 1\"); a.set_ylabel(\"comp 2\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6a836",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Morgan bits -> 0/1 matrix\n",
    "X_fp = np.array([[int(ch) for ch in s] for s in df_morgan[\"Fingerprint\"]], dtype=float)\n",
    "\n",
    "# 2D embeddings\n",
    "Z_pca  = PCA(n_components=2, random_state=0).fit_transform(X_fp)\n",
    "Z_tsne = TSNE(n_components=2, perplexity=30, init=\"pca\", learning_rate=\"auto\",\n",
    "              n_iter=1000, random_state=0).fit_transform(X_fp)\n",
    "Z_umap = UMAP(n_neighbors=30, min_dist=0.1, metric=\"euclidean\", random_state=0).fit_transform(X_fp)\n",
    "\n",
    "# Plots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,4))\n",
    "ax[0].scatter(Z_pca[:,0],  Z_pca[:,1],  s=14, alpha=0.8); ax[0].set_title(\"PCA (Morgan)\")\n",
    "ax[1].scatter(Z_tsne[:,0], Z_tsne[:,1], s=14, alpha=0.8); ax[1].set_title(\"t-SNE (Morgan)\")\n",
    "ax[2].scatter(Z_umap[:,0], Z_umap[:,1], s=14, alpha=0.8); ax[2].set_title(\"UMAP (Morgan)\")\n",
    "for a in ax: a.set_xlabel(\"comp 1\"); a.set_ylabel(\"comp 2\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1adf9",
   "metadata": {},
   "source": [
    "Summary: \n",
    "PCA finds orthogonal directions of variance. t-SNE preserves neighbor probabilities and is great for tight clusters. UMAP preserves a fuzzy kNN graph and supports transforming new points. On molecules, try both t-SNE and UMAP on descriptors and fingerprints, then read the plots with domain knowledge rather than expecting one single correct picture.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 5. Glossary\n",
    "```{glossary}\n",
    "dimension reduction\n",
    "   Map $p$-dimensional features to a few coordinates that keep the most structure for your task.\n",
    "\n",
    "PCA\n",
    "  Linear method that finds orthogonal directions of maximum variance.\n",
    "\n",
    "loading\n",
    "  The weight of each original feature for a principal component.\n",
    "\n",
    "scree plot\n",
    "  Bar or line plot of explained variance ratio per PC.\n",
    "\n",
    "t-SNE\n",
    "  Nonlinear embedding that preserves local neighbor relations. Uses a KL divergence objective.\n",
    "\n",
    "UMAP\n",
    "  Graph-based embedding that models fuzzy set cross entropy on a kNN graph.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 6. In-class activity\n",
    "\n",
    "\n",
    "Scenario:\n",
    "You are screening 20 candidate bio-inhibitors from three chemotypes. Your goal is to build quick descriptor and fingerprint views, create 2D maps, and compare nearest neighbors under different distances before moving to assays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed 20 compounds with groups (3 clusters)\n",
    "mol_records = [\n",
    "    # Group A - hydrophobic aromatics\n",
    "    (\"Toluene\",               \"Cc1ccccc1\",                \"A_hydrophobic_aromatics\"),\n",
    "    (\"Ethylbenzene\",          \"CCc1ccccc1\",               \"A_hydrophobic_aromatics\"),\n",
    "    (\"Propylbenzene\",         \"CCCc1ccccc1\",              \"A_hydrophobic_aromatics\"),\n",
    "    (\"p-Xylene\",              \"Cc1ccc(cc1)C\",             \"A_hydrophobic_aromatics\"),\n",
    "    (\"Cumene\",                \"CC(C)c1ccccc1\",            \"A_hydrophobic_aromatics\"),\n",
    "    (\"Mesitylene\",            \"Cc1cc(C)cc(C)c1\",          \"A_hydrophobic_aromatics\"),\n",
    "    (\"Isobutylbenzene\",       \"CC(C)Cc1ccccc1\",           \"A_hydrophobic_aromatics\"),\n",
    "\n",
    "    # Group B - polar alcohols and acids\n",
    "    (\"Acetic acid\",           \"CC(=O)O\",                  \"B_polar_alc_acid\"),\n",
    "    (\"Propionic acid\",        \"CCC(=O)O\",                 \"B_polar_alc_acid\"),\n",
    "    (\"Butanoic acid\",         \"CCCC(=O)O\",                \"B_polar_alc_acid\"),\n",
    "    (\"Ethanol\",               \"CCO\",                      \"B_polar_alc_acid\"),\n",
    "    (\"1-Propanol\",            \"CCCO\",                     \"B_polar_alc_acid\"),\n",
    "    (\"1-Butanol\",             \"CCCCO\",                    \"B_polar_alc_acid\"),\n",
    "    (\"Ethylene glycol\",       \"OCCO\",                     \"B_polar_alc_acid\"),\n",
    "\n",
    "    # Group C - nitrogen hetero and amines\n",
    "    (\"Pyridine\",              \"n1ccccc1\",                 \"C_N_hetero_amines\"),\n",
    "    (\"Aniline\",               \"Nc1ccccc1\",                \"C_N_hetero_amines\"),\n",
    "    (\"Dimethylaniline\",       \"CN(C)c1ccccc1\",            \"C_N_hetero_amines\"),\n",
    "    (\"Imidazole\",             \"c1ncc[nH]1\",               \"C_N_hetero_amines\"),\n",
    "    (\"Morpholine\",            \"O1CCNCC1\",                 \"C_N_hetero_amines\"),\n",
    "    (\"Piperidine\",            \"N1CCCCC1\",                 \"C_N_hetero_amines\"),\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df_20 = pd.DataFrame(mol_records, columns=[\"Compound Name\",\"SMILES\",\"Group\"])\n",
    "df_20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e1464",
   "metadata": {},
   "source": [
    "### **Q1. Build feature sets and print compact summaries**\n",
    "\n",
    "1. Using `calc_descriptors10` we defined earlier today, create a 20×10 descriptor table `df_fp_20` for these molecules.\n",
    "2. Using `morgan_bits` with `n_bits=64` and `radius=2`, create a 64-bit fingerprint string for each molecule.\n",
    "3. Show the first 24 bits of the fingerprint for all 20 molecules.\n",
    "\n",
    "```python\n",
    "#TO DO\n",
    "```\n",
    "\n",
    "### **Q2. PCA on 10 descriptors**\n",
    "\n",
    "1. Build a feature matrix `X_desc` from the 10 descriptor columns, then standardize with `StandardScaler()`, call it `X_desc_std`.\n",
    "\n",
    "2. Fit `PCA()` on standardized data and project to 2D.\n",
    "\n",
    "3. Create a scatter. Optional: add text labels for each molecule.\n",
    "```python\n",
    "#TO DO\n",
    "```\n",
    "\n",
    "### **Q3. t-SNE on 10 descriptors**\n",
    "\n",
    "1. Use the same standardized 10D matrix `X_desc_std`.\n",
    "\n",
    "2. Fit t-SNE with `n_components=2`, `perplexity=5`, `init=\"pca\"`, `learning_rate=\"auto\"`, `n_iter=1000`, `random_state=0`.\n",
    "\n",
    "3. Make a scatter with text labels\n",
    "\n",
    "4. Change `perplexity` to `2`, `10`, and `15`, see how they are different from each other.\n",
    "```python\n",
    "#TO DO\n",
    "```\n",
    "\n",
    "### **Q4. PCA and t-SNE on Morgan-64 fingerprints**\n",
    "\n",
    "\n",
    "\n",
    "1. Convert each 64-bit fingerprint string to a row of 0 or 1 to form a 20×64 matrix ·X_fp·.\n",
    "\n",
    "2. Run PCA to 2D and plot with labels **and** t-SNE to 2D on `X_fp` with the same t-SNE settings as Q3, and plot with labels.\n",
    "\n",
    "3. Compare with Q3\n",
    "\n",
    "```python\n",
    "#TO DO\n",
    "```\n",
    "\n",
    "### **Q5. Nearest 5 neighbors - descriptors vs fingerprints**\n",
    "\n",
    "\n",
    "1. Pick a query molecule index `q` between `0` and `19`\n",
    "\n",
    "2. Compute a 20×20 Euclidean distance matrix on `X_desc_std` and list the 5 closest neighbors of `q` (exclude itself).\n",
    "\n",
    "3. Compute a 20×20 Tanimoto distance matrix for the fingerprints and list the 5 closest neighbors of `q`.\n",
    "\n",
    "4. Compare the two neighbor lists. Which list better respects group membership for this query?\n",
    "\n",
    "\n",
    "```python\n",
    "#TO DO\n",
    "```\n",
    "\n",
    "\n",
    "## 7. Solution\n",
    "\n",
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 - Solution\n",
    "\n",
    "# 10-descriptor table\n",
    "desc10_20 = df_20[\"SMILES\"].apply(calc_descriptors10)\n",
    "df10_20 = pd.concat([df_20[[\"Compound Name\",\"SMILES\",\"Group\"]], desc10_20], axis=1)\n",
    "\n",
    "# 64-bit Morgan fingerprints\n",
    "df_fp_20 = df_20.copy()\n",
    "df_fp_20[\"Fingerprint\"] = df_fp_20[\"SMILES\"].apply(lambda s: morgan_bits(s, n_bits=64, radius=2))\n",
    "\n",
    "print(\"10-descriptor table (head):\")\n",
    "display(df10_20.head().round(3))\n",
    "\n",
    "print(\"\\nFingerprint preview (first 24 bits):\")\n",
    "fp_preview = df_fp_20[[\"Compound Name\",\"Group\",\"Fingerprint\"]].copy()\n",
    "fp_preview[\"Fingerprint\"] = fp_preview[\"Fingerprint\"].str.slice(0, 24) + \"...\"\n",
    "display(fp_preview)\n",
    "\n",
    "print(\"\\nDescriptor stats:\")\n",
    "display(df10_20.drop(columns=[\"Compound Name\",\"SMILES\",\"Group\"]).agg([\"mean\",\"std\"]).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9d6d9",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - Solution\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "desc_cols = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\n",
    "             \"NumHAcceptors\",\"NumHDonors\",\"NumRotatableBonds\",\n",
    "             \"HeavyAtomCount\",\"FractionCSP3\",\"NumAromaticRings\"]\n",
    "\n",
    "X_desc = df10_20[desc_cols].to_numpy()\n",
    "X_desc_std = StandardScaler().fit_transform(X_desc)\n",
    "\n",
    "pca_desc_full = PCA().fit(X_desc_std)\n",
    "Z_desc = pca_desc_full.transform(X_desc_std)[:, :2]\n",
    "\n",
    "# color map by group\n",
    "group_to_color = {\"A_hydrophobic_aromatics\":\"tab:orange\",\n",
    "                  \"B_polar_alc_acid\":\"tab:blue\",\n",
    "                  \"C_N_hetero_amines\":\"tab:green\"}\n",
    "colors = df10_20[\"Group\"].map(group_to_color).to_numpy()\n",
    "\n",
    "# explained variance\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(np.cumsum(pca_desc_full.explained_variance_ratio_), marker=\"o\")\n",
    "plt.xlabel(\"Number of PCs\"); plt.ylabel(\"Cumulative EVR\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2D scatter with labels\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(Z_desc[:,0], Z_desc[:,1], s=80, c=colors, alpha=0.9)\n",
    "for i, name in enumerate(df10_20[\"Compound Name\"]):\n",
    "    plt.text(Z_desc[i,0]+0.03, Z_desc[i,1], name, fontsize=8)\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.title(\"PCA on 10 descriptors (n=20)\")\n",
    "# legend\n",
    "for g, col in group_to_color.items():\n",
    "    plt.scatter([], [], c=col, label=g)\n",
    "plt.legend(title=\"Group\", loc=\"best\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6a689",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 - Solution\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_desc = TSNE(n_components=2, perplexity=2, learning_rate=\"auto\",\n",
    "                 init=\"pca\", n_iter=1000, random_state=0)\n",
    "Y_desc = tsne_desc.fit_transform(X_desc_std)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(Y_desc[:,0], Y_desc[:,1], s=80, c=colors, alpha=0.9)\n",
    "for i, name in enumerate(df10_20[\"Compound Name\"]):\n",
    "    plt.text(Y_desc[i,0]+0.5, Y_desc[i,1], name, fontsize=8)\n",
    "plt.xlabel(\"tSNE-1\"); plt.ylabel(\"tSNE-2\"); plt.title(\"t-SNE on 10 descriptors (n=20)\")\n",
    "for g, col in group_to_color.items():\n",
    "    plt.scatter([], [], c=col, label=g)\n",
    "plt.legend(title=\"Group\", loc=\"best\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8545d0c",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587efa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 - Solution\n",
    "\n",
    "# 0/1 matrix from bitstrings\n",
    "X_fp = np.array([[int(ch) for ch in s] for s in df_fp_20[\"Fingerprint\"]], dtype=float)\n",
    "\n",
    "# PCA on fingerprint bits\n",
    "pca_fp = PCA(n_components=2, random_state=0).fit(X_fp)\n",
    "Z_fp_pca = pca_fp.transform(X_fp)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(Z_fp_pca[:,0], Z_fp_pca[:,1], s=80, c=colors, alpha=0.9)\n",
    "for i, name in enumerate(df_fp_20[\"Compound Name\"]):\n",
    "    plt.text(Z_fp_pca[i,0]+0.03, Z_fp_pca[i,1], name, fontsize=8)\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.title(\"PCA on Morgan-64 (n=20)\")\n",
    "for g, col in group_to_color.items():\n",
    "    plt.scatter([], [], c=col, label=g)\n",
    "plt.legend(title=\"Group\", loc=\"best\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# t-SNE on fingerprint bits\n",
    "tsne_fp = TSNE(n_components=2, perplexity=5, learning_rate=\"auto\",\n",
    "               init=\"pca\", n_iter=1000, random_state=0)\n",
    "Y_fp = tsne_fp.fit_transform(X_fp)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(Y_fp[:,0], Y_fp[:,1], s=80, c=colors, alpha=0.9)\n",
    "for i, name in enumerate(df_fp_20[\"Compound Name\"]):\n",
    "    plt.text(Y_fp[i,0]+0.5, Y_fp[i,1], name, fontsize=8)\n",
    "plt.xlabel(\"tSNE-1\"); plt.ylabel(\"tSNE-2\"); plt.title(\"t-SNE on Morgan-64 (n=20)\")\n",
    "for g, col in group_to_color.items():\n",
    "    plt.scatter([], [], c=col, label=g)\n",
    "plt.legend(title=\"Group\", loc=\"best\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33dd320",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 - Solution\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from rdkit import DataStructs\n",
    "\n",
    "# choose a query row\n",
    "q = 3  # change during class to test different queries\n",
    "\n",
    "# 1) Euclidean on standardized 10D descriptors\n",
    "D_eu = pairwise_distances(X_desc_std, metric=\"euclidean\")\n",
    "order_eu = np.argsort(D_eu[q])\n",
    "nn_eu_idx = [i for i in order_eu if i != q][:5]\n",
    "\n",
    "print(f\"Query: {df10_20.loc[q,'Compound Name']}  |  Group: {df10_20.loc[q,'Group']}\")\n",
    "print(\"\\nNearest 5 by Euclidean (10 descriptors):\")\n",
    "for j in nn_eu_idx:\n",
    "    print(f\"  {df10_20.loc[j,'Compound Name']:24s}  group={df10_20.loc[j,'Group']:22s}  d_eu={D_eu[q,j]:.3f}\")\n",
    "\n",
    "# 2) Tanimoto on 64-bit fingerprints\n",
    "def fp_string_to_bvect(fp_str):\n",
    "    arr = [int(ch) for ch in fp_str]\n",
    "    bv = DataStructs.ExplicitBitVect(len(arr))\n",
    "    for k, b in enumerate(arr):\n",
    "        if b: bv.SetBit(k)\n",
    "    return bv\n",
    "\n",
    "fps = [fp_string_to_bvect(s) for s in df_fp_20[\"Fingerprint\"]]\n",
    "S_tan = np.zeros((len(fps), len(fps)))\n",
    "for i in range(len(fps)):\n",
    "    S_tan[i, :] = DataStructs.BulkTanimotoSimilarity(fps[i], fps)\n",
    "D_tan = 1.0 - S_tan\n",
    "\n",
    "order_tan = np.argsort(D_tan[q])\n",
    "nn_tan_idx = [i for i in order_tan if i != q][:5]\n",
    "\n",
    "print(\"\\nNearest 5 by Tanimoto (Morgan-64):\")\n",
    "for j in nn_tan_idx:\n",
    "    print(f\"  {df10_20.loc[j,'Compound Name']:24s}  group={df10_20.loc[j,'Group']:22s}  d_tan={D_tan[q,j]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "jupytext_version": "1.16.4",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "source_map": [
   12,
   34,
   72,
   76,
   79,
   124,
   126,
   130,
   164,
   171,
   208,
   217,
   251,
   253,
   273,
   276,
   311,
   318,
   341,
   353,
   443,
   518,
   523,
   527,
   529,
   543,
   600,
   603,
   637,
   656,
   695,
   727,
   752,
   784,
   825,
   831,
   866,
   874,
   960,
   963,
   1007,
   1046,
   1084,
   1106,
   1156,
   1251,
   1269,
   1293,
   1311,
   1352,
   1386,
   1458,
   1480,
   1483,
   1524,
   1528,
   1547,
   1550,
   1585,
   1589
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}