{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f727f599",
   "metadata": {},
   "source": [
    "# Lecture 12 - Self-Supervised Learning\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    ":depth: 1\n",
    "```\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Build **clustering** workflows: pick features, scale, fit, visualize.\n",
    "- Choose and justify distance metrics for descriptors vs fingerprints.\n",
    "- Select k and model type using elbow and silhouette.\n",
    "\n",
    "[![Colab](https://img.shields.io/badge/Open-Colab-orange)](https://colab.research.google.com/drive/1CGznPlVhSet10f820k7TyPvk3kcBdHkC?usp=sharing)\n",
    "\n",
    "\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ae1a63",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# RDKit\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Draw, Descriptors, Crippen, rdMolDescriptors, AllChem, rdFingerprintGenerator, DataStructs\n",
    "    RD = True\n",
    "except Exception:\n",
    "    try:\n",
    "        %pip install rdkit\n",
    "        from rdkit import Chem\n",
    "        from rdkit.Chem import Draw, Descriptors, Crippen, rdMolDescriptors, AllChem, rdFingerprintGenerator, DataStructs\n",
    "        RD = True\n",
    "    except Exception as e:\n",
    "        print(\"RDKit is not available in this environment. Drawing and descriptors will be skipped.\")\n",
    "        RD = False\n",
    "        Chem = None\n",
    "\n",
    "# UMAP install guard\n",
    "try:\n",
    "    import umap\n",
    "    from umap import UMAP\n",
    "    HAVE_UMAP = True\n",
    "except Exception:\n",
    "    HAVE_UMAP = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca11548",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Similar to we we did before, let's first make small helper to compute 4 quick descriptors and a compact fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_desc4(smiles: str):\n",
    "    if not RD:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\": Crippen.MolLogP(m),\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(m),\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(m),\n",
    "    })\n",
    "\n",
    "def morgan_bits(smiles: str, n_bits: int = 128, radius: int = 2):\n",
    "    if not RD:\n",
    "        return np.zeros(n_bits, dtype=int)\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return np.zeros(n_bits, dtype=int)\n",
    "    gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "    fp = gen.GetFingerprint(m)\n",
    "    arr = np.zeros((n_bits,), dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f4474",
   "metadata": {},
   "source": [
    "Load the same C-H oxidation dataset used in Lectures 7 and 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/C_H_oxidation_dataset.csv\"\n",
    "df_raw = pd.read_csv(url)\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162968e",
   "metadata": {},
   "source": [
    "Compute features we will use today and keep the `Reactivity` column for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df_raw[\"SMILES\"].apply(calc_desc4)\n",
    "df = pd.concat([df_raw, desc], axis=1)\n",
    "\n",
    "# fingerprint matrix as 0 or 1\n",
    "FP_BITS = 128\n",
    "fp_mat = np.vstack(df_raw[\"SMILES\"].apply(lambda s: morgan_bits(s, n_bits=FP_BITS, radius=2)).values)\n",
    "\n",
    "# a tidy feature table that has descriptors and the label of interest\n",
    "cols_x = [\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]\n",
    "keep = [\"Compound Name\", \"SMILES\", \"Reactivity\"] + cols_x\n",
    "frame = pd.concat([df[keep].reset_index(drop=True), pd.DataFrame(fp_mat, columns=[f\"fp_{i}\" for i in range(FP_BITS)])], axis=1)\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fc1ea",
   "metadata": {},
   "source": [
    "We will standardize the small descriptor block to avoid scale dominance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(frame[cols_x])\n",
    "X_desc = scaler.transform(frame[cols_x])          # shape: n x 4\n",
    "X_fp   = frame[[c for c in frame.columns if c.startswith(\"fp_\")]].to_numpy().astype(float)  # n x 128\n",
    "y_reac = frame[\"Reactivity\"].astype(str)          # strings such as \"low\", \"medium\", \"high\" if available\n",
    "X_desc[:2], X_fp.shape, y_reac.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dd33a1",
   "metadata": {},
   "source": [
    "In Lecture 11 we mapped high dimensional features to 2D for plots using PCA, t-SNE, and UMAP. Today we take the next step: form clusters that group similar molecules together. We will start simple and add checks.\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "- Clustering uses only $X$. No label $y$ during fit.\n",
    "- Distance matters. For descriptors we use Euclidean on standardized columns. For binary fingerprints many chemists prefer Tanimoto similarity, with distance $d_{\\text{tan}} = 1 - s_{\\text{tan}}$ where\n",
    "  $\n",
    "  s_{\\text{tan}}(i,j) = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}.\n",
    "  $\n",
    "\n",
    "First let's do a quick 2D descriptor map to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=0).fit(X_desc)\n",
    "Z = pca.transform(X_desc)\n",
    "\n",
    "plt.scatter(Z[:,0], Z[:,1], s=12, alpha=0.7)\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA on 4 descriptors - preview\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1d648",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. KMeans on clustering\n",
    "\n",
    "We start with **KMeans** because it is one of the simplest and most widely used clustering algorithms. It gives us a baseline understanding of how groups may form in our dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Fit KMeans for a single $k$\n",
    "\n",
    "In clustering, $k$ is the number of groups we want to divide our data into.  \n",
    "KMeans works by alternating between two steps:\n",
    "\n",
    "1. **Assignment step**  \n",
    "   Each data point $x_i$ is assigned to the nearest cluster center $c_j$ according to Euclidean distance:  \n",
    "\n",
    "   $\n",
    "   \\text{assign}(x_i) = \\arg \\min_j \\| x_i - c_j \\|^2\n",
    "   $\n",
    "\n",
    "2. **Update step**  \n",
    "   Each cluster center $c_j$ is updated to be the mean of all points assigned to it:  \n",
    "\n",
    "   $\n",
    "   c_j = \\frac{1}{|S_j|} \\sum_{x_i \\in S_j} x_i\n",
    "   $\n",
    "\n",
    "The process repeats until the centers stabilize or a maximum number of iterations is reached.\n",
    "\n",
    "The optimization goal is to minimize the **within-cluster sum of squares** (WCSS):\n",
    "\n",
    "$\n",
    "\\min_{c_1, \\dots, c_k} \\sum_{j=1}^k \\sum_{x_i \\in S_j} \\| x_i - c_j \\|^2\n",
    "$\n",
    "\n",
    "\n",
    "Our descriptor space has 4 dimensions. To plot, we project the standardized descriptors into **2D PCA space**.  \n",
    "This does not affect clustering (which is run in the original scaled space), but helps us visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=0, n_init=10).fit(X_desc)\n",
    "labels_km = kmeans.labels_\n",
    "\n",
    "pd.Series(labels_km).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece8f71",
   "metadata": {},
   "source": [
    "Map clusters on our PCA plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "for lab in np.unique(labels_km):\n",
    "    idx = labels_km == lab\n",
    "    plt.scatter(Z[idx,0], Z[idx,1], s=14, alpha=0.8, label=f\"cluster {lab}\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.title(\"KMeans clusters (k=3) on PCA(Descriptors)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d59dc0",
   "metadata": {},
   "source": [
    "```{admonition} ⏰ **Exercise**\n",
    "Try `k=2` and `k=4`.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Peek at cluster centers in the original descriptor space. We inverse transform to original units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_std = kmeans.cluster_centers_                           # in z space\n",
    "centroids_orig = scaler.inverse_transform(centroids_std)          # back to original units\n",
    "cent_tab = pd.DataFrame(centroids_orig, columns=cols_x)\n",
    "cent_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed2e67",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot KMeans clusters with centroids as stars\n",
    "plt.figure(figsize=(5,4))\n",
    "for c in np.unique(labels_km):\n",
    "    plt.scatter(Z[labels_km==c,0], Z[labels_km==c,1], s=14, alpha=0.7, label=f\"c{c}\")\n",
    "cent_pca = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(cent_pca[:,0], cent_pca[:,1], s=500, marker=\"*\", c=\"red\", edgecolor=\"w\", label=\"centroid\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.title(\"KMeans clusters with centroids\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1604a5",
   "metadata": {},
   "source": [
    "### 3.2 What does a single sample look like\n",
    "\n",
    "Sometimes it helps to print one row to see the scaled numbers and the assigned cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7\n",
    "print(\"Compound:\", frame.loc[i, \"Compound Name\"])\n",
    "print(\"Original desc:\", frame.loc[i, cols_x].to_dict())\n",
    "print(\"Scaled desc:\", dict(zip(cols_x, np.round(X_desc[i], 3))))\n",
    "print(\"Cluster:\", labels_km[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1070e7",
   "metadata": {},
   "source": [
    "### 3.3 KMeans on t-SNE and UMAP\n",
    "Let's also take a look at how kmeans can be applied to t-SNE and UMAP we learned during the last lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def embed_descriptors(X, use_umap=True, random_state=0):\n",
    "    if use_umap and HAVE_UMAP:\n",
    "        reducer = UMAP(n_neighbors=15, min_dist=0.10, metric=\"euclidean\", random_state=random_state)\n",
    "        Z = reducer.fit_transform(X)\n",
    "        name = \"UMAP(desc)\"\n",
    "    else:\n",
    "        reducer = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\",\n",
    "                       init=\"pca\", metric=\"euclidean\", random_state=random_state)\n",
    "        Z = reducer.fit_transform(X)\n",
    "        name = \"t-SNE(desc)\"\n",
    "    return Z, name\n",
    "\n",
    "# 1) Embed descriptors\n",
    "Z_desc, name_desc = embed_descriptors(X_desc, use_umap=True, random_state=0)\n",
    "\n",
    "# 2) KMeans on the 2D embedding\n",
    "k = 3\n",
    "km_desc = KMeans(n_clusters=k, random_state=0, n_init=10).fit(Z_desc)\n",
    "labs_desc = km_desc.labels_\n",
    "\n",
    "# 3) Plot clusters\n",
    "plt.figure(figsize=(5,4))\n",
    "for c in np.unique(labs_desc):\n",
    "    idx = labs_desc == c\n",
    "    plt.scatter(Z_desc[idx,0], Z_desc[idx,1], s=14, alpha=0.9, label=f\"c{c}\")\n",
    "plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
    "plt.title(f\"{name_desc} + KMeans(k={k}) on descriptors\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae617e31",
   "metadata": {},
   "source": [
    "```{admonition} ⏰ **Exercise**\n",
    "\n",
    "Change `use_umap = True` to `False` and see the change by using t-sne.\n",
    "\n",
    "Also try to use k = `2`, `3`, and `4`.\n",
    "\n",
    "Run the cell below to see the difference.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5d6d8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def pick_two_per_cluster(labels):\n",
    "    picks = {}\n",
    "    for c in np.unique(labels):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        picks[c] = idx[:2] if len(idx) >= 2 else idx\n",
    "    return picks\n",
    "\n",
    "def show_molecules(indices, smiles_col=\"SMILES\", names_col=\"Compound Name\"):\n",
    "    if not RD:\n",
    "        print(\"RDKit not available, cannot draw molecules.\")\n",
    "        return\n",
    "    mols, legends = [], []\n",
    "    for i in indices:\n",
    "        smi = frame.loc[i, smiles_col]\n",
    "        m = Chem.MolFromSmiles(smi) if smi is not None else None\n",
    "        mols.append(m)\n",
    "        legends.append(f\"{frame.loc[i, names_col]}  (idx {i})\")\n",
    "    img = Draw.MolsToGridImage(mols, molsPerRow=2, subImgSize=(250,250), legends=legends, returnPNG=True)\n",
    "    display(img)\n",
    "\n",
    "# 1) pick two per cluster\n",
    "picks_desc = pick_two_per_cluster(labs_desc)\n",
    "\n",
    "# 2) draw molecules for each cluster\n",
    "for c, idxs in picks_desc.items():\n",
    "    print(f\"\\nDescriptors {name_desc} cluster c{c}: showing up to 2 molecules\")\n",
    "    show_molecules(idxs)\n",
    "\n",
    "# 3) star their positions on the existing embedding\n",
    "plt.figure(figsize=(5,4))\n",
    "for c in np.unique(labs_desc):\n",
    "    idx = labs_desc == c\n",
    "    plt.scatter(Z_desc[idx,0], Z_desc[idx,1], s=14, alpha=0.7, label=f\"c{c}\")\n",
    "\n",
    "for c, idxs in picks_desc.items():\n",
    "    if len(idxs) == 0: \n",
    "        continue\n",
    "    plt.scatter(Z_desc[idxs,0], Z_desc[idxs,1], s=140, marker=\"*\", edgecolor=\"k\", linewidth=0.7, label=f\"c{c} picks\")\n",
    "\n",
    "plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
    "plt.title(f\"{name_desc} + KMeans(k=3) on descriptors with picked molecules\")\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564bcd92",
   "metadata": {},
   "source": [
    "Instead of using descritors, we also try using fingerprints.\n",
    "\n",
    "Different from **Tanimoto similarity** we used before, this time we demonstrated **Jaccard similarity**. For binary molecular fingerprints (0/1 bits), **Jaccard** and **Tanimoto** are mathematically the same.\n",
    "\n",
    "**Jaccard similarity**:  \n",
    "$$\n",
    "s_J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "**Tanimoto similarity**:  \n",
    "$$\n",
    "s_T(A, B) = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}\n",
    "$$\n",
    "\n",
    "Since $|A \\cup B| = |A| + |B| - |A \\cap B|$, we have $s_J = s_T$ for binary fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_fingerprints(X_bits_bool, use_umap=True, random_state=0):\n",
    "    if use_umap and HAVE_UMAP:\n",
    "        reducer = UMAP(n_neighbors=15, min_dist=0.10, metric=\"jaccard\", random_state=random_state)\n",
    "        Z = reducer.fit_transform(X_bits_bool)\n",
    "        name = \"UMAP(fp, Jaccard)\"\n",
    "    else:\n",
    "        D_jac = pairwise_distances(X_bits_bool, metric=\"jaccard\")\n",
    "        reducer = TSNE(n_components=2, metric=\"precomputed\", perplexity=30,\n",
    "                       learning_rate=\"auto\", init=\"random\", random_state=random_state)\n",
    "        Z = reducer.fit_transform(D_jac)\n",
    "        name = \"t-SNE(fp, Jaccard)\"\n",
    "    return Z, name\n",
    "\n",
    "X_fp_bool = X_fp.astype(bool)\n",
    "Z_fp, name_fp = embed_fingerprints(X_fp_bool, use_umap=True, random_state=0)\n",
    "\n",
    "k = 3\n",
    "km_fp = KMeans(n_clusters=k, random_state=0, n_init=10).fit(Z_fp)\n",
    "labs_fp = km_fp.labels_\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "for c in np.unique(labs_fp):\n",
    "    idx = labs_fp == c\n",
    "    plt.scatter(Z_fp[idx,0], Z_fp[idx,1], s=14, alpha=0.9, label=f\"c{c}\")\n",
    "plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
    "plt.title(f\"{name_fp} + KMeans(k={k}) on fingerprints\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6493c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 1) pick two per cluster\n",
    "picks_fp = pick_two_per_cluster(labs_fp)\n",
    "\n",
    "# 2) draw molecules for each cluster\n",
    "for c, idxs in picks_fp.items():\n",
    "    print(f\"\\nFingerprints {name_fp} cluster c{c}: showing up to 2 molecules\")\n",
    "    show_molecules(idxs)\n",
    "\n",
    "# 3) star their positions on the existing embedding\n",
    "plt.figure(figsize=(5,4))\n",
    "for c in np.unique(labs_fp):\n",
    "    idx = labs_fp == c\n",
    "    plt.scatter(Z_fp[idx,0], Z_fp[idx,1], s=14, alpha=0.7, label=f\"c{c}\")\n",
    "\n",
    "for c, idxs in picks_fp.items():\n",
    "    if len(idxs) == 0:\n",
    "        continue\n",
    "    plt.scatter(Z_fp[idxs,0], Z_fp[idxs,1], s=140, marker=\"*\", edgecolor=\"k\", linewidth=0.7, label=f\"c{c} picks\")\n",
    "\n",
    "plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
    "plt.title(f\"{name_fp} + KMeans(k=3) on fingerprints with picked molecules\")\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b49c2d",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Picking k and validating clusters\n",
    "\n",
    "Choosing the right number of clusters $k$ is a central step in KMeans.  \n",
    "If $k$ is too small, distinct groups may be forced together.  \n",
    "If $k$ is too large, the algorithm may split natural groups unnecessarily.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 Elbow plot\n",
    "\n",
    "KMeans optimizes the **within-cluster sum of squares** (WCSS), often denoted $W_k$:\n",
    "\n",
    "$\n",
    "W_k = \\sum_{j=1}^k \\sum_{x_i \\in S_j} \\| x_i - c_j \\|^2\n",
    "$\n",
    "\n",
    "- $S_j$ = the set of points in cluster $j$  \n",
    "- $c_j$ = the centroid of cluster $j$  \n",
    "- $\\| x_i - c_j \\|^2$ = squared Euclidean distance\n",
    "\n",
    "As $k$ increases, $W_k$ always decreases, since more clusters mean smaller groups.  \n",
    "But the **rate of decrease slows down**. The \"elbow\" of the curve marks a good trade-off:  \n",
    "beyond this point, adding clusters yields little gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(2, 10)\n",
    "inertias = []\n",
    "for kk in ks:\n",
    "    km = KMeans(n_clusters=kk, random_state=0, n_init=10).fit(X_desc)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.plot(list(ks), inertias, marker=\"o\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow sweep on descriptors\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b39923",
   "metadata": {},
   "source": [
    "### 4.2 Silhouette score\n",
    "\n",
    "Another way to decide $k$ is the **silhouette score**, which measures how well each point fits within its cluster compared to others.\n",
    "\n",
    "For a point $i$:\n",
    "\n",
    "1. Let $a(i)$ = average distance of $i$ to all other points in the **same cluster** (intra-cluster distance).  \n",
    "2. Let $b(i)$ = the minimum average distance of $i$ to points in **any other cluster** (nearest-cluster distance).  \n",
    "\n",
    "Then the silhouette of point $i$ is:\n",
    "\n",
    "$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max \\{ a(i), b(i) \\}}\n",
    "$\n",
    "\n",
    "In particular:\n",
    "\n",
    "- $s(i)$ ranges in $[-1, 1]$.  \n",
    "- $s(i) \\approx 1$: point is well clustered (much closer to its own cluster than others).  \n",
    "- $s(i) \\approx 0$: point is on a boundary between clusters.  \n",
    "- $s(i) < 0$: point may be misclassified (closer to another cluster).  \n",
    "\n",
    "Take home messgae: higher $S$ the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3803fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil = []\n",
    "for kk in ks:\n",
    "    km = KMeans(n_clusters=kk, random_state=0, n_init=10).fit(X_desc)\n",
    "    sil.append(silhouette_score(X_desc, km.labels_))\n",
    "\n",
    "pd.DataFrame({\"k\": list(ks), \"silhouette\": np.round(sil,3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307498e",
   "metadata": {},
   "source": [
    "Visualize the silhouette for a chosen k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(X, labels):\n",
    "    s = silhouette_samples(X, labels)\n",
    "    order = np.argsort(labels)\n",
    "    s_sorted = s[order]\n",
    "    lbl_sorted = np.array(labels)[order]\n",
    "    plt.figure(figsize=(5,3))\n",
    "    y0 = 0\n",
    "    for c in np.unique(lbl_sorted):\n",
    "        vals = s_sorted[lbl_sorted == c]\n",
    "        y1 = y0 + len(vals)\n",
    "        plt.barh(np.arange(y0, y1), vals, edgecolor=\"none\")\n",
    "        plt.text(0.02, (y0+y1)/2, f\"c{c}\", va=\"center\")\n",
    "        y0 = y1\n",
    "    plt.xlabel(\"silhouette\")\n",
    "    plt.ylabel(\"samples (grouped by cluster)\")\n",
    "    plt.title(\"Silhouette plot\")\n",
    "    plt.axvline(np.mean(s), color=\"k\", linestyle=\"--\")\n",
    "    plt.show()\n",
    "\n",
    "plot_silhouette(X_desc, labels_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b8e45",
   "metadata": {},
   "source": [
    "```{admonition} ⏰ **Exercise 4**\n",
    "\n",
    "In section 3, we also have `Z_desc` from either t-sne or umap, try to use `Z_desc` to calcuate elbow and silhouette.\n",
    "\n",
    "We also have `Z_fp`, try to use it to make the plot as well.\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Alternative clustering methods\n",
    "\n",
    "\n",
    "KMeans assumes clusters are roughly spherical and similar in size.  \n",
    "But real chemical or molecular data may not follow this pattern.  \n",
    "That is why it is useful to explore **alternative clustering methods** that can adapt to different cluster shapes.\n",
    "\n",
    "\n",
    "\n",
    "### 5.1 Agglomerative (Ward)\n",
    "\n",
    "Agglomerative clustering is a **hierarchical method**.  \n",
    "It does not start with predefined cluster centers. Instead, it begins by treating each point as its own cluster and then **merges clusters step by step** until only $k$ remain. Here are the steps:\n",
    "\n",
    "1. **Initialization**: each point is its own cluster.  \n",
    "2. **Iteration**: repeatedly merge the two clusters that are closest.  \n",
    "3. **Stop** when exactly $k$ clusters remain.\n",
    "\n",
    "\n",
    "Different definitions of \"closest\" give different results.  \n",
    "The **Ward method** merges the two clusters that cause the smallest increase in within-cluster variance.\n",
    "\n",
    "Formally, at each step it chooses the merge that minimizes the increase of:\n",
    "\n",
    "$\n",
    "\\sum_{j=1}^{k} \\sum_{x_i \\in S_j} \\| x_i - c_j \\|^2\n",
    "$\n",
    "\n",
    "This is similar to the KMeans objective, but built hierarchically rather than iteratively reassigning points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\n",
    "lab_agg = agg.fit_predict(X_desc)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "for lab in np.unique(lab_agg):\n",
    "    idx = lab_agg == lab\n",
    "    plt.scatter(Z[idx,0], Z[idx,1], s=14, alpha=0.8, label=f\"agg {lab}\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.title(\"Agglomerative on descriptors\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "if y_reac.nunique() > 1:\n",
    "    print(\"ARI:\", round(adjusted_rand_score(y_reac, lab_agg), 3),\n",
    "          \"NMI:\", round(normalized_mutual_info_score(y_reac, lab_agg), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb405bb6",
   "metadata": {},
   "source": [
    "### 5.2 DBSCAN\n",
    "\n",
    "Unlike KMeans or Agglomerative, **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) does not require $k$ in advance.  \n",
    "Instead, it groups points based on density and labels sparse points as noise The key ideas are:\n",
    "\n",
    "- A point is a core point if at least `min_samples` neighbors fall within radius $\\varepsilon$ (epsilon).  \n",
    "- Points within $\\varepsilon$ of a core point are part of the same cluster.  \n",
    "- Clusters expand outward from core points.  \n",
    "- Points not reachable from any core point are labeled noise ($-1$).\n",
    "\n",
    "So, why DBSCAN is useful? It can find **non-spherical** clusters (e.g., moons, rings). As we will see for the example below. Also it handles noise explicitly and does not force every point into a cluster.  \n",
    "\n",
    "But: results depend strongly on $\\varepsilon$ and `min_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.65, min_samples=5).fit(X_desc)\n",
    "lab_db = db.labels_\n",
    "np.unique(lab_db, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "for lab in np.unique(lab_db):\n",
    "    idx = lab_db == lab\n",
    "    name = \"noise\" if lab == -1 else f\"db {lab}\"\n",
    "    plt.scatter(Z[idx,0], Z[idx,1], s=14, alpha=0.8, label=name)\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.title(\"DBSCAN on descriptors\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a57c05",
   "metadata": {},
   "source": [
    "Tip: tune `eps` by scanning a small grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_grid = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0, 1.2]\n",
    "rows = []\n",
    "for e in eps_grid:\n",
    "    db = DBSCAN(eps=e, min_samples=8).fit(X_desc)\n",
    "    labs = db.labels_\n",
    "    n_noise = np.sum(labs == -1)\n",
    "    n_clu = len(np.unique(labs[labs!=-1]))\n",
    "    rows.append({\"eps\": e, \"n_clusters\": n_clu, \"n_noise\": int(n_noise)})\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e8670",
   "metadata": {},
   "source": [
    "Below are some examples for you to get a better idea on their difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dad701",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Column 1: KMeans. Pick k by highest silhouette.\n",
    "- Column 2: Agglomerative (Ward). Pick k by highest silhouette.\n",
    "- Column 3: DBSCAN. Pick eps and min_samples by highest silhouette.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from itertools import cycle, islice\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Build toy datasets with different structure\n",
    "# ---------------------------\n",
    "n_samples = 1500\n",
    "rng_seed = 170\n",
    "\n",
    "noisy_circles = make_circles(n_samples=n_samples, factor=0.5, noise=0.05, random_state=rng_seed)\n",
    "noisy_moons   = make_moons(n_samples=n_samples, noise=0.05, random_state=rng_seed)\n",
    "blobs         = make_blobs(n_samples=n_samples, random_state=8)\n",
    "\n",
    "# Anisotropic blobs via linear transform\n",
    "X_aniso, y_aniso = make_blobs(n_samples=n_samples, random_state=rng_seed)\n",
    "transformation = np.array([[0.6, -0.6],\n",
    "                           [-0.4, 0.8]])\n",
    "X_aniso = X_aniso @ transformation\n",
    "aniso = (X_aniso, y_aniso)\n",
    "\n",
    "# Blobs with varied variances\n",
    "varied = make_blobs(n_samples=n_samples,\n",
    "                    cluster_std=[1.0, 2.5, 0.5],\n",
    "                    random_state=rng_seed)\n",
    "\n",
    "# No structure cloud\n",
    "rs = np.random.RandomState(rng_seed)\n",
    "no_structure = (rs.rand(n_samples, 2), None)\n",
    "\n",
    "datasets = [\n",
    "    (\"noisy_circles\", noisy_circles),\n",
    "    (\"noisy_moons\",   noisy_moons),\n",
    "    (\"varied_blobs\",  varied),\n",
    "    (\"aniso_blobs\",   aniso),\n",
    "    (\"blobs\",         blobs),\n",
    "    (\"no_structure\",  no_structure),\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Color helper\n",
    "# ---------------------------\n",
    "base_colors = [\n",
    "    \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\",\n",
    "    \"#d62728\", \"#9467bd\", \"#8c564b\",\n",
    "    \"#e377c2\", \"#7f7f7f\", \"#bcbd22\"\n",
    "]\n",
    "\n",
    "def colors_for_labels(y_pred, noise_color=\"#000000\"):\n",
    "    \"\"\"\n",
    "    Map cluster labels to colors.\n",
    "    DBSCAN noise points labeled as -1 get black.\n",
    "    \"\"\"\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    pos = y_pred[y_pred >= 0]\n",
    "    n_labels = int(pos.max()) + 1 if pos.size else 1\n",
    "    palette = np.array(list(islice(cycle(base_colors), n_labels)))\n",
    "    col = palette[np.clip(y_pred, 0, n_labels - 1)]\n",
    "    col[y_pred == -1] = noise_color\n",
    "    return col\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Scoring utility\n",
    "# ---------------------------\n",
    "def safe_silhouette(X, labels):\n",
    "    \"\"\"\n",
    "    Return silhouette score if valid. Otherwise a very low score.\n",
    "    Valid means at least 2 clusters and no single-cluster assignment.\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels)\n",
    "    uniq = np.unique(labels)\n",
    "    # Remove noise for the check when all are noise\n",
    "    if len(uniq) == 1:\n",
    "        return -1e9\n",
    "    # If DBSCAN produced only noise and one cluster, also invalid\n",
    "    if np.all(labels == -1):\n",
    "        return -1e9\n",
    "    # Check at least 2 non-noise clusters\n",
    "    nn = labels[labels != -1]\n",
    "    if nn.size == 0 or np.unique(nn).size < 2:\n",
    "        return -1e9\n",
    "    try:\n",
    "        return silhouette_score(X, labels)\n",
    "    except Exception:\n",
    "        return -1e9\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Model selection per method\n",
    "# ---------------------------\n",
    "def select_kmeans(X, k_range=range(2, 5)):\n",
    "    best = {\"score\": -1e9, \"k\": None, \"labels\": None, \"fit_time\": 0.0}\n",
    "    for k in k_range:\n",
    "        km = cluster.KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        t0 = time.time()\n",
    "        labels = km.fit_predict(X)\n",
    "        fit_t = time.time() - t0\n",
    "        score = safe_silhouette(X, labels)\n",
    "        if score > best[\"score\"]:\n",
    "            best = {\"score\": score, \"k\": k, \"labels\": labels, \"fit_time\": fit_t}\n",
    "    return best\n",
    "\n",
    "def select_ward(X, k_range=range(2, 5), n_neighbors=10):\n",
    "    # Build a sparse graph to encourage local merges\n",
    "    connectivity = kneighbors_graph(X, n_neighbors=n_neighbors, include_self=False)\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "    best = {\"score\": -1e9, \"k\": None, \"labels\": None, \"fit_time\": 0.0}\n",
    "    for k in k_range:\n",
    "        ward = cluster.AgglomerativeClustering(n_clusters=k, linkage=\"ward\", connectivity=connectivity)\n",
    "        t0 = time.time()\n",
    "        labels = ward.fit_predict(X)\n",
    "        fit_t = time.time() - t0\n",
    "        score = safe_silhouette(X, labels)\n",
    "        if score > best[\"score\"]:\n",
    "            best = {\"score\": score, \"k\": k, \"labels\": labels, \"fit_time\": fit_t}\n",
    "    return best\n",
    "\n",
    "def select_dbscan(X, eps_grid=None, min_samples_grid=(3, 5, 8)):\n",
    "    # If eps grid not given, make a small sweep adapted to standardized data\n",
    "    if eps_grid is None:\n",
    "        eps_grid = np.linspace(0.05, 0.5, 10)\n",
    "    best = {\"score\": -1e9, \"eps\": None, \"min_samples\": None, \"labels\": None, \"fit_time\": 0.0}\n",
    "    for eps in eps_grid:\n",
    "        for ms in min_samples_grid:\n",
    "            db = cluster.DBSCAN(eps=eps, min_samples=ms)\n",
    "            t0 = time.time()\n",
    "            labels = db.fit_predict(X)\n",
    "            fit_t = time.time() - t0\n",
    "            score = safe_silhouette(X, labels)\n",
    "            if score > best[\"score\"]:\n",
    "                best = {\"score\": score, \"eps\": eps, \"min_samples\": ms, \"labels\": labels, \"fit_time\": fit_t}\n",
    "    return best\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Figure layout: 6 rows x 3 columns\n",
    "# ---------------------------\n",
    "n_rows = len(datasets)\n",
    "n_cols = 3\n",
    "fig = plt.figure(figsize=(n_cols * 3.2, n_rows * 2.6))\n",
    "plt.subplots_adjust(left=.04, right=.99, bottom=.04, top=.92, wspace=.07, hspace=.18)\n",
    "\n",
    "col_titles = [\"KMeans (best k)\", \"Agglomerative Ward (best k)\", \"DBSCAN (best eps, ms)\"]\n",
    "\n",
    "plot_idx = 1\n",
    "for row_i, (ds_name, ds) in enumerate(datasets):\n",
    "    X, y = ds\n",
    "    # Standardize for fair distance use\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # KMeans\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        best_km = select_kmeans(X)\n",
    "    ax = plt.subplot(n_rows, n_cols, plot_idx); plot_idx += 1\n",
    "    cols = colors_for_labels(best_km[\"labels\"])\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=8, c=cols, linewidths=0, alpha=0.95)\n",
    "    if row_i == 0:\n",
    "        ax.set_title(col_titles[0], fontsize=11, pad=8)\n",
    "    ax.set_ylabel(ds_name, fontsize=11) if (plot_idx - 2) % 3 == 0 else None\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.text(0.02, 0.98, f\"k={best_km['k']}  sil={best_km['score']:.2f}\\n{best_km['fit_time']:.2f}s\",\n",
    "            transform=ax.transAxes, ha=\"left\", va=\"top\", fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.8, edgecolor=\"none\"))\n",
    "\n",
    "    # Ward\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        best_wd = select_ward(X)\n",
    "    ax = plt.subplot(n_rows, n_cols, plot_idx); plot_idx += 1\n",
    "    cols = colors_for_labels(best_wd[\"labels\"])\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=8, c=cols, linewidths=0, alpha=0.95)\n",
    "    if row_i == 0:\n",
    "        ax.set_title(col_titles[1], fontsize=11, pad=8)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.text(0.02, 0.98, f\"k={best_wd['k']}  sil={best_wd['score']:.2f}\\n{best_wd['fit_time']:.2f}s\",\n",
    "            transform=ax.transAxes, ha=\"left\", va=\"top\", fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.8, edgecolor=\"none\"))\n",
    "\n",
    "    # DBSCAN\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        best_db = select_dbscan(X)\n",
    "    ax = plt.subplot(n_rows, n_cols, plot_idx); plot_idx += 1\n",
    "    cols = colors_for_labels(best_db[\"labels\"])\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=8, c=cols, linewidths=0, alpha=0.95)\n",
    "    if row_i == 0:\n",
    "        ax.set_title(col_titles[2], fontsize=11, pad=8)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    label = f\"eps={best_db['eps']:.2f}, ms={best_db['min_samples']}\"\n",
    "    ax.text(0.02, 0.98, f\"{label}\\nsil={best_db['score']:.2f}\\n{best_db['fit_time']:.2f}s\",\n",
    "            transform=ax.transAxes, ha=\"left\", va=\"top\", fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.8, edgecolor=\"none\"))\n",
    "\n",
    "fig.suptitle(\"Clustering gallery with automatic selection per dataset\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78caf94f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "---\n",
    "## 6. Glossary\n",
    "\n",
    "```{glossary}\n",
    "\n",
    "clustering\n",
    "    Group samples using only $X$, no labels during fit.\n",
    "\n",
    "KMeans\n",
    "    A clustering algorithm that assigns points to k centroids, updating assignments and centroids to minimize within-cluster variance.\n",
    "\n",
    "Agglomerative clustering\n",
    "    A hierarchical algorithm that starts with each point as its own cluster and merges clusters step by step until k remain.\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "    A density-based clustering method that groups points when they have enough neighbors within a radius. Points that don’t belong to any dense region are labeled noise.\n",
    "\n",
    "elbow method\n",
    "    A heuristic for selecting the number of clusters in KMeans by plotting inertia vs k. The \"elbow\" marks diminishing returns from adding more clusters.\n",
    "\n",
    "silhouette score\n",
    "    A metric for clustering quality. For each point, compares average distance to its own cluster vs the nearest other cluster. Ranges from -1 (bad) to +1 (good).\n",
    "\n",
    "Tanimoto / Jaccard similarity\n",
    "    For binary fingerprints, both measure overlap divided by union. Often used to compare molecular fingerprints in chemoinformatics.\n",
    "\n",
    "\n",
    "```\n",
    "---\n",
    "## 7. In-class activity\n",
    "\n",
    "### Q1. t-SNE on 10 descriptors\n",
    "Rebuild features using 10 descriptors, embed with t-SNE, plot in 2D.\n",
    "\n",
    "Hint: copy and use function `def calc_descriptors10(smiles: str)` from Lecture 11 if you forget how to define the 10 descriptor.\n",
    "```python\n",
    "# TO DO\n",
    "```\n",
    "\n",
    "### Q2. Elbow on KMeans\n",
    "Choose a reasonable k by inertia.\n",
    "\n",
    "1) Fit `KMeans` for k in {2..9}.\n",
    "2) Record `inertia_` for each k.\n",
    "3) Plot `inertia` vs `k` and inspect the elbow.\n",
    "```python\n",
    "# TO DO\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Q3. Silhouette on KMeans\n",
    "Choose k by separation vs compactness.\n",
    "\n",
    "1) Fit `KMeans` for k in {2..9}.\n",
    "2) Compute `silhouette_score()` for each k.\n",
    "3) Plot `silhouette` vs `k`. Report the best k by this metric.\n",
    "\n",
    "```python\n",
    "# TO DO\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Q4. Agglomerative sweep with plots\n",
    "Compare another clustering model and visualize the chosen solution.\n",
    "\n",
    "1) For k in {2..9}, fit `AgglomerativeClustering(linkage=\"ward\")`.\n",
    "2) Compute the silhouette for each k and plot `silhouette` vs `k`.\n",
    "3) Pick the best k by silhouette, refit, then plot the cluster assignments on the same t-SNE plane from Q1.\n",
    "\n",
    "```python\n",
    "# TO DO\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Solutions\n",
    "\n",
    "**Q1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. t-SNE on 10 descriptors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumes df_raw is already loaded and RDKit imports exist:\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors\n",
    "\n",
    "def calc_descriptors10(smiles: str):\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\": Crippen.MolLogP(m),\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(m),\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(m),\n",
    "        \"NumHAcceptors\": rdMolDescriptors.CalcNumHBA(m),\n",
    "        \"NumHDonors\": rdMolDescriptors.CalcNumHBD(m),\n",
    "        \"NumRotatableBonds\": rdMolDescriptors.CalcNumRotatableBonds(m),\n",
    "        \"HeavyAtomCount\": Descriptors.HeavyAtomCount(m),\n",
    "        \"FractionCSP3\": rdMolDescriptors.CalcFractionCSP3(m),\n",
    "        \"NumAromaticRings\": rdMolDescriptors.CalcNumAromaticRings(m)\n",
    "    })\n",
    "\n",
    "# 10 descriptors\n",
    "desc10 = df_raw[\"SMILES\"].apply(calc_descriptors10)\n",
    "df10 = pd.concat([df_raw.reset_index(drop=True), desc10.reset_index(drop=True)], axis=1)\n",
    "\n",
    "cols10 = [\n",
    "    \"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"NumHAcceptors\",\n",
    "    \"NumHDonors\",\"NumRotatableBonds\",\"HeavyAtomCount\",\n",
    "    \"FractionCSP3\",\"NumAromaticRings\"\n",
    "]\n",
    "scaler10 = StandardScaler().fit(df10[cols10])\n",
    "X10 = scaler10.transform(df10[cols10])\n",
    "\n",
    "# t-SNE embedding for descriptors (reused in Q4 and Q5)\n",
    "tsne10 = TSNE(n_components=2, perplexity=30, learning_rate=\"auto\",\n",
    "              init=\"pca\", metric=\"euclidean\", random_state=0)\n",
    "Z10 = tsne10.fit_transform(X10)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(Z10[:,0], Z10[:,1], s=12, alpha=0.85)\n",
    "plt.xlabel(\"t-SNE 1\"); plt.ylabel(\"t-SNE 2\")\n",
    "plt.title(\"t-SNE on 10 descriptors\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319b5ec",
   "metadata": {},
   "source": [
    "**Q2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Elbow on KMeans (k=2..9)\n",
    "\n",
    "ks = range(2, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=10).fit(X10)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(list(ks), inertias, marker=\"o\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow on 10-descriptor KMeans\")\n",
    "plt.grid(True); plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame({\"k\": list(ks), \"inertia\": inertias}).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd7d25",
   "metadata": {},
   "source": [
    "**Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Silhouette on KMeans (k=2..9)\n",
    "sil_scores = []\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init=10).fit(X10)\n",
    "    sil_scores.append(silhouette_score(X10, km.labels_))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(list(ks), sil_scores, marker=\"o\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Silhouette\")\n",
    "plt.title(\"Silhouette vs k on 10-descriptor KMeans\")\n",
    "plt.grid(True); plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k_sil = list(ks)[int(np.argmax(sil_scores))]\n",
    "print(\"Best k by silhouette:\", best_k_sil)\n",
    "pd.DataFrame({\"k\": list(ks), \"silhouette\": np.round(sil_scores, 3)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e51dc",
   "metadata": {},
   "source": [
    "**Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Agglomerative sweep with plots\n",
    "sil_agg = []\n",
    "for k in ks:\n",
    "    agg = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
    "    labels_agg = agg.fit_predict(X10)\n",
    "    sil_agg.append(silhouette_score(X10, labels_agg))\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(list(ks), sil_agg, marker=\"o\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Silhouette\")\n",
    "plt.title(\"Agglomerative (ward) silhouette vs k on 10 descriptors\")\n",
    "plt.grid(True); plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k_agg = list(ks)[int(np.argmax(sil_agg))]\n",
    "print(\"Best k for Agglomerative by silhouette:\", best_k_agg)\n",
    "\n",
    "# Fit best k and plot clusters on t-SNE plane (Z10)\n",
    "agg_best = AgglomerativeClustering(n_clusters=best_k_agg, linkage=\"ward\")\n",
    "labels_agg_best = agg_best.fit_predict(X10)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "for c in np.unique(labels_agg_best):\n",
    "    idx = labels_agg_best == c\n",
    "    plt.scatter(Z10[idx,0], Z10[idx,1], s=12, alpha=0.9, label=f\"c{c}\")\n",
    "plt.xlabel(\"t-SNE 1\"); plt.ylabel(\"t-SNE 2\")\n",
    "plt.title(f\"Agglomerative (ward) on t-SNE, k={best_k_agg}\")\n",
    "plt.legend(bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "source_map": [
   12,
   32,
   72,
   78,
   103,
   107,
   111,
   115,
   128,
   132,
   138,
   154,
   162,
   208,
   214,
   218,
   227,
   236,
   242,
   254,
   259,
   265,
   269,
   302,
   313,
   360,
   377,
   408,
   435,
   462,
   475,
   501,
   508,
   512,
   533,
   575,
   589,
   605,
   611,
   619,
   623,
   633,
   636,
   847,
   932,
   986,
   990,
   1007,
   1012,
   1030,
   1034
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}