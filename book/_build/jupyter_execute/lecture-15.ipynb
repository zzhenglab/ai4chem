{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c9b8d1",
   "metadata": {},
   "source": [
    "# Lecture 15 - Multi-objective Bayesian Optimization\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    ":depth: 1\n",
    "```\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Connect single objective Bayesian Optimization to multiobjective problems.\n",
    "- Define Pareto dominance, Pareto front, scalarization, hypervolume, and expected hypervolume improvement.\n",
    "- Build a simple **multiobjective active learning** loop on a materials.\n",
    "\n",
    "[![Colab](https://img.shields.io/badge/Open-Colab-orange)](https://colab.research.google.com/drive/1QvnxeGN3JpZl1viIaQ9WxQrNJ6gYFkya?usp=sharing)\n",
    "\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df28ce8e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 0.1 Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5.4, 3.6)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem, RDLogger\n",
    "    from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors, QED, Draw\n",
    "    RD = True\n",
    "except Exception:\n",
    "    try:\n",
    "      %pip install rdkit\n",
    "      from rdkit import Chem, RDLogger\n",
    "      from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors, QED, Draw\n",
    "      RD = True\n",
    "    except:\n",
    "      print(\"RDKit not installed\")\n",
    "      RD = False\n",
    "      Chem = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d707752",
   "metadata": {},
   "source": [
    "## 1. What is multiobjective optimization\n",
    "\n",
    "In previous lecture, we used a **surrogate model** to represent an expensive experiment, then selected the next experiment using an **acquisition function** like Expected Improvement to iteratively find the optimized yield by tuning reaction parameter such as temperature. \n",
    "\n",
    "We learned that we can:\n",
    "\n",
    "1) Fit a surrogate on $(X, y)$.  \n",
    "2) Predict $\\mu(x)$ and $\\sigma(x)$ on candidates.  \n",
    "3) Score an acquisition $a(x)$ (EI, UCB, PI).  \n",
    "4) Pick $x_{next} = \\arg\\max a(x)$ and run the experiment.  \n",
    "5) Update data and repeat.\n",
    "\n",
    "\n",
    "\n",
    "Today we extend that idea to *multiple* goals at once, for example high yield and high purity with decent reproducibility.\n",
    "\n",
    "\n",
    "In multiobjective optimization we consider a vector objective\n",
    "$${\\bf f}(x) = \\big(f_1(x), f_2(x), \\ldots, f_M(x)\\big).$$\n",
    "\n",
    "For chemical synthesis you might set\n",
    "- $f_1$ as yield to maximize,\n",
    "- $f_2$ as purity or selectivity to maximize,\n",
    "- $f_3$ as cost to minimize or reproducibility score to maximize.\n",
    "\n",
    "There is usually no single $x$ that maximizes all objectives together. Instead we aim to approximate the **Pareto front**.\n",
    "\n",
    "Below are a few definitions associated with this idea:\n",
    "- **Dominance**: a point $a$ dominates $b$ if it is at least as good on all objectives and strictly better on at least one.  \n",
    "- **Pareto set**: the set of nondominated decision points.  \n",
    "- **Pareto front**: the image of the Pareto set in objective space.  \n",
    "- **Hypervolume**: the volume of objective space dominated by the current front relative to a reference point.\n",
    "\n",
    "\n",
    "Below is a tiny 2D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5963d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 540x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.1 Simple dominance and Pareto front in 2D\n",
    "def pareto_mask(Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return a boolean mask for nondominated rows of Y to maximize both columns.\"\"\"\n",
    "    n = Y.shape[0]\n",
    "    keep = np.ones(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if not keep[i]:\n",
    "            continue\n",
    "        dominates = np.all(Y[i] >= Y, axis=1) & np.any(Y[i] > Y, axis=1)\n",
    "        keep[dominates] = False\n",
    "        keep[i] = True\n",
    "    return keep\n",
    "\n",
    "Y_demo = np.array([\n",
    "    [0.2, 0.9],[0.6, 0.6],\n",
    "    [0.9, 0.2],[0.5, 0.8],\n",
    "    [0.5, 0.5],[0.4, 0.7],\n",
    "    [0.7, 0.3],[0.8, 0.4],\n",
    "    [0.3, 0.3], [0.2, 0.5],\n",
    "    [0.6, 0.5], [0.4, 0.3],\n",
    "    [0.3, 0.2], [0.3, 0.5],\n",
    "\n",
    "])\n",
    "\n",
    "mask = pareto_mask(Y_demo)\n",
    "Y_nd = Y_demo[mask]\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(Y_demo[:,0], Y_demo[:,1], c=[\"tab:blue\"]*len(Y_demo), alpha=0.6, label=\"All\")\n",
    "plt.scatter(Y_nd[:,0], Y_nd[:,1], c=\"tab:red\", label=\"Pareto\")\n",
    "plt.xlabel(\"Yield\")\n",
    "plt.ylabel(\"Selectivity\")\n",
    "plt.title(\"Pareto front in objective space\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2342c8",
   "metadata": {},
   "source": [
    "```{admonition} â° **Exercise**\n",
    "\n",
    "Add a new point `[0.7, 0.6]` to `Y_demo`, recompute the mask, and plot again. Which earlier nondominated points become dominated now?\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Load the MOF toy dataset\n",
    "\n",
    "We will use the provided full factorial style synthetic dataset that mimics MOF synthesis outcomes across temperature, time, concentration, solvent, and linker choice. The file includes yield, purity, and reproducibility.\n",
    "\n",
    "We will use a synthetic MOF dataset that records MOF synthesis outcomes across temperature, time, concentration, solvent, and linker choice. The file includes yield, purity, and reproducibility:\n",
    "\n",
    "- `yield` in $[0, 0.99]$ with noise, boosts, and failures.\n",
    "- `purity` in $[0, 1]$ with discontinuities.\n",
    "- `reproducibility` in $\\{0.25, 0.5, 0.75, 1.0\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06566dc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2.1 Load the dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttps://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/mof_yield_dataset.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_raw = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df_raw \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:495\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    494\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:604\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:533\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    532\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    465\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:613\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# 2.1 Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/mof_yield_dataset.csv\"\n",
    "df_raw = pd.read_csv(url)\n",
    "df_raw "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6933737b",
   "metadata": {},
   "source": [
    "The reaction parameters include:\n",
    "   1) temperature (Â°C): 10 levels -> `[25, 40, 55, 70, 85, 100, 115, 130, 155, 160]`\n",
    "   2) time_h (hours): 10 levels -> `[12, 24, ..., 120]`\n",
    "   3) concentration_M (M): 10 levels -> `[0.05, 0.10, ..., 0.50]`\n",
    "   4) solvent_DMF: one-hot binary {`0`=H2O, `1`=dimethyl foramide}\n",
    "   5) organic linker (10 choices), such as `Fumaric acid`, `Trimesic acid`, and `Benzimidazole`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a701dc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "df = df_raw.sample(20000, random_state=0).reset_index(drop=True) # we use full set for now\n",
    "\n",
    "cols_with_few_values = ['temperature', 'time_h', 'concentration_M', 'solvent_DMF']\n",
    "\n",
    "desc_catlike = (\n",
    "    df_raw\n",
    "    .assign(**{c: df_raw[c].astype('category') for c in cols_with_few_values})\n",
    "    [ ['smiles'] + cols_with_few_values ]\n",
    "    .describe(include='all').T[['count','unique','top','freq']]\n",
    ")\n",
    "\n",
    "desc_catlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375cb8d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# How many linkers and a quick count\n",
    "linker_counts = df[\"smiles\"].value_counts()\n",
    "linker_counts.head(10), \"n_linkers:\", df[\"smiles\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b9030",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature engineering for multiobjective modeling\n",
    "\n",
    "We will build a light feature set using both synthesis conditions and molecular structure information. This compact design provides chemical and process context without relying on heavy cheminformatics toolkits.\n",
    "\n",
    "### 3.1 Minimal numeric features\n",
    "\n",
    "We begin with numeric synthesis parameters that directly influence material outcomes:\n",
    "`temperature`, `time_h`, `concentration_M`, and `solvent_DMF`.\n",
    "These are copied into a numeric feature frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"temperature\", \"time_h\", \"concentration_M\", \"solvent_DMF\"]\n",
    "X_num = df[num_cols].astype(float).copy()\n",
    "X_num "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb4e84",
   "metadata": {},
   "source": [
    "### 3.2 One-hot encoding for SMILES\n",
    "\n",
    "The SMILES column lists discrete molecular structures.\n",
    "To make this information usable by machine learning models, we convert it into a numeric form through **one-hot encoding**.\n",
    "\n",
    "One-hot encoding creates a binary vector for each category:\n",
    "\n",
    "- Each unique SMILES gets its own column.\n",
    "\n",
    "- A value of `1` marks the presence of that molecule in a given row, and `0` otherwise.\n",
    "\n",
    "This prevents the model from interpreting SMILES identifiers as ordinal numbers while preserving molecular identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c69fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define encoder\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform SMILES into binary vectors\n",
    "smiles_encoded = enc.fit_transform(df[['smiles']])\n",
    "\n",
    "# Retrieve encoded column names\n",
    "smiles_feature_names = enc.get_feature_names_out(['smiles'])\n",
    "\n",
    "# Build DataFrame for encoded SMILES\n",
    "X_smiles = pd.DataFrame(smiles_encoded, columns=smiles_feature_names, index=df.index)\n",
    "\n",
    "# Combine numeric and encoded features\n",
    "X = pd.concat([X_num, X_smiles], axis=1)\n",
    "\n",
    "\n",
    "print(X.shape, X.columns[:10].tolist())\n",
    "X.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94541736",
   "metadata": {},
   "source": [
    "### 3.3 Targets for multiobjective learning\n",
    "\n",
    "We will form a 3 objective vector:\n",
    "- $f_1$: `yield` to maximize.\n",
    "- $f_2$: `purity` to maximize.\n",
    "- $f_3$: numeric reproducibility where higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Target matrix\n",
    "Y = df[[\"yield\",\"purity\",\"reproducibility\"]].astype(float).values\n",
    "print(Y.min(axis=0), Y.max(axis=0))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87e15b",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Pareto utilities and hypervolume in 2D\n",
    "\n",
    "This section builds intuition for multiobjective comparison using two outcomes at a time before returning to three. \n",
    "\n",
    "\n",
    "We use $f_1=$ = yield and $f_2=$  = purity. The goal is to identify the subset of experiments that are not outperformed by any other experiment across both objectives. These are the candidates on the Pareto front. Working in 2D makes it easy to see which points survive and how much area of the objective space they dominate relative to a chosen reference.\n",
    "\n",
    "```{admonition} Note\n",
    "Note that in reality, we usually start with only a couple of experiments. So we will show the case for both **50** experiment and the full dataset (**20000** experiments).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcc599",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def nondominated_mask(F: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Non dominated set for maximization.\n",
    "    F: shape (n_points, n_obj)\n",
    "    Returns boolean mask where True means non dominated.\n",
    "    \"\"\"\n",
    "    F = np.asarray(F)\n",
    "    n = F.shape[0]\n",
    "    mask = np.ones(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if not mask[i]:\n",
    "            continue\n",
    "        dominates_i = ((F >= F[i]).all(axis=1) & (F > F[i]).any(axis=1))\n",
    "        dominates_i[i] = False\n",
    "        if np.any(dominates_i):\n",
    "            mask[i] = False\n",
    "    return mask\n",
    "\n",
    "\n",
    "pairs = [(0, 1), (0, 2), (1, 2)]\n",
    "labels = [\"yield\", \"purity\", \"reproducibility\"]\n",
    "\n",
    "\n",
    " # 50 experiment sample: pairwise plots\n",
    "rng = np.random.default_rng(0)\n",
    "idx_50 = rng.choice(Y.shape[0], size=50, replace=False)\n",
    "Y50 = Y[idx_50]\n",
    "\n",
    "mask_50 = nondominated_mask(Y50)\n",
    "front_50 = Y50[mask_50]\n",
    "\n",
    "for i, j in pairs:\n",
    "    plt.scatter(Y50[:, i], Y50[:, j], s=18, color = \"green\", alpha=0.35, label=\"all 50\")\n",
    "    plt.scatter(front_50[:, i], front_50[:, j], color = \"red\", s=28, alpha=0.95, label=\"Pareto 50\", edgecolor=\"k\")\n",
    "    plt.xlabel(labels[i]); plt.ylabel(labels[j])\n",
    "    plt.title(f\"{labels[i]} vs {labels[j]}  |  50 sample\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31179cd4",
   "metadata": {},
   "source": [
    "Now let's look at the full dataset. Again, note that in reality you will not be able to see this \"final answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c419a7",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Full set: pairwise plots\n",
    "mask_full = nondominated_mask(Y)\n",
    "front_full = Y[mask_full]\n",
    "\n",
    "\n",
    "for i, j in pairs:\n",
    "    plt.scatter(Y[:, i], Y[:, j], s=6, alpha=0.3, label=\"all\")\n",
    "    plt.scatter(front_full[:, i], front_full[:, j], s=18, alpha=0.9, label=\"Pareto\", edgecolor=\"k\")\n",
    "    plt.xlabel(labels[i]); plt.ylabel(labels[j])\n",
    "    plt.title(f\"{labels[i]} vs {labels[j]}  |  full set\")\n",
    "    plt.legend()\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887b4b5",
   "metadata": {},
   "source": [
    "Next, we talk about an important idea, **hypervolume (HV)** which is the volume of the dominated region bounded by a reference point $r$. It is a way to measure how much of the â€œgoodâ€ region of objective space is covered by your Pareto front. In two objectives (for example, `yield` and `purity`), each experiment corresponds to a point. The Pareto front marks those points that are not outperformed by any other in both metrics. The hypervolume is then the total area (or volume in higher dimensions) between that front and a chosen **reference point**.\n",
    "\n",
    "```{admonition} Note\n",
    "You can think of it as the portion of the map where the model or experiments are doing well. If your Pareto front moves outward toward higher yield and purity, the dominated region expands, and the hypervolume increases. A larger hypervolume means better overall trade offs across objectives.\n",
    "```\n",
    "\n",
    "The reference point defines the lower bound of the region you measure against. It must be **worse than all observed points** in every objective because hypervolume represents improvement relative to that point. For yield and purity, which are maximized, this means picking a reference near the bottom left corner of the plot.\n",
    "\n",
    "If all your data are normalized between 0 and 1, the natural choice is `(0, 0)` â€” representing the absolute worst case (zero yield, zero purity). You can also use `(0.2, 0.2)` or another slightly higher reference if you want to focus only on improvements beyond a minimum acceptable performance threshold. That reference would mean: â€œWe only care about the area above 20% yield and 20% purity.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2b1b1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def staircase_polygon_2d(front_2d: np.ndarray, ref=(0.0, 0.0)):\n",
    "    \"\"\"\n",
    "    Build polygon that represents the dominated region up to ref for maximization in 2D.\n",
    "    Returns xs, ys ready for plt.fill.\n",
    "    \"\"\"\n",
    "    F = front_2d[np.argsort(-front_2d[:, 0])]   # sort by x (f1) descending\n",
    "    y_env = np.maximum.accumulate(F[:, 1])       # envelope in y (f2)\n",
    "    xs = np.concatenate([[ref[0]], F[:, 0], [ref[0]]])\n",
    "    ys = np.concatenate([[ref[1]], y_env,    [y_env[-1]]])\n",
    "    return xs, ys\n",
    "\n",
    "def hypervolume_2d(front_2d: np.ndarray, ref=(0.0, 0.0)) -> float:\n",
    "    \"\"\"\n",
    "    Hypervolume in 2D for maximization.\n",
    "    \"\"\"\n",
    "    F = front_2d[np.argsort(-front_2d[:, 0])]\n",
    "    y_env = np.maximum.accumulate(F[:, 1])\n",
    "    xs = np.concatenate([F[:, 0], [ref[0]]])\n",
    "    ys = np.concatenate([y_env,   [y_env[-1]]])\n",
    "    widths = np.maximum(0.0, xs[:-1] - xs[1:])\n",
    "    heights = np.maximum(0.0, ys[:-1] - ref[1])\n",
    "    return float(np.sum(widths * heights))\n",
    "\n",
    "# Pareto front for the 50 set\n",
    "mask50 = nondominated_mask(Y50)\n",
    "front50 = Y50[mask50]\n",
    "\n",
    "front50.shape, mask50.sum()\n",
    "\n",
    "\n",
    "# 2D plot for the 50 experiment set\n",
    "ref = (0.0, 0.0)  # adjust if your outcomes are not in [0, 1]\n",
    "\n",
    "F2_50 = front50[:, [0, 1]]\n",
    "hv_50 = hypervolume_2d(F2_50, ref=ref)\n",
    "\n",
    "plt.scatter(Y50[:, 0], Y50[:, 1], s=18,  color = \"green\",alpha=0.35, label=\"all 50\")\n",
    "plt.scatter(front50[:, 0], front50[:, 1], s=36, alpha=0.95, color = \"red\", label=\"Pareto 50\", edgecolor=\"k\")\n",
    "\n",
    "xs50, ys50 = staircase_polygon_2d(F2_50, ref=ref)\n",
    "plt.fill(xs50, ys50, alpha=0.2, label=f\"HV 50 = {hv_50:.4f}\")\n",
    "plt.scatter([ref[0]], [ref[1]], s=40, marker=\"x\", label=\"reference\")\n",
    "\n",
    "plt.xlabel(\"yield\")\n",
    "plt.ylabel(\"purity\")\n",
    "plt.title(\"Yield vs Purity for 50 experiments\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 3D plot for the 50 experiment set\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(Y50[:, 0], Y50[:, 1], Y50[:, 2], s=18, color = \"green\", alpha=0.35, label=\"all 50\")\n",
    "ax.scatter(front50[:, 0], front50[:, 1], front50[:, 2], color = \"red\", s=40, alpha=0.95, label=\"Pareto 50\", edgecolor=\"k\")\n",
    "\n",
    "ax.set_xlabel(\"yield\")\n",
    "ax.set_ylabel(\"purity\")\n",
    "ax.set_zlabel(\"reproducibility\")\n",
    "ax.set_title(\"3D Pareto view for 50 experiments\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d57fd7",
   "metadata": {},
   "source": [
    "Let's assume we run 3 additional experiments and we are so lucky that they achieve strong outcomes around high yield and purity (`[0.90, 0.80, 0.70]`, `[0.92, 0.78, 0.75]`, and ` [0.88, 0.82, 0.65]`). \n",
    "\n",
    "We can take a look at the change to the hypervolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bcb3d2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 1) Simulate three new high-performing experiments and append to the 50-set\n",
    "#    We keep reproducibility reasonable; adjust as needed based on your context.\n",
    "new_points = np.array([\n",
    "    [0.90, 0.80, 0.70],\n",
    "    [0.92, 0.78, 0.75],\n",
    "    [0.88, 0.82, 0.65],\n",
    "], dtype=float)\n",
    "\n",
    "Y50_updated = np.vstack([Y50, new_points])\n",
    "\n",
    "# Recompute Pareto front for the updated 50+3 set\n",
    "mask50_new = nondominated_mask(Y50_updated)\n",
    "front50_new = Y50_updated[mask50_new]\n",
    "\n",
    "front50.shape, front50_new.shape\n",
    "# 2) Hypervolume before vs after on yield-purity (2D)\n",
    "ref = (0.0, 0.0)  # same reference as before\n",
    "\n",
    "F2_old = front50[:, [0, 1]]\n",
    "F2_new = front50_new[:, [0, 1]]\n",
    "\n",
    "hv_old = hypervolume_2d(F2_old, ref=ref)\n",
    "hv_new = hypervolume_2d(F2_new, ref=ref)\n",
    "delta_hv = hv_new - hv_old\n",
    "\n",
    "print(f\"HV (old 50) = {hv_old:.6f} | HV (updated 53) = {hv_new:.6f} | Î”HV = {delta_hv:.6f}\")\n",
    "# 3) 2D visualization: overlay old HV region, new HV region, and highlight new points\n",
    "plt.scatter(Y50[:, 0], Y50[:, 1], s=18, alpha=0.25, label=\"all 50\")\n",
    "plt.scatter(front50[:, 0], front50[:, 1], s=28, alpha=0.95, label=\"old Pareto 50\", edgecolor=\"k\")\n",
    "\n",
    "# Updated points and front\n",
    "plt.scatter(Y50_updated[:, 0], Y50_updated[:, 1], s=12, alpha=0.15, label=\"all 53\")\n",
    "plt.scatter(front50_new[:, 0], front50_new[:, 1], s=36, alpha=0.95, label=\"new Pareto 53\", marker='^', edgecolor=\"k\")\n",
    "\n",
    "# Highlight just the three new experiments\n",
    "plt.scatter(new_points[:, 0], new_points[:, 1], s=60, label=\"new experiments\", marker='*')\n",
    "\n",
    "# Fill old HV region\n",
    "xs_old, ys_old = staircase_polygon_2d(F2_old, ref=ref)\n",
    "plt.fill(xs_old, ys_old, alpha=0.15, label=f\"HV old = {hv_old:.4f}\")\n",
    "\n",
    "# Fill new HV region\n",
    "xs_new, ys_new = staircase_polygon_2d(F2_new, ref=ref)\n",
    "plt.fill(xs_new, ys_new, alpha=0.15, label=f\"HV new = {hv_new:.4f}\")\n",
    "\n",
    "plt.scatter([ref[0]], [ref[1]], s=40, marker=\"x\", label=\"reference\")\n",
    "\n",
    "plt.xlabel(\"yield\")\n",
    "plt.ylabel(\"purity\")\n",
    "plt.title(f\"Yield vs Purity: HV increase Î” = {delta_hv:.4f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87912584",
   "metadata": {},
   "source": [
    "From above plot, we see that the **hypervolume increase** equals the additional â€œgoodâ€ region covered by the improved front. Using the same reference point keeps comparisons fair. In synthesis terms, this reflects the *net expansion of acceptable performance space when higher yield and purity conditions are discovered*.\n",
    "\n",
    "We can compare with the full dataset, and this give you an idea of the maxiumn hypervolume you can achieve. But, again, keep in mind that the entire space / synthesis-yield relationship is usually a blackbox function and you won't know the max hypervolume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f043b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Full set Pareto front and 2D plot\n",
    "mask_full = nondominated_mask(Y)\n",
    "front_full = Y[mask_full]\n",
    "\n",
    "F2_full = front_full[:, [0, 1]]\n",
    "hv_full = hypervolume_2d(F2_full, ref=ref)\n",
    "\n",
    "plt.scatter(Y[:, 0], Y[:, 1], s=6, alpha=0.15, label=\"all 20k\")\n",
    "plt.scatter(front_full[:, 0], front_full[:, 1], s=22, alpha=0.95, label=\"Pareto full\", edgecolor=\"k\")\n",
    "\n",
    "xs_full, ys_full = staircase_polygon_2d(F2_full, ref=ref)\n",
    "plt.fill(xs_full, ys_full, alpha=0.2, label=f\"HV full = {hv_full:.4f}\")\n",
    "plt.scatter([ref[0]], [ref[1]], s=40, marker=\"x\", label=\"reference\")\n",
    "\n",
    "plt.xlabel(\"yield\")\n",
    "plt.ylabel(\"purity\")\n",
    "plt.title(\"Yield vs Purity for full 20k experiments\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 3D plot for the full set\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], s=6, alpha=0.1, label=\"all 20k\")\n",
    "ax.scatter(front_full[:, 0], front_full[:, 1], front_full[:, 2], s=24, alpha=0.95, label=\"Pareto full\", edgecolor=\"k\")\n",
    "\n",
    "ax.set_xlabel(\"yield\")\n",
    "ax.set_ylabel(\"purity\")\n",
    "ax.set_zlabel(\"reproducibility\")\n",
    "ax.set_title(\"3D Pareto view for full 20k experiments\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Hypervolume wrt ref {ref}: 50 = {hv_50:.6f}  |  full = {hv_full:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ee123",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Scalarization and surrogate models for each objective\n",
    "\n",
    "\n",
    "A common way to turn multiple objectives into one is **weighted sum**:\n",
    "$$\n",
    "s_w(x) = \\sum_{j=1}^m w_j \\,\\tilde f_j(x)\n",
    "$$\n",
    "with $w_j \\ge 0$ and $\\sum_j w_j = 1$, where $\\tilde f_j$ are normalized objectives. This is fast and easy to implement.\n",
    "\n",
    "> Caution: linear scalarization can miss non-convex parts of the Pareto front. Still, it is a practical baseline and pairs well with BO.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We now assume we only have a 50 point subset in hand. All modeling, selection, and validation in this section uses only `X50` and `Y50`. The idea is simple: fit one surrogate per objective on the training split of these 50 points, evaluate on the held out split, and then use a scalarized acquisition to propose new experiments from the same 50 point design space.\n",
    "\n",
    "\n",
    "### 5.1 Train test split and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y50 = Y[idx_50]\n",
    "X50 = np.asarray(X)[idx_50]  \n",
    "# Train test split within the 50 points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    np.asarray(X50), np.asarray(Y50), test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Standardize features using only training data\n",
    "scaler_X = StandardScaler().fit(X_train)\n",
    "Xtr = scaler_X.transform(X_train)\n",
    "Xte = scaler_X.transform(X_test)\n",
    "\n",
    "Ytr = Y_train.copy()\n",
    "Yte = Y_test.copy()\n",
    "\n",
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c6ae7",
   "metadata": {},
   "source": [
    "### 5.2 Three GPs with Matern on the 50 point split\n",
    "Here we fit one GP per objective on `Xtr, Ytr`. We use a Matern kernel with a small white noise term for numerical stability. Scores are printed on `Xte, Yte`. GPs are flexible and give calibrated uncertainties for acquisition functions. On larger feature sets they can be slow, which is why we also show Random Forests.\n",
    "\n",
    "```{admonition} Note\n",
    "Recall, the kernel is a function that defines the covariance or similarity between data points, determining how the Gaussian Process generalizes from the training data.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd581d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "kernel = C(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=1e-3)\n",
    "\n",
    "gps = []\n",
    "for m in range(Ytr.shape[1]):\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True, n_restarts_optimizer=2, random_state=m)\n",
    "    gpr.fit(Xtr, Ytr[:, m])\n",
    "    gps.append(gpr)\n",
    "\n",
    "for m, name in enumerate([\"yield\", \"purity\", \"reproducibility\"]):\n",
    "    pred = gps[m].predict(Xte)\n",
    "    print(name, \"R2:\", r2_score(Yte[:, m], pred), \"MAE:\", mean_absolute_error(Yte[:, m], pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca6da4",
   "metadata": {},
   "source": [
    "### 5.3 RF trio trained only on the 50 point split\n",
    "\n",
    "Here we train three RF models on `Xtr, Ytr` and score on `Xte, Yte`. RFs scale well and give a simple uncertainty proxy from the spread across trees, which we will use for acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "rfs = []\n",
    "for m in range(Ytr.shape[1]):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=10 + m,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(Xtr, Ytr[:, m])\n",
    "    rfs.append(rf)\n",
    "\n",
    "for m, name in enumerate([\"yield\", \"purity\", \"reproducibility\"]):\n",
    "    pred = rfs[m].predict(Xte)\n",
    "    print(name, \"R2:\", r2_score(Yte[:, m], pred), \"MAE:\", mean_absolute_error(Yte[:, m], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef74db0",
   "metadata": {},
   "source": [
    "We see that due to limited data size, both GP and RF surrogate models have a poor R2 value.\n",
    "\n",
    "```{admonition} Note\n",
    "Use RF surrogates when the dataset is larger or when GP training is slow. RF also gives a simple uncertainty proxy from the spread across trees, which we will use later for bandit style decisions.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Acquisition for multiobjective BO\n",
    "\n",
    "We now simulate planning within the 50 point space. Think of each of the 50 rows as a candidate experiment. At each iteration we will:\n",
    "\n",
    "1) fit the three RF surrogates on the experiments already â€œrunâ€ (a small seed set from the 50),\n",
    "2) evaluate a scalarized acquisition on a candidate cloud sampled from `Xtr`,\n",
    "3) propose the single best next point, and\n",
    "4) emulate an outcome by nearest neighbor lookup inside the same 50 point training pool.\n",
    "\n",
    "This is a pool based simulation that stays within the 50 points you actually have. We also track an approximate 3D hypervolume of the observed set, using a crude Monte Carlo estimate in the unit cube for quick feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for candidate cloud and scalarized EI\n",
    "from typing import Tuple\n",
    "from scipy.stats import norm\n",
    "\n",
    "def candidate_cloud(Xbase: np.ndarray, n: int = 2000, seed: int = 0) -> np.ndarray:\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idx = rng.choice(Xbase.shape[0], size=min(n, Xbase.shape[0]), replace=False)\n",
    "    return Xbase[idx]\n",
    "\n",
    "def rf_mu_sigma(model: RandomForestRegressor, Xc: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    preds = np.stack([est.predict(Xc) for est in model.estimators_], axis=1)\n",
    "    return preds.mean(axis=1), preds.std(axis=1) + 1e-6\n",
    "\n",
    "def ei_from_mu_sigma(mu: np.ndarray, sd: np.ndarray, best: float, xi: float = 0.01) -> np.ndarray:\n",
    "    sd = np.maximum(sd, 1e-12)\n",
    "    z = (mu - best - xi) / sd\n",
    "    return (mu - best - xi) * norm.cdf(z) + sd * norm.pdf(z)\n",
    "\n",
    "def sample_simplex(M: int) -> np.ndarray:\n",
    "    return np.random.dirichlet(alpha=np.ones(M))\n",
    "    \n",
    "def hv_approx_3d(Ym: np.ndarray, ref: np.ndarray, n_ref: int = 4000, seed: int = 0) -> float:\n",
    "    # crude Monte Carlo hypervolume estimate over [ref, 1]^3 for maximization\n",
    "    rng = np.random.RandomState(seed)\n",
    "    U = rng.uniform(low=ref, high=np.array([1, 1, 1]), size=(n_ref, 3))\n",
    "    mask = np.zeros(n_ref, dtype=bool)\n",
    "    for y in Ym[nondominated_mask(Ym)]:\n",
    "        mask |= np.all(U <= y, axis=1)\n",
    "    return mask.mean() * np.prod(1 - ref)\n",
    "\n",
    "# Run a few scalarized BO steps on the 50 point pool\n",
    "\n",
    "rng = np.random.RandomState(7)\n",
    "\n",
    "# Candidate cloud drawn from the 50 point training pool\n",
    "Xc = candidate_cloud(Xtr, n=2000, seed=2)\n",
    "\n",
    "# Start with 8 random seed experiments from the 50 training rows\n",
    "seed_idx = rng.choice(Xtr.shape[0], size=8, replace=False)\n",
    "D_X = Xtr[seed_idx]\n",
    "D_Y = Ytr[seed_idx]\n",
    "\n",
    "history_hv = []\n",
    "ref_point = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "# Track initial HV\n",
    "history_hv.append(hv_approx_3d(D_Y, ref_point, n_ref=5000, seed=1))\n",
    "\n",
    "n_rounds = 3 # you can change it to 15, for html purpose we use 3\n",
    "for t in range(n_rounds):\n",
    "    # Refit RF surrogates on the current observed subset\n",
    "    for m in range(3):\n",
    "        rfs[m].fit(D_X, D_Y[:, m])\n",
    "\n",
    "    # Predict mean and uncertainty on the candidate cloud\n",
    "    mu_list, sd_list = [], []\n",
    "    for m in range(3):\n",
    "        mu_m, sd_m = rf_mu_sigma(rfs[m], Xc)\n",
    "        mu_list.append(mu_m); sd_list.append(sd_m)\n",
    "    MU = np.column_stack(mu_list)\n",
    "    SD = np.column_stack(sd_list)\n",
    "\n",
    "    # Random scalarization weights on the simplex\n",
    "    w = sample_simplex(3)\n",
    "\n",
    "    # Scalarized mean and scalarized EI baseline\n",
    "    g_mu = MU @ w\n",
    "    mu_D = np.column_stack([rfs[m].predict(D_X) for m in range(3)])\n",
    "    best_g = float((mu_D @ w).max())\n",
    "\n",
    "    # Scalarized uncertainty under independence approximation\n",
    "    g_sd = np.sqrt(np.maximum(1e-8, np.sum((SD * w) ** 2, axis=1)))\n",
    "\n",
    "    # EI on scalarized surrogate\n",
    "    acq = ei_from_mu_sigma(g_mu, g_sd, best=best_g, xi=0.01)\n",
    "\n",
    "    # Pick next candidate\n",
    "    j = int(np.argmax(acq))\n",
    "    x_next = Xc[j:j+1]\n",
    "\n",
    "    # Emulate measurement by nearest neighbor in the 50 training pool\n",
    "    k = int(np.argmin(np.linalg.norm(Xtr - x_next, axis=1)))\n",
    "    y_next = Ytr[k:k+1]\n",
    "\n",
    "    # Update observed set\n",
    "    D_X = np.vstack([D_X, x_next])\n",
    "    D_Y = np.vstack([D_Y, y_next])\n",
    "\n",
    "    # Track 3D HV\n",
    "    hv_now = hv_approx_3d(D_Y, ref_point, n_ref=3000, seed=100 + t)\n",
    "    history_hv.append(hv_now)\n",
    "\n",
    "print(\"Observed points:\", D_X.shape[0], \"Final approx HV:\", history_hv[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f9b5f",
   "metadata": {},
   "source": [
    "We seed with a handful of points from `Xtr, Ytr`. Each round we refit the RF trio on the current observed set, score a candidate cloud from `Xtr` using random scalarization with EI, pick the best candidate, and emulate the measurement by nearest neighbor lookup inside `Xtr` to fetch its `Ytr`. Hypervolume in 3D is tracked after each addition.\n",
    "\n",
    "\n",
    "The curve below shows how the approximate 3D hypervolume grows as the loop adds points selected by random scalarization with EI. This is a quick way to judge whether the front is expanding within the 50 point design space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a99fa5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.plot(history_hv, marker=\"o\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"approx hypervolume (3D)\")\n",
    "plt.title(\"Scalarization based BO progress on the 50 point pool\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d1d69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 7. End to end MOBO loop on the MOF dataset\n",
    "\n",
    "We now put everything together as a small, complete multiobjective active learning loop that starts from 50 points and grows by proposing 10 new points per round for 15 rounds. The goal is to expand the Pareto set over yield, purity, and reproducibility while tracking a clear 2D hypervolume on yield and purity.\n",
    "\n",
    "Choices in this section:\n",
    "- Surrogate: one Gaussian Process per objective\n",
    "- Acquisition: random candidate cloud, scalarization with fixed equal weights, and Expected Improvement on the scalarized objective\n",
    "- Weights: fixed at `[0.33, 0.33, 0.33]` for the entire run to keep behavior easy to interpret\n",
    "- Evaluation: adding the selected pool rows directly, which emulates running the real experiments\n",
    "\n",
    "We will show the following results:\n",
    "- Iteration prints with the scalarized EI top scores\n",
    "- The three best proposed conditions per round\n",
    "- A four panel figure with hypervolume trace and per objective best so far curves\n",
    "- A separate plot comparing the yield vs purity Pareto front at the start and at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76dc9d0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# === 7. End to end MOBO with a strict oracle and discrete search space ===\n",
    "from typing import Any, Tuple, Dict, List, Callable\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel\n",
    "from scipy.stats import norm\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Define the discrete reaction space and ligand catalog\n",
    "# ---------------------------\n",
    "\n",
    "# Knob grids (exact discrete levels)\n",
    "TEMP_GRID = [25, 40, 55, 70, 85, 100, 115, 130, 155, 160]                    # 10 levels\n",
    "TIME_GRID = [12, 24, 36, 48, 60, 72, 84, 96, 108, 120]                        # 10 levels\n",
    "CONC_GRID = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50]      # 10 levels\n",
    "DMF_GRID  = [0, 1]                                                             # H2O or DMF\n",
    "\n",
    "# Ligand catalog: (name, smiles, MW, logP, TPSA, n_rings, family)\n",
    "LIGANDS = [\n",
    "    (\"H2BDC\", \"O=C(O)c1ccc(cc1)C(=O)O\", 166.13, 1.2, 75.0, 1, \"BDC\"),\n",
    "    (\"Fumaric acid\", \"O=C(O)/C=C/C(=O)O\", 116.07, -0.6, 75.0, 0, \"Aliphatic diacid\"),\n",
    "    (\"2-methylimidazole\", \"Cc1ncc[nH]1\", 82.10, 0.2, 29.0, 1, \"Azole\"),\n",
    "    (\"Trimesic acid\", \"O=C(O)c1cc(C(=O)O)cc(C(=O)O)c1\", 210.14, 1.4, 112.0, 1, \"Triacid\"),\n",
    "    (\"H2BDC-NH2\", \"Nc1cc(C(=O)O)ccc1C(=O)O\", 181.15, 1.0, 101.0, 1, \"BDC\"),\n",
    "    (\"H2BDC-F\", \"O=C(O)c1cc(F)ccc1C(=O)O\", 184.13, 1.4, 75.0, 1, \"BDC\"),\n",
    "    (\"1,4-NDC\", \"O=C(O)c1ccc2cccc(C(=O)O)c2c1\", 216.19, 2.0, 75.0, 2, \"Naphthalene diacid\"),\n",
    "    (\"2,6-NDC\", \"O=C(O)c1cccc2c1ccc(C(=O)O)c2\", 216.19, 2.0, 75.0, 2, \"Naphthalene diacid\"),\n",
    "    (\"4,4'-BPDC\", \"O=C(O)c1ccc(cc1)-c2ccc(cc2)C(=O)O\", 242.23, 2.5, 75.0, 2, \"Biphenyl diacid\"),\n",
    "    (\"Benzimidazole\", \"c1ccc2[nH]cnc2c1\", 118.14, 1.3, 25.0, 2, \"Azole\"),\n",
    "]\n",
    "SMILES_LIST = [s for (_, s, *_rest) in LIGANDS]\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Validate df matches the discrete space exactly (assertions)\n",
    "# ---------------------------\n",
    "\n",
    "# Ensure df has required columns\n",
    "req_cols = [\"temperature\",\"time_h\",\"concentration_M\",\"solvent_DMF\",\"smiles\",\"yield\",\"purity\",\"reproducibility\"]\n",
    "missing = [c for c in req_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"df missing columns: {missing}\")\n",
    "\n",
    "# Assert discrete levels match\n",
    "assert set(df[\"temperature\"].unique())      <= set(TEMP_GRID), \"df temperature outside allowed grid\"\n",
    "assert set(df[\"time_h\"].unique())           <= set(TIME_GRID), \"df time_h outside allowed grid\"\n",
    "assert set(df[\"concentration_M\"].unique())  <= set(CONC_GRID), \"df concentration_M outside allowed grid\"\n",
    "assert set(df[\"solvent_DMF\"].unique())      <= set(DMF_GRID),  \"df solvent_DMF outside allowed grid\"\n",
    "assert set(df[\"smiles\"].unique())           <= set(SMILES_LIST), \"df smiles not in ligand list\"\n",
    "\n",
    "# check full cross count\n",
    "assert len(df) == 20000\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Build search_space and the oracle from df\n",
    "#    search_space: list of valid tuples (T, time, conc, dmf, smiles)\n",
    "#    run_experiment(choice): returns (yield, purity, reproducibility)\n",
    "# ---------------------------\n",
    "\n",
    "# Build mapping from key -> outcomes using df (ground truth)\n",
    "key_to_y: Dict[Tuple[Any,Any,Any,Any,Any], Tuple[float,float,float]] = {}\n",
    "for r in df[[\"temperature\",\"time_h\",\"concentration_M\",\"solvent_DMF\",\"smiles\",\"yield\",\"purity\",\"reproducibility\"]].itertuples(index=False, name=None):\n",
    "    T, tt, cc, d, smi, y1, y2, y3 = r\n",
    "    key_to_y[(T, tt, cc, d, smi)] = (float(y1), float(y2), float(y3))\n",
    "\n",
    "# Build search_space strictly from the CARTESIAN PRODUCT of allowed levels and ligands\n",
    "# and filter to keys that actually exist in df (robust to any missing combos)\n",
    "search_space: List[Tuple[Any,Any,Any,Any,Any]] = []\n",
    "for T, tt, cc, d, smi in product(TEMP_GRID, TIME_GRID, CONC_GRID, DMF_GRID, SMILES_LIST):\n",
    "    key = (T, tt, cc, d, smi)\n",
    "    if key in key_to_y:\n",
    "        search_space.append(key)\n",
    "\n",
    "# Oracle the BO will call. No direct df reads elsewhere.\n",
    "def run_experiment(choice: Tuple[Any,Any,Any,Any,Any]) -> Tuple[float,float,float]:\n",
    "    \"\"\"\n",
    "    Accepts only a 5-tuple:\n",
    "      (temperature, time_h, concentration_M, solvent_DMF, smiles)\n",
    "    Returns the true outcomes (yield, purity, reproducibility).\n",
    "    Raises KeyError if choice is not in the dataset-backed search space.\n",
    "    \"\"\"\n",
    "    return key_to_y[choice]\n",
    "\n",
    "# Build a map from choice -> position in search_space (for indexing)\n",
    "key_to_pos: Dict[Tuple[Any,Any,Any,Any,Any], int] = {\n",
    "    key: i for i, key in enumerate(search_space)\n",
    "}\n",
    "\n",
    "# Map your df row indices in idx_50 to positions in search_space\n",
    "initial_idx = np.array([\n",
    "    key_to_pos[\n",
    "        (df.loc[i,\"temperature\"], df.loc[i,\"time_h\"], df.loc[i,\"concentration_M\"], df.loc[i,\"solvent_DMF\"], df.loc[i,\"smiles\"])\n",
    "    ] for i in idx_50\n",
    "], dtype=int)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Featurization of choices (no class)\n",
    "# ---------------------------\n",
    "\n",
    "# Fit OneHot on smiles across the whole search_space\n",
    "smiles_col = np.array([[key[4]] for key in search_space])  # shape (N, 1)\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "ohe.fit(smiles_col)\n",
    "\n",
    "def choice_to_vector(choice: Tuple[Any,Any,Any,Any,Any]) -> np.ndarray:\n",
    "    T, tt, cc, d, smi = choice\n",
    "    num = np.array([float(T), float(tt), float(cc), float(d)], dtype=float)\n",
    "    smi_vec = ohe.transform(np.array([[smi]]))[0]\n",
    "    return np.concatenate([num, smi_vec], axis=0)\n",
    "\n",
    "def choices_to_matrix(choices: List[Tuple[Any,Any,Any,Any,Any]]) -> np.ndarray:\n",
    "    num_block = np.array([[float(c[0]), float(c[1]), float(c[2]), float(c[3])] for c in choices], dtype=float)\n",
    "    smi_block = ohe.transform(np.array([[c[4]] for c in choices]))\n",
    "    return np.hstack([num_block, smi_block])\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Pareto + hypervolume helpers on (yield, purity)\n",
    "# ---------------------------\n",
    "\n",
    "def nondominated_mask(F: np.ndarray) -> np.ndarray:\n",
    "    F = np.asarray(F)\n",
    "    n = F.shape[0]\n",
    "    mask = np.ones(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if not mask[i]:\n",
    "            continue\n",
    "        dom = ((F >= F[i]).all(axis=1) & (F > F[i]).any(axis=1))\n",
    "        dom[i] = False\n",
    "        if np.any(dom):\n",
    "            mask[i] = False\n",
    "    return mask\n",
    "\n",
    "def hypervolume_2d(front_2d: np.ndarray, ref=(0.0, 0.0)) -> float:\n",
    "    F = front_2d[np.argsort(-front_2d[:, 0])]\n",
    "    y_env = np.maximum.accumulate(F[:, 1])\n",
    "    xs = np.concatenate([F[:, 0], [ref[0]]])\n",
    "    ys = np.concatenate([y_env,   [y_env[-1]]])\n",
    "    widths  = np.maximum(0.0, xs[:-1] - xs[1:])\n",
    "    heights = np.maximum(0.0, ys[:-1] - ref[1])\n",
    "    return float(np.sum(widths * heights))\n",
    "\n",
    "def hv_obs_2d(Y_obs: np.ndarray, ref=(0.0, 0.0)) -> float:\n",
    "    Y2 = Y_obs[:, [0, 1]]\n",
    "    front = Y2[nondominated_mask(Y2)]\n",
    "    return hypervolume_2d(front, ref)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Scalarized EI and candidate sampling\n",
    "# ---------------------------\n",
    "\n",
    "def scalarized_ei(mu: np.ndarray, sd: np.ndarray, w: np.ndarray, y_best: float, xi: float=0.01) -> np.ndarray:\n",
    "    mu_g = (mu * w.reshape(1, -1)).sum(axis=1)\n",
    "    var_g = ((sd**2) * (w.reshape(1, -1)**2)).sum(axis=1)\n",
    "    sd_g = np.sqrt(np.maximum(1e-12, var_g))\n",
    "    z = (mu_g - y_best - xi) / sd_g\n",
    "    return (mu_g - y_best - xi) * norm.cdf(z) + sd_g * norm.pdf(z)\n",
    "\n",
    "def sample_cloud(pool_idx: np.ndarray, n: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    if pool_idx.size <= n:\n",
    "        return pool_idx\n",
    "    rel = rng.choice(pool_idx.size, size=n, replace=False)\n",
    "    return pool_idx[rel]\n",
    "\n",
    "# ---------------------------\n",
    "# 6) MOBO runner using only run_experiment(choice)\n",
    "# ---------------------------\n",
    "\n",
    "def run_mobo_with_oracle(\n",
    "    search_space: List[Tuple[Any,Any,Any,Any,Any]],\n",
    "    run_experiment: Callable[[Tuple[Any,Any,Any,Any,Any]], Tuple[float,float,float]],\n",
    "    initial_idx: np.ndarray,\n",
    "    weights: np.ndarray = np.array([0.33,0.33,0.33]),\n",
    "    rounds: int = 15,\n",
    "    batch: int = 10,\n",
    "    cloud: int = 6000,\n",
    "    ref_2d: Tuple[float,float] = (0.0, 0.0),\n",
    "    rng_seed: int = 11,\n",
    ") -> Dict[str, Any]:\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    w = w / w.sum()\n",
    "\n",
    "    # Observed and pool tracked as positions in search_space\n",
    "    obs_idx = np.array(initial_idx, dtype=int)\n",
    "    pool_idx = np.setdiff1d(np.arange(len(search_space)), obs_idx, assume_unique=False)\n",
    "\n",
    "    # Evaluate oracle for initial 50\n",
    "    X_obs = choices_to_matrix([search_space[i] for i in obs_idx])\n",
    "    Y_obs = np.array([run_experiment(search_space[i]) for i in obs_idx], dtype=float)\n",
    "    Y_start = Y_obs.copy()\n",
    "\n",
    "    hv_trace = [hv_obs_2d(Y_obs, ref=ref_2d)]\n",
    "    best3_log: List[pd.DataFrame] = []\n",
    "    Y_hist: List[np.ndarray] = [Y_obs.copy()]\n",
    "\n",
    "    for t in range(1, rounds + 1):\n",
    "        # Fit GPs on observed\n",
    "        scX = StandardScaler().fit(X_obs)\n",
    "        Xtr = scX.transform(X_obs)\n",
    "\n",
    "        kernel = C(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=1e-4)\n",
    "        gps = []\n",
    "        for j in range(3):\n",
    "            gp = GaussianProcessRegressor(\n",
    "                kernel=kernel, normalize_y=True, n_restarts_optimizer=1, random_state=100 + j\n",
    "            )\n",
    "            gp.fit(Xtr, Y_obs[:, j])\n",
    "            gps.append(gp)\n",
    "\n",
    "        # Candidate cloud from pool indices\n",
    "        if pool_idx.size == 0:\n",
    "            print(f\"Iter {t:02d} | pool empty, stopping.\")\n",
    "            break\n",
    "        cand_abs = sample_cloud(pool_idx, n=min(cloud, pool_idx.size), rng=rng)\n",
    "        Xc = scX.transform(choices_to_matrix([search_space[i] for i in cand_abs]))\n",
    "\n",
    "        # Predict mean and std for each objective\n",
    "        MU = []; SD = []\n",
    "        for j in range(3):\n",
    "            mu_j, sd_j = gps[j].predict(Xc, return_std=True)\n",
    "            MU.append(mu_j); SD.append(sd_j)\n",
    "        MU = np.column_stack(MU)\n",
    "        SD = np.column_stack(SD)\n",
    "\n",
    "        # EI baseline from observed\n",
    "        mu_obs = np.column_stack([gps[j].predict(scX.transform(X_obs)) for j in range(3)])\n",
    "        best_scalar = float((mu_obs @ w).max())\n",
    "\n",
    "        # Scalarized EI with fixed weights\n",
    "        acq = scalarized_ei(MU, SD, w=w, y_best=best_scalar, xi=0.01)\n",
    "\n",
    "        # Pick top-k\n",
    "        k = min(batch, cand_abs.size)\n",
    "        pick_rel = np.argsort(-acq)[:k]\n",
    "        new_abs = cand_abs[pick_rel]\n",
    "\n",
    "        # Log top-3 choices and outcomes via oracle\n",
    "        top3_abs = new_abs[: min(3, new_abs.size)]\n",
    "        top3_choices = [search_space[i] for i in top3_abs]\n",
    "        top3_out = np.array([run_experiment(ch) for ch in top3_choices])\n",
    "        top3_df = pd.DataFrame(\n",
    "            np.column_stack([top3_choices, top3_out]),\n",
    "            columns=[\"temperature\",\"time_h\",\"concentration_M\",\"solvent_DMF\",\"smiles\",\"yield\",\"purity\",\"reproducibility\"]\n",
    "        )\n",
    "        best3_log.append(top3_df)\n",
    "\n",
    "        print(f\"Iter {t:02d} | HV={hv_trace[-1]:.4f} | selected {k}\")\n",
    "        for r_i in range(top3_df.shape[0]):\n",
    "            row = top3_df.iloc[r_i]\n",
    "            print(f\"  #{r_i+1}: temp={row['temperature']}, time_h={row['time_h']}, \"\n",
    "                  f\"conc={row['concentration_M']}, DMF={row['solvent_DMF']}, \"\n",
    "                  f\"yield={float(row['yield']):.3f}, purity={float(row['purity']):.3f}, \"\n",
    "                  f\"repro={float(row['reproducibility']):.3f}\")\n",
    "\n",
    "        # Feedback: call oracle for all selected\n",
    "        X_new = choices_to_matrix([search_space[i] for i in new_abs])\n",
    "        Y_new = np.array([run_experiment(search_space[i]) for i in new_abs], dtype=float)\n",
    "\n",
    "        X_obs = np.vstack([X_obs, X_new])\n",
    "        Y_obs = np.vstack([Y_obs, Y_new])\n",
    "\n",
    "        # Remove selected from pool\n",
    "        mask_keep = ~np.isin(pool_idx, new_abs)\n",
    "        pool_idx = pool_idx[mask_keep]\n",
    "\n",
    "        # Track\n",
    "        hv_trace.append(hv_obs_2d(Y_obs, ref=ref_2d))\n",
    "        Y_hist.append(Y_obs.copy())\n",
    "\n",
    "    return {\n",
    "        \"hv_trace\": hv_trace,\n",
    "        \"best3_log\": best3_log,\n",
    "        \"Y_obs_start\": Y_start,\n",
    "        \"Y_obs_final\": Y_obs,\n",
    "        \"Y_hist\": Y_hist,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacb5f4",
   "metadata": {},
   "source": [
    "Above we define the function (click to see more details) and below we run everything togther with fixed weights, 15 rounds, 10 picks/round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_mobo_with_oracle(\n",
    "    search_space=search_space,\n",
    "    run_experiment=run_experiment,\n",
    "    initial_idx=initial_idx,\n",
    "    weights=np.array([0.33, 0.33, 0.33]),\n",
    "    rounds=20,\n",
    "    batch=10,\n",
    "    cloud=6000,\n",
    "    ref_2d=(0.0, 0.0),\n",
    "    rng_seed=17\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4324b40b",
   "metadata": {},
   "source": [
    "We can plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf891835",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 8) Four-panel progress plot + Pareto before vs after (yield, purity)\n",
    "# ---------------------------\n",
    "\n",
    "hv_trace = results[\"hv_trace\"]\n",
    "Y_hist   = results[\"Y_hist\"]\n",
    "Y_start  = results[\"Y_obs_start\"]\n",
    "Y_final  = results[\"Y_obs_final\"]\n",
    "\n",
    "best_y = [y[:,0].max() for y in Y_hist]\n",
    "best_p = [y[:,1].max() for y in Y_hist]\n",
    "best_r = [y[:,2].max() for y in Y_hist]\n",
    "iters  = np.arange(len(hv_trace))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(11, 8))\n",
    "\n",
    "axes[0,0].plot(iters, hv_trace, marker=\"o\")\n",
    "axes[0,0].set_title(\"Hypervolume on yield, purity\")\n",
    "axes[0,0].set_xlabel(\"iteration\"); axes[0,0].set_ylabel(\"hypervolume\")\n",
    "\n",
    "axes[0,1].plot(iters, best_y, marker=\"o\")\n",
    "axes[0,1].set_title(\"Best yield observed\")\n",
    "axes[0,1].set_xlabel(\"iteration\"); axes[0,1].set_ylabel(\"yield\")\n",
    "\n",
    "axes[1,0].plot(iters, best_p, marker=\"o\")\n",
    "axes[1,0].set_title(\"Best purity observed\")\n",
    "axes[1,0].set_xlabel(\"iteration\"); axes[1,0].set_ylabel(\"purity\")\n",
    "\n",
    "axes[1,1].plot(iters, best_r, marker=\"o\")\n",
    "axes[1,1].set_title(\"Best reproducibility observed\")\n",
    "axes[1,1].set_xlabel(\"iteration\"); axes[1,1].set_ylabel(\"reproducibility\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pareto start vs end on yieldâ€“purity\n",
    "def nondominated_mask_2d(F2: np.ndarray) -> np.ndarray:\n",
    "    F = np.asarray(F2)\n",
    "    n = F.shape[0]\n",
    "    mask = np.ones(n, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if not mask[i]:\n",
    "            continue\n",
    "        dom = ((F >= F[i]).all(axis=1) & (F > F[i]).any(axis=1))\n",
    "        dom[i] = False\n",
    "        if np.any(dom):\n",
    "            mask[i] = False\n",
    "    return mask\n",
    "\n",
    "Y2_start = Y_start[:, [0,1]]\n",
    "Y2_final = Y_final[:, [0,1]]\n",
    "front_start = Y2_start[nondominated_mask_2d(Y2_start)]\n",
    "front_final = Y2_final[nondominated_mask_2d(Y2_final)]\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(Y2_start[:,0], Y2_start[:,1], s=12, alpha=0.6, label=\"all start\")\n",
    "plt.scatter(Y2_final[:,0], Y2_final[:,1], s=12, alpha=0.3, label=\"all final\")\n",
    "plt.scatter(front_start[:,0], front_start[:,1], s=35, label=\"front start\", edgecolor=\"k\")\n",
    "plt.scatter(front_final[:,0], front_final[:,1], s=35, label=\"front final\", marker=\"^\", edgecolor=\"k\")\n",
    "plt.xlabel(\"yield\"); plt.ylabel(\"purity\")\n",
    "plt.title(\"Pareto front before vs after\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1051913",
   "metadata": {},
   "source": [
    "Finally, we want to run the same MOBO loop four times per surrogate type (`GP` and `RF`), one for each acquisition: `EI`, `UCB`, `PI`, and `greedy mean`. The loop proposes only from the discrete `search_space`, gets outcomes only through `run_experiment(choice)`, and never reads targets from the dataframe. We track the **user-defined scalarized value** (fixed weights `[0.33, 0.33, 0.33]`) over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f87a58",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Tuple, List, Dict, Callable\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import norm\n",
    "\n",
    "# ---------- Featurization (same as before, no dataframe access) ----------\n",
    "\n",
    "def build_smiles_ohe(search_space: List[Tuple[Any,Any,Any,Any,Any]]) -> OneHotEncoder:\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    ohe.fit(np.array([[c[4]] for c in search_space]))\n",
    "    return ohe\n",
    "\n",
    "def choices_to_matrix(search_space: List[Tuple[Any,Any,Any,Any,Any]],\n",
    "                      idx: np.ndarray,\n",
    "                      ohe: OneHotEncoder) -> np.ndarray:\n",
    "    sel = [search_space[i] for i in idx]\n",
    "    num = np.array([[float(c[0]), float(c[1]), float(c[2]), float(c[3])] for c in sel], dtype=float)\n",
    "    smi = ohe.transform(np.array([[c[4]] for c in sel]))\n",
    "    return np.hstack([num, smi])\n",
    "\n",
    "# ---------- Scalarization helpers ----------\n",
    "\n",
    "def scalarize(Y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    return (Y * w.reshape(1, -1)).sum(axis=1)\n",
    "\n",
    "def best_scalar_so_far(Y_obs: np.ndarray, w: np.ndarray) -> float:\n",
    "    return float(scalarize(Y_obs, w).max())\n",
    "\n",
    "def best_each_so_far(Y_obs: np.ndarray) -> Tuple[float,float,float]:\n",
    "    return float(Y_obs[:,0].max()), float(Y_obs[:,1].max()), float(Y_obs[:,2].max())\n",
    "\n",
    "# ---------- Acquisition on scalarized stats ----------\n",
    "\n",
    "def acq_scores(mu: np.ndarray, sd: np.ndarray, w: np.ndarray, kind: str, xi: float, kappa: float, y_best: float) -> np.ndarray:\n",
    "    mu_g = (mu * w.reshape(1, -1)).sum(axis=1)\n",
    "    var_g = ((sd**2) * (w.reshape(1, -1)**2)).sum(axis=1)\n",
    "    sd_g = np.sqrt(np.maximum(var_g, 1e-12))\n",
    "\n",
    "    if kind == \"greedy\":\n",
    "        return mu_g\n",
    "    if kind == \"ucb\":\n",
    "        return mu_g + kappa * sd_g\n",
    "    if kind == \"pi\":\n",
    "        z = (mu_g - y_best - xi) / sd_g\n",
    "        return norm.cdf(z)\n",
    "    if kind == \"ei\":\n",
    "        z = (mu_g - y_best - xi) / sd_g\n",
    "        return (mu_g - y_best - xi) * norm.cdf(z) + sd_g * norm.pdf(z)\n",
    "    raise ValueError(\"Unknown acquisition kind\")\n",
    "\n",
    "# ---------- One run returns all four traces ----------\n",
    "\n",
    "def run_once_full_traces(search_space: List[Tuple[Any,Any,Any,Any,Any]],\n",
    "                         run_experiment: Callable[[Tuple[Any,Any,Any,Any,Any]], Tuple[float,float,float]],\n",
    "                         initial_idx: np.ndarray,\n",
    "                         w: np.ndarray,\n",
    "                         rounds: int,\n",
    "                         batch: int,\n",
    "                         cloud: int,\n",
    "                         model_type: str,      # 'gp' or 'rf'\n",
    "                         acq_kind: str,        # 'ei' | 'ucb' | 'pi' | 'greedy'\n",
    "                         rng_seed: int = 0,\n",
    "                         kappa: float = 1.0,\n",
    "                         xi: float = 0.01) -> Dict[str, List[float]]:\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    ohe = build_smiles_ohe(search_space)\n",
    "\n",
    "    obs_idx = np.array(initial_idx, dtype=int)\n",
    "    pool_idx = np.setdiff1d(np.arange(len(search_space)), obs_idx, assume_unique=False)\n",
    "\n",
    "    # Initial oracle eval\n",
    "    X_obs = choices_to_matrix(search_space, obs_idx, ohe)\n",
    "    Y_obs = np.array([run_experiment(search_space[i]) for i in obs_idx], dtype=float)\n",
    "\n",
    "    # Traces\n",
    "    best_scalar = [best_scalar_so_far(Y_obs, w)]\n",
    "    by, bp, br = best_each_so_far(Y_obs)\n",
    "    best_y = [by]; best_p = [bp]; best_r = [br]\n",
    "\n",
    "    for t in range(1, rounds + 1):\n",
    "        scX = StandardScaler().fit(X_obs)\n",
    "        Xtr = scX.transform(X_obs)\n",
    "\n",
    "        # Surrogates\n",
    "        if model_type == \"gp\":\n",
    "            models = []\n",
    "            kernel = C(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=1e-4)\n",
    "            for j in range(3):\n",
    "                gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True, n_restarts_optimizer=1,\n",
    "                                              random_state=100 + j + 17*t)\n",
    "                gp.fit(Xtr, Y_obs[:, j])\n",
    "                models.append(gp)\n",
    "        elif model_type == \"rf\":\n",
    "            models = []\n",
    "            for j in range(3):\n",
    "                rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=3, max_features=\"sqrt\",\n",
    "                                           random_state=200 + j + 13*t, n_jobs=-1)\n",
    "                rf.fit(Xtr, Y_obs[:, j])\n",
    "                models.append(rf)\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'gp' or 'rf'\")\n",
    "\n",
    "        if pool_idx.size == 0:\n",
    "            # record and stop\n",
    "            best_scalar.append(best_scalar_so_far(Y_obs, w))\n",
    "            by, bp, br = best_each_so_far(Y_obs)\n",
    "            best_y.append(by); best_p.append(bp); best_r.append(br)\n",
    "            break\n",
    "\n",
    "        # Candidate cloud from pool\n",
    "        cand_abs = pool_idx if pool_idx.size <= cloud else rng.choice(pool_idx, size=cloud, replace=False)\n",
    "        Xc = scX.transform(choices_to_matrix(search_space, cand_abs, ohe))\n",
    "\n",
    "        # Predict\n",
    "        MU = []; SD = []\n",
    "        for j, m in enumerate(models):\n",
    "            if model_type == \"gp\":\n",
    "                mu_j, sd_j = m.predict(Xc, return_std=True)\n",
    "            else:\n",
    "                preds = np.stack([est.predict(Xc) for est in m.estimators_], axis=1)\n",
    "                mu_j = preds.mean(axis=1)\n",
    "                sd_j = preds.std(axis=1) + 1e-6\n",
    "            MU.append(mu_j); SD.append(sd_j)\n",
    "        MU = np.column_stack(MU)\n",
    "        SD = np.column_stack(SD)\n",
    "\n",
    "        # Acquisition\n",
    "        y_best = best_scalar_so_far(Y_obs, w)\n",
    "        scores = acq_scores(MU, SD, w, kind=acq_kind, xi=xi, kappa=kappa, y_best=y_best)\n",
    "\n",
    "        # Pick and query oracle\n",
    "        k = min(batch, cand_abs.size)\n",
    "        pick_rel = np.argsort(-scores)[:k]\n",
    "        new_abs = cand_abs[pick_rel]\n",
    "\n",
    "        X_new = choices_to_matrix(search_space, new_abs, ohe)\n",
    "        Y_new = np.array([run_experiment(search_space[i]) for i in new_abs], dtype=float)\n",
    "\n",
    "        X_obs = np.vstack([X_obs, X_new])\n",
    "        Y_obs = np.vstack([Y_obs, Y_new])\n",
    "        pool_idx = pool_idx[~np.isin(pool_idx, new_abs)]\n",
    "\n",
    "        # Update traces\n",
    "        best_scalar.append(best_scalar_so_far(Y_obs, w))\n",
    "        by, bp, br = best_each_so_far(Y_obs)\n",
    "        best_y.append(by); best_p.append(bp); best_r.append(br)\n",
    "\n",
    "    return {\n",
    "        \"best_scalar\": best_scalar,\n",
    "        \"best_yield\": best_y,\n",
    "        \"best_purity\": best_p,\n",
    "        \"best_repro\": best_r\n",
    "    }\n",
    "\n",
    "# ---------- Runner and 8-panel plot ----------\n",
    "\n",
    "def compare_rf_gp_acq_8panels(search_space: List[Tuple[Any,Any,Any,Any,Any]],\n",
    "                              run_experiment: Callable[[Tuple[Any,Any,Any,Any,Any]], Tuple[float,float,float]],\n",
    "                              initial_idx: np.ndarray,\n",
    "                              rounds: int = 15,\n",
    "                              batch: int = 10,\n",
    "                              cloud: int = 6000,\n",
    "                              weights: np.ndarray = np.array([0.33, 0.33, 0.33]),\n",
    "                              rng_seed: int = 123,\n",
    "                              kappa: float = 1.0,\n",
    "                              xi: float = 0.01) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    For each surrogateâˆˆ{GP, RF} and acquisitionâˆˆ{EI, UCB, PI, Greedy}, run the loop\n",
    "    and plot one panel per combo. Each panel shows 4 lines:\n",
    "      - best yield so far\n",
    "      - best purity so far\n",
    "      - best reproducibility so far\n",
    "      - best scalarized value so far\n",
    "    \"\"\"\n",
    "    w = np.asarray(weights, dtype=float); w /= w.sum()\n",
    "    acqs = [\"ei\", \"ucb\", \"pi\", \"greedy\"]\n",
    "    surgs = [\"gp\", \"rf\"]\n",
    "\n",
    "    traces: Dict[str, Dict[str, Dict[str, List[float]]]] = {\"gp\": {}, \"rf\": {}}\n",
    "    for acq in acqs:\n",
    "        for mt in surgs:\n",
    "            traces[mt][acq] = run_once_full_traces(\n",
    "                search_space, run_experiment, initial_idx,\n",
    "                w, rounds, batch, cloud,\n",
    "                model_type=mt, acq_kind=acq,\n",
    "                rng_seed=rng_seed, kappa=kappa, xi=xi\n",
    "            )\n",
    "\n",
    "    # 8 panels: 2 rows (GP, RF) x 4 cols (EI, UCB, PI, Greedy)\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(18, 8), sharex=False, sharey=False)\n",
    "    acq_titles = {\"ei\": \"EI\", \"ucb\": \"UCB\", \"pi\": \"PI\", \"greedy\": \"Greedy mean\"}\n",
    "    surg_titles = {\"gp\": \"GP\", \"rf\": \"RF\"}\n",
    "    \n",
    "    # Define palette \n",
    "    colors = {\n",
    "        \"yield\": \"#b6d09e\",          # green\n",
    "        \"purity\": \"#f5cfa8\",         # peach\n",
    "        \"repro\": \"#94cee5\",          # blue\n",
    "        \"scalar\": \"#e9a2a3\"          # pink\n",
    "    }\n",
    "\n",
    "    for row, mt in enumerate(surgs):\n",
    "        for col, acq in enumerate(acqs):\n",
    "            t = traces[mt][acq]\n",
    "            it = np.arange(len(t[\"best_scalar\"]))\n",
    "            ax = axes[row, col]\n",
    "            ax.plot(it, t[\"best_yield\"], marker=\"o\", color=colors[\"yield\"], label=\"best yield\")\n",
    "            ax.plot(it, t[\"best_purity\"], marker=\"s\", color=colors[\"purity\"], label=\"best purity\")\n",
    "            ax.plot(it, t[\"best_repro\"], marker=\"^\", color=colors[\"repro\"], label=\"best reproducibility\")\n",
    "            ax.plot(it, t[\"best_scalar\"], marker=\"D\", color=colors[\"scalar\"], label=\"best scalarized\")\n",
    "            ax.set_title(f\"{surg_titles[mt]} â€” {acq_titles[acq]}\")\n",
    "            ax.set_xlabel(\"iteration\")\n",
    "            ax.set_ylabel(\"value\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            if row == 0 and col == 0:\n",
    "                ax.legend(loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a7a5b",
   "metadata": {},
   "source": [
    "```python\n",
    "traces = compare_rf_gp_acq_8panels(\n",
    "    search_space=search_space,\n",
    "    run_experiment=run_experiment,\n",
    "    initial_idx=initial_idx,   # 50 starting indices in search_space\n",
    "    rounds=15,\n",
    "    batch=10,\n",
    "    cloud=6000,\n",
    "    weights=np.array([0.33, 0.33, 0.33]),\n",
    "    rng_seed=42,\n",
    "    kappa=1.0,\n",
    "    xi=0.01\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacaf13",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=\"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/lec-15-trace1.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3abe4de",
   "metadata": {},
   "source": [
    "Below, we will see when we change the weights, the MOBO will pay attention to objectives with higher weights and results can be different. \n",
    "\n",
    "```python\n",
    "traces2 = compare_rf_gp_acq_8panels(\n",
    "    search_space=search_space,\n",
    "    run_experiment=run_experiment,\n",
    "    initial_idx=initial_idx,   # 50 starting indices in search_space\n",
    "    rounds=15,\n",
    "    batch=10,\n",
    "    cloud=6000,\n",
    "    weights=np.array([0.8, 0.15, 0.05]), #care more on reaction yield\n",
    "    rng_seed=42,\n",
    "    kappa=1.0,\n",
    "    xi=0.01\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757cdebf",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(url=\"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/lec-15-trace2.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8973e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Glossary\n",
    "\n",
    "```{glossary}\n",
    "Pareto dominance\n",
    "  Point $a$ dominates $b$ if $a$ is at least as good on all objectives and strictly better on one.\n",
    "\n",
    "Pareto front\n",
    "  The set of nondominated objective vectors. Moving along the front trades objectives.\n",
    "\n",
    "Scalarization\n",
    "  Combine multiple objectives into one with weights $w_m \\ge 0$ and $\\sum w_m = 1$ to apply single objective methods.\n",
    "\n",
    "Hypervolume\n",
    "  Measure of the volume dominated by the current Pareto front compared to a reference point. Larger is better.\n",
    "\n",
    "Expected Hypervolume Improvement\n",
    "  Acquisition used in multiobjective BO that prefers candidates expected to increase hypervolume.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## 9. In-class activity\n",
    "\n",
    "\n",
    "### Q1. Build a tiny BO â€œcoachâ€ with an LLM \n",
    "\n",
    "Use an LLM (ChatGPT or Claude) as your coding buddy to create a function in Colab or a small UI that runs Bayesian Optimization (BO) on a reaction you choose. You define the variable space and the objectives, feed a few data points, and your tool suggests 3 new experiments. After you run them, you type in the results and the tool updates, ready to suggest 3 more.\n",
    "\n",
    "What you will produce out of this activity:\n",
    "A function that:\n",
    "   - choose the reaction family and define the variable space\n",
    "   - input your measured results\n",
    "   - Output â€œSuggest 3â€ to get the next batch\n",
    "   - Support â€œAdd resultsâ€ to append new data and keep going\n",
    "\n",
    "\n",
    "Suggested scope\n",
    "Pick one of the following case:\n",
    "- Organic reaction screening\n",
    "- Catalysis screening\n",
    "- Liquid nanoparticle synthesis\n",
    "- MOF or polymer synthesis\n",
    "\n",
    "Keep the variable space small at first. For example:\n",
    "- temperature in {25, 50, 75, 100}\n",
    "- time in {1, 2, 4}\n",
    "- solvent flag in {0, 1}\n",
    "- catalyst choice in {A, B, C}\n",
    "Your objectives can be one or more of: yield, particle size, selectivity.\n",
    "\n",
    "Interaction rules for the BO tool\n",
    "- You input 3 measured points â†’ tool suggests 3\n",
    "- You input 9 measured points â†’ tool suggests 3\n",
    "- You can append results anytime and resuggest\n",
    "- Stay within your discrete grid. Suggestions must be valid choices from your space\n",
    "\n",
    "\n",
    "```{admonition} Note\n",
    "Hint: Below are examples of prompts you can give to an LLM to generate code.\n",
    "```\n",
    "\n",
    "\n",
    "**Version** A (simpler):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc909d38",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "**Task**  \n",
    "Write Python code for Google Colab that performs multi-objective Bayesian optimization for chemists using only lists and a pandas DataFrame that the user edits by hand between runs. No widgets. No external UI. The workflow is: user defines variables and objectives, enters any existing experiments in lists, runs a function to get new suggestions, updates the lists with results after running the lab work, then calls the function again.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1) **Environment and packages**\n",
    "- Use only standard Colab-friendly libraries: `numpy`, `pandas`, `itertools`, `scikit-learn` GaussianProcessRegressor and kernels, and `scipy` for acquisition optimization if needed.\n",
    "- Random seed support for reproducibility: a single `seed` parameter.\n",
    "\n",
    "2) **User inputs as plain Python lists**\n",
    "- Independent variables: the user defines a dict named `variables` where each key is a clean name without prefix and each value is a dict with `start`, `end`, `interval`. Example:\n",
    "  python\n",
    "  variables = {\n",
    "      \"temperature\": {\"start\": 50, \"end\": 100, \"interval\": 10},\n",
    "      \"time\": {\"start\": 1, \"end\": 5, \"interval\": 1}\n",
    "  }\n",
    " \n",
    "- Objectives: the user defines a list `objectives` with 1 to 3 names, assumed normalized to 0..1. Example: `objectives = [\"yield\", \"selectivity\"]`.\n",
    "- Objective weights: list `objective_weights`. If 1 objective, weight is [1.0]. If multiple, user provides weights that sum to 1.0. Validate the sum equals 1.0 within a small tolerance.\n",
    "\n",
    "3) **Internal naming scheme**\n",
    "- Code must convert variable names to columns with `var_` prefix and objective names to `obj_` prefix. Example: `temperature` becomes `var_temperature`, `yield` becomes `obj_yield`.\n",
    "- Add a column `iteration` to track status. Use:\n",
    "  - `-1` for not tried\n",
    "  - `1, 2, ...` for completed iterations\n",
    "  - The next suggested set gets the next integer\n",
    "\n",
    "4) **Space construction**\n",
    "- Build the full Cartesian grid from `variables` based on start, end, interval. Validate that `(end - start)` is divisible by `interval`. Raise a clear error message if not.\n",
    "- Create a DataFrame `df_space` with all `var_...` columns, empty `obj_...` columns, and `iteration` initialized to `-1`.\n",
    "\n",
    "5) **Manual data entry by the user**\n",
    "- The user maintains two Python lists of equal length that describe completed experiments:\n",
    "  - `run_conditions`: list of dicts for variables. Example:\n",
    "    python\n",
    "    run_conditions = [\n",
    "        {\"temperature\": 70, \"time\": 3},\n",
    "        {\"temperature\": 60, \"time\": 4},\n",
    "    ]\n",
    "    \n",
    "  - `run_results`: list of dicts for objectives. Example:\n",
    "\n",
    "    python\n",
    "    run_results = [\n",
    "        {\"yield\": 0.62, \"selectivity\": 0.80},\n",
    "        {\"yield\": 0.55, \"selectivity\": 0.83},\n",
    "    ]\n",
    "    \n",
    "- Code must validate that lengths match and all names match defined variables and objectives.\n",
    "\n",
    "6) **Merging runs into the master DataFrame**\n",
    "- A utility function updates `df_space`:\n",
    "  - Finds rows that match each `run_conditions` entry on all `var_...` columns.\n",
    "  - Writes the `obj_...` values.\n",
    "  - Sets `iteration` to the current completed iteration number. If no prior data, set to `1`. If prior exists, set to `max(iteration) + 1` only for new rows.\n",
    "- If a run condition does not appear in `df_space`, raise a helpful error that the value is out of grid.\n",
    "\n",
    "7) **Modeling and acquisition**\n",
    "- Default model is Gaussian Process with RBF kernel and WhiteKernel noise term. Use one model per objective.\n",
    "- Scale inputs to 0..1 across each variable dimension before modeling. Keep a small epsilon to avoid zero length.\n",
    "- Acquisition: Expected Improvement on the **weighted scalarized objective**. Steps:\n",
    "  - Fit one GP per objective on completed rows.\n",
    "  - Predict mean and std for each objective across all candidate points with `iteration == -1`.\n",
    "  - Compute a weighted sum of predicted means to get scalar mean. For variance, combine via a simple diagonal approximation by weighting the std terms. Document that this is an approximation.\n",
    "  - Compute EI vs the best observed weighted scalar value among completed rows.\n",
    "- Tie-breaking for equal scores should be stable by index order.\n",
    "\n",
    "8) **Suggestion function**\n",
    "- Provide a main function:\n",
    "  python\n",
    "  def suggest_experiments(variables, objectives, objective_weights, run_conditions, run_results, batch_size=3, seed=123, save_csv=False, csv_path=None):\n",
    "      \n",
    "      #Returns:\n",
    "        #suggestions_df: DataFrame with the next set of suggested experiments (var_ columns only)\n",
    "        #df_space: Updated master DataFrame with iteration values and any new writes\n",
    "      #\n",
    "\n",
    "- Behavior:\n",
    "  - Build or rebuild `df_space` from `variables`.\n",
    "  - Integrate `run_conditions` and `run_results` into `df_space`.\n",
    "  - If there is no completed data yet, pick `batch_size` random points from `iteration == -1` as suggestions and set their `iteration` to `1`. Return suggestions.\n",
    "  - Otherwise, fit GP models as above, compute EI on all `iteration == -1` rows, pick top `batch_size` rows, set their `iteration` to `max(iteration) + 1`, and return suggestions.\n",
    "  - If `save_csv` is True, write `df_space` to `csv_path` if provided, otherwise to `experiment_<timestamp>.csv`.\n",
    "\n",
    "9) **Outputs and instructions**\n",
    "- Print a short summary:\n",
    "  - Number of variables and total grid size\n",
    "  - Number of completed runs found\n",
    "  - Batch size and iteration number suggested\n",
    "- Return `suggestions_df` that shows only `var_...` columns for the user to run in the lab.\n",
    "- Also return the full `df_space` so the user can save or inspect it.\n",
    "\n",
    "10) **Round trip workflow for the user**\n",
    "- First call:\n",
    "  - Define `variables`, `objectives`, `objective_weights`.\n",
    "  - Set `run_conditions = []` and `run_results = []` if starting fresh.\n",
    "  - Call `suggest_experiments(...)` to get initial suggestions.\n",
    "- After the lab:\n",
    "  - Append the new completed conditions to `run_conditions` and the measured results to `run_results`.\n",
    "  - Call `suggest_experiments(...)` again to receive the next suggestions.\n",
    "- Provide a short example section in the notebook with a tiny space and fake results to demonstrate 2 iterations.\n",
    "\n",
    "11) **Validation and helpful errors**\n",
    "- Check weights sum to 1.0 within 1e-6.\n",
    "- Check variable ranges and intervals.\n",
    "- Check names match exactly.\n",
    "- If no available points remain, print a clear message and return empty suggestions.\n",
    "\n",
    "12) **No UI, no widgets, no files required**\n",
    "- Everything runs in cells.\n",
    "- The only persistence is optional CSV save when `save_csv=True`.\n",
    "\n",
    "---\n",
    "\n",
    "### Example usage block to include in the notebook\n",
    "\n",
    "python\n",
    "# 1) Define variables and objectives\n",
    "variables = {\n",
    "    \"temperature\": {\"start\": 50, \"end\": 70, \"interval\": 10},\n",
    "    \"time\": {\"start\": 1, \"end\": 3, \"interval\": 1},\n",
    "}\n",
    "objectives = [\"yield\", \"selectivity\"]\n",
    "objective_weights = [0.6, 0.4]  # sums to 1.0\n",
    "\n",
    "# 2) Start with no completed data\n",
    "run_conditions = []\n",
    "run_results = []\n",
    "\n",
    "# 3) First call: random suggestions\n",
    "suggestions, df_space = suggest_experiments(\n",
    "    variables, objectives, objective_weights,\n",
    "    run_conditions, run_results,\n",
    "    batch_size=2, seed=42, save_csv=False\n",
    ")\n",
    "print(\"Suggested experiments:\")\n",
    "print(suggestions)\n",
    "\n",
    "# 4) Pretend we ran them in the lab, now enter results\n",
    "# Convert suggested rows back to plain dicts with original names\n",
    "run_conditions.extend([\n",
    "    {\"temperature\": int(row[\"var_temperature\"]), \"time\": int(row[\"var_time\"])}\n",
    "    for _, row in suggestions.iterrows()\n",
    "])\n",
    "run_results.extend([\n",
    "    {\"yield\": 0.55, \"selectivity\": 0.78},\n",
    "    {\"yield\": 0.61, \"selectivity\": 0.74},\n",
    "])\n",
    "\n",
    "# 5) Second call: model-based suggestions\n",
    "suggestions, df_space = suggest_experiments(\n",
    "    variables, objectives, objective_weights,\n",
    "    run_conditions, run_results,\n",
    "    batch_size=2, seed=42, save_csv=False\n",
    ")\n",
    "print(\"Next suggestions:\")\n",
    "print(suggestions)\n",
    "\n",
    "\n",
    "Include all functions and imports needed to run this end to end in a fresh Colab session.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f07af",
   "metadata": {},
   "source": [
    "LLM output code from **Version A** prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f6d15",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Utility: validation\n",
    "\n",
    "def _validate_variables(variables: Dict[str, Dict[str, float]]):\n",
    "    if not isinstance(variables, dict) or len(variables) == 0:\n",
    "        raise ValueError(\"`variables` must be a non-empty dict of {name: {start, end, interval}}.\")\n",
    "    for k, v in variables.items():\n",
    "        if not isinstance(v, dict):\n",
    "            raise ValueError(f\"Variable '{k}' must map to a dict with keys start, end, interval.\")\n",
    "        for key in [\"start\", \"end\", \"interval\"]:\n",
    "            if key not in v:\n",
    "                raise ValueError(f\"Variable '{k}' missing key '{key}'.\")\n",
    "        start, end, interval = v[\"start\"], v[\"end\"], v[\"interval\"]\n",
    "        if interval <= 0:\n",
    "            raise ValueError(f\"Variable '{k}' has non-positive interval.\")\n",
    "        diff = end - start\n",
    "        # allow ints or floats that divide cleanly\n",
    "        if abs((diff / interval) - round(diff / interval)) > 1e-9:\n",
    "            raise ValueError(\n",
    "                f\"Variable '{k}' has (end - start) not divisible by interval. \"\n",
    "                f\"Got start={start}, end={end}, interval={interval}.\"\n",
    "            )\n",
    "\n",
    "def _validate_objectives_and_weights(objectives: List[str], objective_weights: List[float]):\n",
    "    if not isinstance(objectives, list) or len(objectives) == 0 or len(objectives) > 3:\n",
    "        raise ValueError(\"`objectives` must be a list with 1 to 3 names.\")\n",
    "    if not isinstance(objective_weights, list) or len(objective_weights) != len(objectives):\n",
    "        raise ValueError(\"`objective_weights` must be a list the same length as `objectives`.\")\n",
    "    wsum = float(np.sum(objective_weights))\n",
    "    if abs(wsum - 1.0) > 1e-6:\n",
    "        raise ValueError(f\"`objective_weights` must sum to 1.0 within 1e-6. Got {wsum}.\")\n",
    "\n",
    "def _validate_run_lists(run_conditions: List[Dict], run_results: List[Dict]):\n",
    "    if not isinstance(run_conditions, list) or not isinstance(run_results, list):\n",
    "        raise ValueError(\"`run_conditions` and `run_results` must be lists.\")\n",
    "    if len(run_conditions) != len(run_results):\n",
    "        raise ValueError(\"`run_conditions` and `run_results` must have equal length.\")\n",
    "\n",
    "# --- Space construction\n",
    "\n",
    "def _build_space_df(variables: Dict[str, Dict[str, float]], objectives: List[str]) -> pd.DataFrame:\n",
    "    _validate_variables(variables)\n",
    "    var_names = list(variables.keys())\n",
    "\n",
    "    # Build per-variable discrete grids\n",
    "    grids = []\n",
    "    for name in var_names:\n",
    "        start = variables[name][\"start\"]\n",
    "        end = variables[name][\"end\"]\n",
    "        interval = variables[name][\"interval\"]\n",
    "        # inclusive end\n",
    "        n_steps = int(round((end - start) / interval)) + 1\n",
    "        grid = [start + i * interval for i in range(n_steps)]\n",
    "        grids.append(grid)\n",
    "\n",
    "    # Cartesian product\n",
    "    all_rows = list(product(*grids))\n",
    "    df = pd.DataFrame(all_rows, columns=[f\"var_{n}\" for n in var_names])\n",
    "\n",
    "    # Add empty objective columns\n",
    "    for obj in objectives:\n",
    "        df[f\"obj_{obj}\"] = np.nan\n",
    "\n",
    "    # Iteration status\n",
    "    df[\"iteration\"] = -1\n",
    "    return df\n",
    "\n",
    "# --- Merge completed runs\n",
    "\n",
    "def _merge_runs_into_space(\n",
    "    df_space: pd.DataFrame,\n",
    "    variables: Dict[str, Dict[str, float]],\n",
    "    objectives: List[str],\n",
    "    run_conditions: List[Dict],\n",
    "    run_results: List[Dict],\n",
    ") -> Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Writes completed runs into df_space.\n",
    "    Returns updated df_space and max_completed_iteration.\n",
    "    \"\"\"\n",
    "    _validate_run_lists(run_conditions, run_results)\n",
    "    var_names = list(variables.keys())\n",
    "    obj_names = list(objectives)\n",
    "\n",
    "    # Validate names in runs\n",
    "    for i, rc in enumerate(run_conditions):\n",
    "        if set(rc.keys()) != set(var_names):\n",
    "            raise ValueError(\n",
    "                f\"run_conditions[{i}] keys must match variables exactly: {var_names}. \"\n",
    "                f\"Got {sorted(rc.keys())}.\"\n",
    "            )\n",
    "    for i, rr in enumerate(run_results):\n",
    "        if set(rr.keys()) != set(obj_names):\n",
    "            raise ValueError(\n",
    "                f\"run_results[{i}] keys must match objectives exactly: {obj_names}. \"\n",
    "                f\"Got {sorted(rr.keys())}.\"\n",
    "            )\n",
    "\n",
    "    if len(run_conditions) == 0:\n",
    "        return df_space, 0\n",
    "\n",
    "    # Assign iteration numbers to completed rows in order of provided runs\n",
    "    next_it = 1\n",
    "    matched_any = False\n",
    "    # Track which rows were already written in this pass to avoid double-assigning\n",
    "    # if the same condition appears twice. Last one wins for objectives, but iteration\n",
    "    # will follow the order provided.\n",
    "    for idx_pair, (rc, rr) in enumerate(zip(run_conditions, run_results), start=1):\n",
    "        # Build a boolean mask that matches all var_ columns\n",
    "        mask = np.ones(len(df_space), dtype=bool)\n",
    "        for vname, vval in rc.items():\n",
    "            mask &= (df_space[f\"var_{vname}\"] == vval)\n",
    "        matches = df_space[mask]\n",
    "        if len(matches) == 0:\n",
    "            raise ValueError(\n",
    "                f\"run_conditions[{idx_pair-1}] does not match any grid row. \"\n",
    "                f\"At least one value is outside the defined grid.\"\n",
    "            )\n",
    "\n",
    "        # If multiple identical rows exist, use the first by index to keep stability\n",
    "        tgt_idx = matches.index[0]\n",
    "        matched_any = True\n",
    "\n",
    "        # Write objectives\n",
    "        for oname, oval in rr.items():\n",
    "            df_space.loc[tgt_idx, f\"obj_{oname}\"] = float(oval)\n",
    "\n",
    "        # Set iteration number in sequential order of provided runs\n",
    "        df_space.loc[tgt_idx, \"iteration\"] = next_it\n",
    "        next_it += 1\n",
    "\n",
    "    max_completed_it = int(df_space[\"iteration\"].clip(lower=0).max()) if matched_any else 0\n",
    "    return df_space, max_completed_it\n",
    "\n",
    "# --- Scaling to [0, 1]\n",
    "\n",
    "def _scale_X(df_rows: pd.DataFrame, variables: Dict[str, Dict[str, float]]) -> np.ndarray:\n",
    "    var_names = list(variables.keys())\n",
    "    X = df_rows[[f\"var_{v}\" for v in var_names]].to_numpy(dtype=float)\n",
    "    mins = np.array([variables[v][\"start\"] for v in var_names], dtype=float)\n",
    "    maxs = np.array([variables[v][\"end\"] for v in var_names], dtype=float)\n",
    "    span = maxs - mins\n",
    "    eps = 1e-12\n",
    "    span = np.where(span < eps, eps, span)\n",
    "    X_scaled = (X - mins) / span\n",
    "    X_scaled = np.clip(X_scaled, 0.0, 1.0)\n",
    "    return X_scaled\n",
    "\n",
    "# --- GP fitting and EI\n",
    "\n",
    "def _fit_gp_models(\n",
    "    X_train: np.ndarray,\n",
    "    Y_list: List[np.ndarray],\n",
    "    seed: int,\n",
    ") -> List[GaussianProcessRegressor]:\n",
    "    models = []\n",
    "    for y in Y_list:\n",
    "        # Drop NaNs if any are present\n",
    "        mask = ~np.isnan(y)\n",
    "        if mask.sum() < 1:\n",
    "            # No data for this objective\n",
    "            models.append(None)\n",
    "            continue\n",
    "        Xt = X_train[mask]\n",
    "        yt = y[mask]\n",
    "        kernel = RBF(length_scale=np.ones(X_train.shape[1])) + WhiteKernel(noise_level=1e-6)\n",
    "        gp = GaussianProcessRegressor(\n",
    "            kernel=kernel,\n",
    "            alpha=0.0,            # use WhiteKernel for noise\n",
    "            normalize_y=True,\n",
    "            random_state=seed,\n",
    "            n_restarts_optimizer=1\n",
    "        )\n",
    "        gp.fit(Xt, yt)\n",
    "        models.append(gp)\n",
    "    return models\n",
    "\n",
    "def _predict_objectives(models: List[GaussianProcessRegressor], X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "    #Returns:\n",
    "      #means: shape (n_points, n_obj)\n",
    "      #stds:  shape (n_points, n_obj)\n",
    "    \n",
    "    n_points = X.shape[0]\n",
    "    n_obj = len(models)\n",
    "    means = np.full((n_points, n_obj), np.nan, dtype=float)\n",
    "    stds = np.full((n_points, n_obj), np.nan, dtype=float)\n",
    "    for j, gp in enumerate(models):\n",
    "        if gp is None:\n",
    "            continue\n",
    "        m, s = gp.predict(X, return_std=True)\n",
    "        means[:, j] = m\n",
    "        stds[:, j] = s\n",
    "    return means, stds\n",
    "\n",
    "def _weighted_scalarization(means: np.ndarray, stds: np.ndarray, weights: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simple diagonal approximation for variance:\n",
    "      mu_s = sum_j w_j * mu_j\n",
    "      sigma_s = sqrt(sum_j (w_j * sigma_j)^2)\n",
    "    \"\"\"\n",
    "    mu_s = np.nansum(means * weights[None, :], axis=1)\n",
    "    sigma_s = np.sqrt(np.nansum((stds * weights[None, :]) ** 2, axis=1))\n",
    "    return mu_s, sigma_s\n",
    "\n",
    "def _expected_improvement(mu: np.ndarray, sigma: np.ndarray, best: float, xi: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    EI against best observed scalar value.\n",
    "    \"\"\"\n",
    "    imp = mu - best - xi\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        Z = np.where(sigma > 0, imp / sigma, 0.0)\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei = np.where(sigma <= 0, 0.0, ei)\n",
    "    # Negative EI not useful\n",
    "    ei = np.maximum(ei, 0.0)\n",
    "    return ei\n",
    "\n",
    "# --- Main API\n",
    "\n",
    "def suggest_experiments(\n",
    "    variables: Dict[str, Dict[str, float]],\n",
    "    objectives: List[str],\n",
    "    objective_weights: List[float],\n",
    "    run_conditions: List[Dict],\n",
    "    run_results: List[Dict],\n",
    "    batch_size: int = 3,\n",
    "    seed: int = 123,\n",
    "    save_csv: bool = False,\n",
    "    csv_path: str = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    #\n",
    "    #Returns:\n",
    "      #suggestions_df: DataFrame of suggested experiments (var_ columns only)\n",
    "      #df_space: full updated DataFrame\n",
    "    #\n",
    "    rng = np.random.default_rng(seed)\n",
    "    _validate_objectives_and_weights(objectives, objective_weights)\n",
    "\n",
    "    # Build space fresh each call\n",
    "    df_space = _build_space_df(variables, objectives)\n",
    "\n",
    "    # Merge completed runs\n",
    "    df_space, max_completed_iteration = _merge_runs_into_space(\n",
    "        df_space, variables, objectives, run_conditions, run_results\n",
    "    )\n",
    "\n",
    "    # Summary numbers\n",
    "    var_cols = [c for c in df_space.columns if c.startswith(\"var_\")]\n",
    "    obj_cols = [f\"obj_{o}\" for o in objectives]\n",
    "    grid_size = len(df_space)\n",
    "    n_vars = len(var_cols)\n",
    "\n",
    "    # Completed rows have iteration > 0 and no NaNs in objectives\n",
    "    completed_mask = (df_space[\"iteration\"] > 0)\n",
    "    for oc in obj_cols:\n",
    "        completed_mask &= ~df_space[oc].isna()\n",
    "    n_completed = int(completed_mask.sum())\n",
    "\n",
    "    # Candidates are rows not yet tried\n",
    "    cand_mask = df_space[\"iteration\"] == -1\n",
    "    n_candidates = int(cand_mask.sum())\n",
    "\n",
    "    if n_candidates == 0:\n",
    "        print(f\"No available points remain. Grid size {grid_size}. Completed runs {n_completed}.\")\n",
    "        return pd.DataFrame(columns=var_cols), df_space\n",
    "\n",
    "    # If no completed data yet, return random suggestions\n",
    "    if n_completed == 0:\n",
    "        choose_k = min(batch_size, n_candidates)\n",
    "        cand_indices = df_space.index[cand_mask].to_numpy()\n",
    "        rng.shuffle(cand_indices)\n",
    "        chosen = cand_indices[:choose_k]\n",
    "        # Assign iteration numbers to suggested rows\n",
    "        next_it = 1  # first iteration of the campaign\n",
    "        for idx in chosen:\n",
    "            df_space.loc[idx, \"iteration\"] = next_it\n",
    "        suggestions_df = df_space.loc[chosen, var_cols].reset_index(drop=True)\n",
    "\n",
    "        print(f\"Variables: {n_vars}, grid size: {grid_size}\")\n",
    "        print(f\"Completed runs found: {n_completed}\")\n",
    "        print(f\"Suggesting batch of {choose_k} for iteration {next_it}\")\n",
    "        if save_csv:\n",
    "            ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            out = csv_path if csv_path else f\"experiment_{ts}.csv\"\n",
    "            df_space.to_csv(out, index=False)\n",
    "            print(f\"Saved df_space to: {out}\")\n",
    "        return suggestions_df, df_space\n",
    "\n",
    "    # Model-based suggestions\n",
    "    # Prepare training and candidate matrices in [0, 1]\n",
    "    X_train = _scale_X(df_space.loc[completed_mask, var_cols], variables)\n",
    "    Y_list = [df_space.loc[completed_mask, f\"obj_{o}\"].to_numpy(dtype=float) for o in objectives]\n",
    "    models = _fit_gp_models(X_train, Y_list, seed=seed)\n",
    "\n",
    "    # Predict on candidates\n",
    "    X_cand = _scale_X(df_space.loc[cand_mask, var_cols], variables)\n",
    "    means, stds = _predict_objectives(models, X_cand)\n",
    "\n",
    "    # Weighted scalarization of predictive distribution\n",
    "    w = np.array(objective_weights, dtype=float)\n",
    "    mu_s, sigma_s = _weighted_scalarization(means, stds, w)\n",
    "\n",
    "    # Best observed weighted scalar on completed data\n",
    "    completed_means = np.vstack([df_space.loc[completed_mask, f\"obj_{o}\"].to_numpy(dtype=float) for o in objectives]).T\n",
    "    best_scalar = float((completed_means * w[None, :]).sum(axis=1).max())\n",
    "\n",
    "    # EI on candidates\n",
    "    ei = _expected_improvement(mu_s, sigma_s, best_scalar, xi=0.0)\n",
    "\n",
    "    # Stable tie-breaking by original index order\n",
    "    cand_idx = df_space.index[cand_mask].to_numpy()\n",
    "    order = np.lexsort((-ei, cand_idx))  # sort by EI desc, then index asc\n",
    "    chosen = cand_idx[order][:min(batch_size, len(cand_idx))]\n",
    "\n",
    "    # Assign next iteration number to suggestions\n",
    "    next_it = int(max_completed_iteration) + 1 if max_completed_iteration > 0 else 1\n",
    "    for idx in chosen:\n",
    "        df_space.loc[idx, \"iteration\"] = next_it\n",
    "\n",
    "    suggestions_df = df_space.loc[chosen, var_cols].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Variables: {n_vars}, grid size: {grid_size}\")\n",
    "    print(f\"Completed runs found: {n_completed}\")\n",
    "    print(f\"Suggesting batch of {len(chosen)} for iteration {next_it}\")\n",
    "\n",
    "    if save_csv:\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out = csv_path if csv_path else f\"experiment_{ts}.csv\"\n",
    "        df_space.to_csv(out, index=False)\n",
    "        print(f\"Saved df_space to: {out}\")\n",
    "\n",
    "    return suggestions_df, df_space\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "  ###EXAMPLE USE#####  \n",
    "# 1) Define variables and objectives\n",
    "variables = {\n",
    "    \"temperature\": {\"start\": 50, \"end\": 70, \"interval\": 10},\n",
    "    \"time\": {\"start\": 1, \"end\": 3, \"interval\": 1},\n",
    "}\n",
    "objectives = [\"yield\", \"selectivity\"]\n",
    "objective_weights = [0.6, 0.4]  # sums to 1.0\n",
    "\n",
    "# 2) Start with no completed data\n",
    "run_conditions = []\n",
    "run_results = []\n",
    "\n",
    "# 3) First call: random suggestions\n",
    "suggestions, df_space = suggest_experiments(\n",
    "    variables, objectives, objective_weights,\n",
    "    run_conditions, run_results,\n",
    "    batch_size=2, seed=42, save_csv=False\n",
    ")\n",
    "print(\"Suggested experiments:\")\n",
    "print(suggestions)\n",
    "\n",
    "\n",
    "# 4) Pretend we ran them in the lab, now enter results\n",
    "# Convert suggested rows back to plain dicts with original names\n",
    "run_conditions.extend([\n",
    "    {\"temperature\": int(row[\"var_temperature\"]), \"time\": int(row[\"var_time\"])}\n",
    "    for _, row in suggestions.iterrows()\n",
    "])\n",
    "run_results.extend([\n",
    "    {\"yield\": 0.55, \"selectivity\": 0.78},\n",
    "    {\"yield\": 0.61, \"selectivity\": 0.74},\n",
    "])\n",
    "\n",
    "# 5) Second call: model-based suggestions\n",
    "suggestions, df_space = suggest_experiments(\n",
    "    variables, objectives, objective_weights,\n",
    "    run_conditions, run_results,\n",
    "    batch_size=2, seed=42, save_csv=False\n",
    ")\n",
    "print(\"Next suggestions:\")\n",
    "print(suggestions)\n",
    "\n",
    "\n",
    "# THEN you can do third call....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fac678",
   "metadata": {},
   "source": [
    "Now we can also consider **Version B** (more features), with UI and bottons for someone who does not code in your lab to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1e41a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "promptB = \"\"\"\n",
    "\n",
    "**Task:**  \n",
    "In Google Colab, write Python code that creates a **popup-style interactive UI** for *multi-objective Bayesian optimization* for chemists. The tool should guide users through setting up or continuing an optimization experiment with intuitive inputs and automated CSV handling.\n",
    "\n",
    "---\n",
    "\n",
    "### **General Requirements**\n",
    "- Use **Google Colab UI elements** (`ipywidgets` or `gradio` if suitable) to create a popup-style or form-based interface.  \n",
    "- Ensure all UI components are clearly labeled and organized in steps.  \n",
    "- Include inline validation (e.g., weight sum equals 1, valid numeric intervals).  \n",
    "- Save and read CSV files locally in the Colab environment.  \n",
    "- Code must be modular, with functions for UI rendering, CSV generation, data validation, and Bayesian suggestion updates.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Start Screen**\n",
    "When the notebook runs, display a popup or landing panel with **two mode buttons**:\n",
    "1. **Establish New Experiment**\n",
    "2. **Start from Existing Experiment**\n",
    "\n",
    "Each button triggers a separate workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Establish New Experiment Workflow**\n",
    "When the user clicks **\"Establish New Experiment\"**, do the following:\n",
    "\n",
    "#### **2.1 Variable Definition**\n",
    "- Ask for **at least one independent variable** (e.g., temperature, pressure).  \n",
    "  Each variable requires:\n",
    "  - Name input field (e.g., â€œTemperatureâ€)  \n",
    "  - Numeric start value  \n",
    "  - Numeric end value  \n",
    "  - Interval value  \n",
    "- Provide a **â€œ+ Add another variableâ€** button to add more variables dynamically.\n",
    "- Each variable name should automatically be prefixed as `var_<name>` in the output (e.g., `var_temperature`).\n",
    "\n",
    "#### **2.2 Objective Definition**\n",
    "- Ask for **1 to 3 objective variables** (e.g., yield, selectivity).\n",
    "- Each objective name will be automatically prefixed as `obj_<name>`.\n",
    "- All objectives are **assumed to be normalized (0â€“1)** values.\n",
    "- If more than one objective is defined:\n",
    "  - Ask the user to assign **weights** to each.\n",
    "  - Validate that all weights sum to **1.0**.\n",
    "- If only one objective is defined, it automatically receives a weight of **1.0**.\n",
    "\n",
    "#### **2.3 Space and Experiment Setup**\n",
    "Ask for the following:\n",
    "- **Total number of reaction space points** (the total grid size generated from variable intervals).\n",
    "- **Number of experiments to run at a time** (default = 3).\n",
    "- **Whether existing experimental data is available** (Yes/No radio buttons).\n",
    "\n",
    "#### **2.4 Handling Existing Data**\n",
    "- If **Yes**:\n",
    "  - Display a small data entry form (or file upload option) where the user can enter or upload:\n",
    "    - Columns matching variable names (`var_...`)\n",
    "    - Objective values (`obj_...`)\n",
    "  - These entries are assigned **iteration = 1** in the CSV (indicating completed experiments).\n",
    "- If **No**:\n",
    "  - Perform **random sampling** to suggest the first N (default 3) experiments.\n",
    "  - Assign these as **iteration = 1** (indicating suggested experiments).\n",
    "\n",
    "#### **2.5 CSV Generation**\n",
    "- Automatically generate a **CSV file** with the following:\n",
    "  - Columns:\n",
    "    - All `var_...` variables\n",
    "    - All `obj_...` objectives (blank for suggested experiments)\n",
    "    - `iteration` column (integer values)\n",
    "  - Initially fill:\n",
    "    - All possible variable combinations (full space)\n",
    "    - Objective columns as blank\n",
    "    - `iteration` = -1 for untested combinations\n",
    "    - Suggested experiment rows marked with the **largest iteration number**\n",
    "- Save the CSV file locally with a user-defined or auto-generated name (`experiment_<timestamp>.csv`).\n",
    "\n",
    "#### **2.6 Default Algorithm**\n",
    "- Default optimization algorithm: **Gaussian Process (GP)** with **Expected Improvement (EI)**.\n",
    "- Display a summary of suggested experiments to the user before saving.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Start from Existing Experiment Workflow**\n",
    "When the user clicks **â€œStart from Existing Experimentâ€**:\n",
    "- Prompt them to **upload or select an existing local CSV file**.\n",
    "\n",
    "#### **3.1 CSV Parsing**\n",
    "After upload:\n",
    "- Automatically scan the CSV:\n",
    "  - Identify variable columns (`var_...`)\n",
    "  - Identify objective columns (`obj_...`)\n",
    "  - Identify iteration column\n",
    "- Verify:\n",
    "  - At least one `obj_...` column exists.\n",
    "  - The **largest iteration number** corresponds to completed experiments.\n",
    "  - If objective values for the latest iteration are missing, display a **reminder** to fill them before proceeding.\n",
    "\n",
    "#### **3.2 Next Suggestion Generation**\n",
    "- Once validated:\n",
    "  - Fit the **Bayesian model** using completed experiment data.\n",
    "  - Suggest the next N experiments (using the same number as before unless the user specifies otherwise).\n",
    "  - Append the new suggestions to the CSV file.\n",
    "  - Assign these new rows **iteration = (max_iteration + 1)**.\n",
    "  - Display the suggested experiments for user review.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Additional Functional Features**\n",
    "- **Validation Checks:**\n",
    "  - Numeric range consistency (start < end).\n",
    "  - Interval divides the range evenly.\n",
    "  - Objective weight sum = 1.0.\n",
    "  - Variable and objective names cannot repeat.\n",
    "- **UI Elements:**\n",
    "  - Progress indicators between steps.\n",
    "  - Clear success messages after saving CSV.\n",
    "  - Option to reopen the main menu without rerunning the cell.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Data Handling Notes**\n",
    "- All CSV files should include:\n",
    "  \n",
    "  var_xxx | var_yyy | ... | obj_aaa | obj_bbb | iteration\n",
    "  \n",
    "- Default â€œunrunâ€ rows â†’ `iteration = -1`.\n",
    "- Suggested (pending) experiments â†’ next iteration number.\n",
    "- Completed experiments â†’ filled objective values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Suggested Extensions (Optional)**\n",
    "You may include:\n",
    "- Plotting functions (2D or 3D scatter for objectives and variables).  \n",
    "- Export results to Excel.  \n",
    "- Log system to track runs and timestamps.  \n",
    "- Optional parameter for random seed for reproducibility.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1dbde3",
   "metadata": {},
   "source": [
    "LLM output code from **Version B** prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0e0eb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as w\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers: validation\n",
    "# =========================\n",
    "\n",
    "def _is_float(x) -> bool:\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def validate_variables(var_rows: List[Dict]) -> Tuple[bool, str]:\n",
    "    seen = set()\n",
    "    for i, row in enumerate(var_rows, 1):\n",
    "        name = row[\"name\"].strip()\n",
    "        if not name:\n",
    "            return False, f\"Variable {i}: name required\"\n",
    "        if name.lower() in seen:\n",
    "            return False, f\"Variable {i}: duplicate name '{name}'\"\n",
    "        seen.add(name.lower())\n",
    "\n",
    "        if not (_is_float(row[\"start\"]) and _is_float(row[\"end\"]) and _is_float(row[\"step\"])):\n",
    "            return False, f\"Variable {name}: start, end, and interval must be numeric\"\n",
    "\n",
    "        start = float(row[\"start\"])\n",
    "        end = float(row[\"end\"])\n",
    "        step = float(row[\"step\"])\n",
    "\n",
    "        if not start < end:\n",
    "            return False, f\"Variable {name}: start must be less than end\"\n",
    "        if step <= 0:\n",
    "            return False, f\"Variable {name}: interval must be positive\"\n",
    "\n",
    "        span = end - start\n",
    "        k = span / step\n",
    "        if abs(round(k) - k) > 1e-8:\n",
    "            return False, f\"Variable {name}: interval does not divide range evenly. Choose step so (end - start)/step is an integer\"\n",
    "    return True, \"\"\n",
    "\n",
    "def validate_objectives(obj_rows: List[Dict]) -> Tuple[bool, str, np.ndarray]:\n",
    "    if not obj_rows:\n",
    "        return False, \"At least one objective required\", np.array([])\n",
    "    if len(obj_rows) > 3:\n",
    "        return False, \"Maximum 3 objectives\", np.array([])\n",
    "    seen = set()\n",
    "    for i, row in enumerate(obj_rows, 1):\n",
    "        name = row[\"name\"].strip()\n",
    "        if not name:\n",
    "            return False, f\"Objective {i}: name required\", np.array([])\n",
    "        if name.lower() in seen:\n",
    "            return False, f\"Objective {i}: duplicate name '{name}'\", np.array([])\n",
    "        seen.add(name.lower())\n",
    "    # weights\n",
    "    if len(obj_rows) == 1:\n",
    "        weights = np.array([1.0], dtype=float)\n",
    "        return True, \"\", weights\n",
    "\n",
    "    weights = []\n",
    "    for row in obj_rows:\n",
    "        wv = row[\"weight\"]\n",
    "        if not _is_float(wv):\n",
    "            return False, f\"Weight for objective '{row['name']}' must be numeric\", np.array([])\n",
    "        weights.append(float(wv))\n",
    "    weights = np.array(weights, dtype=float)\n",
    "    s = weights.sum()\n",
    "    if not np.isfinite(s):\n",
    "        return False, \"Weights must be finite numbers\", np.array([])\n",
    "    if abs(s - 1.0) > 1e-6:\n",
    "        return False, f\"Weight sum must be 1.0. Current sum = {s:.6f}\", np.array([])\n",
    "    if np.any(weights < 0):\n",
    "        return False, \"Weights must be nonnegative\", np.array([])\n",
    "    return True, \"\", weights\n",
    "\n",
    "def validate_grid_size(var_rows: List[Dict], claimed_total: int) -> Tuple[bool, str, int]:\n",
    "    sizes = []\n",
    "    for row in var_rows:\n",
    "        start = float(row[\"start\"])\n",
    "        end = float(row[\"end\"])\n",
    "        step = float(row[\"step\"])\n",
    "        n = int(round((end - start) / step)) + 1\n",
    "        sizes.append(n)\n",
    "    total = int(np.prod(sizes)) if sizes else 0\n",
    "    if claimed_total is not None and claimed_total > 0 and claimed_total != total:\n",
    "        return False, f\"Computed grid size is {total} but you entered {claimed_total}\", total\n",
    "    return True, \"\", total\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers: space, CSV, IO\n",
    "# =========================\n",
    "\n",
    "def make_grid_dataframe(var_rows: List[Dict]) -> pd.DataFrame:\n",
    "    grids = []\n",
    "    col_names = []\n",
    "    for row in var_rows:\n",
    "        start = float(row[\"start\"])\n",
    "        end = float(row[\"end\"])\n",
    "        step = float(row[\"step\"])\n",
    "        n = int(round((end - start) / step)) + 1\n",
    "        vals = [start + i * step for i in range(n)]\n",
    "        col = f\"var_{row['name'].strip().lower()}\"\n",
    "        col_names.append(col)\n",
    "        grids.append(vals)\n",
    "\n",
    "    if not grids:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_points = list(itertools.product(*grids))\n",
    "    df = pd.DataFrame(all_points, columns=col_names)\n",
    "    return df\n",
    "\n",
    "def add_objective_columns(df: pd.DataFrame, obj_rows: List[Dict]) -> List[str]:\n",
    "    obj_cols = []\n",
    "    for row in obj_rows:\n",
    "        col = f\"obj_{row['name'].strip().lower()}\"\n",
    "        obj_cols.append(col)\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    if \"iteration\" not in df.columns:\n",
    "        df[\"iteration\"] = -1\n",
    "    return obj_cols\n",
    "\n",
    "def save_experiment_csv(df: pd.DataFrame, filename: Optional[str] = None) -> str:\n",
    "    if filename is None or not filename.strip():\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"experiment_{ts}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    return filename\n",
    "\n",
    "def parse_uploaded_csv(content: bytes) -> pd.DataFrame:\n",
    "    return pd.read_csv(io.BytesIO(content))\n",
    "\n",
    "def detect_columns(df: pd.DataFrame) -> Tuple[List[str], List[str], str]:\n",
    "    var_cols = [c for c in df.columns if c.startswith(\"var_\")]\n",
    "    obj_cols = [c for c in df.columns if c.startswith(\"obj_\")]\n",
    "    it_col = \"iteration\" if \"iteration\" in df.columns else None\n",
    "    return var_cols, obj_cols, it_col\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Bayesian Suggestion (GP + EI)\n",
    "# =========================\n",
    "\n",
    "def fit_gp_ei_scorer(X_obs: np.ndarray, y_obs: np.ndarray):\n",
    "    # Kernel: C * Matern + White\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=np.ones(X_obs.shape[1]), length_scale_bounds=(1e-2, 1e3), nu=2.5) + WhiteKernel(noise_level=1e-6, noise_level_bounds=(1e-8, 1e-1))\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, n_restarts_optimizer=3, normalize_y=True, random_state=0)\n",
    "    gp.fit(X_obs, y_obs)\n",
    "\n",
    "    best_y = np.max(y_obs)\n",
    "\n",
    "    def ei(X_cand: np.ndarray) -> np.ndarray:\n",
    "        mu, std = gp.predict(X_cand, return_std=True)\n",
    "        std = np.clip(std, 1e-12, None)\n",
    "        Z = (mu - best_y) / std\n",
    "        from scipy.stats import norm\n",
    "        ei_vals = (mu - best_y) * norm.cdf(Z) + std * norm.pdf(Z)\n",
    "        ei_vals[std < 1e-12] = 0.0\n",
    "        return ei_vals\n",
    "\n",
    "    return ei\n",
    "\n",
    "def suggest_next(df: pd.DataFrame,\n",
    "                 var_cols: List[str],\n",
    "                 obj_cols: List[str],\n",
    "                 n_suggest: int,\n",
    "                 weights: np.ndarray,\n",
    "                 random_seed: Optional[int] = None) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    # Completed rows have all obj columns filled\n",
    "    completed_mask = df[obj_cols].notna().all(axis=1)\n",
    "    untested_mask = df[\"iteration\"] == -1\n",
    "\n",
    "    # If no completed data, pick random untested\n",
    "    if completed_mask.sum() == 0:\n",
    "        cand_idx = np.where(untested_mask)[0]\n",
    "        if len(cand_idx) == 0:\n",
    "            return pd.DataFrame(columns=df.columns)\n",
    "        pick = rng.choice(cand_idx, size=min(n_suggest, len(cand_idx)), replace=False)\n",
    "        return df.iloc[pick]\n",
    "\n",
    "    # Weighted sum objective (objectives assumed normalized 0..1)\n",
    "    Y = df.loc[completed_mask, obj_cols].values.astype(float)\n",
    "    y_obs = (Y * weights).sum(axis=1)\n",
    "    X_obs = df.loc[completed_mask, var_cols].values.astype(float)\n",
    "\n",
    "    # Candidates are untested rows\n",
    "    X_cand_df = df.loc[untested_mask, var_cols]\n",
    "    if X_cand_df.empty:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    # Fit GP and rank by EI\n",
    "    try:\n",
    "        scorer = fit_gp_ei_scorer(X_obs, y_obs)\n",
    "        ei_vals = scorer(X_cand_df.values.astype(float))\n",
    "        order = np.argsort(-ei_vals)\n",
    "        top_idx = X_cand_df.index.values[order[:min(n_suggest, len(order))]]\n",
    "        return df.loc[top_idx]\n",
    "    except Exception:\n",
    "        # Fallback to random if GP fails\n",
    "        cand_idx = X_cand_df.index.values\n",
    "        rng.shuffle(cand_idx)\n",
    "        return df.loc[cand_idx[:min(n_suggest, len(cand_idx))]]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UI components\n",
    "# =========================\n",
    "\n",
    "class ChemBOUI:\n",
    "    def __init__(self):\n",
    "        self.main_box = w.VBox()\n",
    "        self._build_start_screen()\n",
    "        display(self.main_box)\n",
    "\n",
    "    def _build_start_screen(self):\n",
    "        title = w.HTML(\"<h2>Multi-objective Bayesian Optimization</h2>\")\n",
    "        desc = w.HTML(\"Choose a mode to begin\")\n",
    "        new_btn = w.Button(description=\"Establish New Experiment\", button_style=\"primary\", icon=\"plus\")\n",
    "        exist_btn = w.Button(description=\"Start from Existing Experiment\", button_style=\"\", icon=\"upload\")\n",
    "\n",
    "        new_btn.on_click(lambda _: self._new_experiment_flow())\n",
    "        exist_btn.on_click(lambda _: self._existing_experiment_flow())\n",
    "        self.main_menu = w.VBox([title, desc, w.HBox([new_btn, exist_btn])])\n",
    "        self._set_content(self.main_menu)\n",
    "\n",
    "    def _set_content(self, widget):\n",
    "        self.main_box.children = [widget]\n",
    "\n",
    "    def _back_to_menu(self, *_):\n",
    "        self._set_content(self.main_menu)\n",
    "\n",
    "    # ---------- New experiment workflow ----------\n",
    "\n",
    "    def _new_experiment_flow(self):\n",
    "        # Step containers\n",
    "        self.var_rows_ui = []   # each: dict of widgets for a variable row\n",
    "        self.obj_rows_ui = []   # each: dict of widgets for an objective row\n",
    "        self.existing_entries = []  # manual data entry rows\n",
    "\n",
    "        # Step 2.1 Variables\n",
    "        hdr1 = w.HTML(\"<h3>Step 1 of 4: Define variables</h3>\")\n",
    "        self.var_box = w.VBox([])\n",
    "        add_var_btn = w.Button(description=\"+ Add another variable\", icon=\"plus\")\n",
    "        add_var_btn.on_click(lambda _: self._add_variable_row())\n",
    "\n",
    "        # Add initial variable row\n",
    "        self._add_variable_row()\n",
    "\n",
    "        # Step 2.2 Objectives\n",
    "        hdr2 = w.HTML(\"<h3>Step 2 of 4: Define objectives</h3>\")\n",
    "        self.obj_box = w.VBox([])\n",
    "        add_obj_btn = w.Button(description=\"+ Add objective\", icon=\"plus\")\n",
    "        add_obj_btn.on_click(lambda _: self._add_objective_row())\n",
    "        # Add first objective row by default\n",
    "        self._add_objective_row()\n",
    "\n",
    "        # Step 2.3 Space and experiment setup\n",
    "        hdr3 = w.HTML(\"<h3>Step 3 of 4: Space and experiment setup</h3>\")\n",
    "        self.total_points_in = w.BoundedIntText(value=0, min=0, description=\"Total grid points (check)\", layout=w.Layout(width=\"360px\"))\n",
    "        self.batch_in = w.BoundedIntText(value=3, min=1, description=\"Experiments per batch\", layout=w.Layout(width=\"360px\"))\n",
    "        self.use_existing_radio = w.RadioButtons(options=[\"No\", \"Yes\"], description=\"Existing data available?\")\n",
    "        self.seed_in = w.BoundedIntText(value=0, min=0, description=\"Random seed (optional)\", layout=w.Layout(width=\"360px\"))\n",
    "        self.filename_in = w.Text(value=\"\", placeholder=\"experiment_<timestamp>.csv\", description=\"CSV name\", layout=w.Layout(width=\"420px\"))\n",
    "\n",
    "        # Step 2.4 Existing data entry or upload\n",
    "        hdr4 = w.HTML(\"<h3>Step 4 of 4: Existing data (optional)</h3>\")\n",
    "        self.upload_widget = w.FileUpload(accept=\".csv\", multiple=False)\n",
    "        self.manual_add_button = w.Button(description=\"+ Add one completed row manually\", icon=\"edit\")\n",
    "        self.manual_rows_box = w.VBox([])\n",
    "\n",
    "        self.use_existing_radio.observe(self._toggle_existing_data_ui, names=\"value\")\n",
    "        self.manual_add_button.on_click(self._add_manual_completed_row)\n",
    "\n",
    "        # Progress and actions\n",
    "        self.progress = w.IntProgress(value=0, min=0, max=4)\n",
    "        self.status = w.HTML(\"\")\n",
    "        self.next_btn = w.Button(description=\"Generate CSV and suggestions\", button_style=\"success\", icon=\"cogs\")\n",
    "        self.next_btn.on_click(self._finalize_new_experiment)\n",
    "        self.back_btn = w.Button(description=\"Back to main menu\", icon=\"home\")\n",
    "        self.back_btn.on_click(self._back_to_menu)\n",
    "\n",
    "        container = w.VBox([\n",
    "            hdr1, self.var_box, add_var_btn,\n",
    "            w.HTML(\"<hr>\"),\n",
    "            hdr2, self.obj_box, add_obj_btn,\n",
    "            w.HTML(\"<hr>\"),\n",
    "            hdr3, w.HBox([self.total_points_in, self.batch_in]), w.HBox([self.use_existing_radio, self.seed_in]), self.filename_in,\n",
    "            w.HTML(\"<hr>\"),\n",
    "            hdr4,\n",
    "            w.HTML(\"Upload a CSV that matches the variable and objective columns OR add one or more completed rows manually\"),\n",
    "            w.HBox([w.VBox([w.HTML(\"<b>Upload CSV</b>\"), self.upload_widget]),\n",
    "                    w.VBox([w.HTML(\"<b>Manual entry</b>\"), self.manual_add_button, self.manual_rows_box])] ),\n",
    "            w.HTML(\"<hr>\"),\n",
    "            self.progress, self.status,\n",
    "            w.HBox([self.next_btn, self.back_btn])\n",
    "        ])\n",
    "        self._set_content(container)\n",
    "        self._toggle_existing_data_ui()\n",
    "\n",
    "    def _add_variable_row(self):\n",
    "        name = w.Text(placeholder=\"e.g., temperature\", description=\"Name\", layout=w.Layout(width=\"280px\"))\n",
    "        start = w.FloatText(value=0.0, description=\"Start\", layout=w.Layout(width=\"200px\"))\n",
    "        end = w.FloatText(value=1.0, description=\"End\", layout=w.Layout(width=\"200px\"))\n",
    "        step = w.FloatText(value=0.1, description=\"Interval\", layout=w.Layout(width=\"200px\"))\n",
    "        rm = w.Button(description=\"Remove\", icon=\"trash\", layout=w.Layout(width=\"120px\"))\n",
    "        row_widgets = dict(name=name, start=start, end=end, step=step, rm=rm)\n",
    "        box = w.HBox([name, start, end, step, rm])\n",
    "\n",
    "        def _rm(_):\n",
    "            self.var_rows_ui.remove(row_widgets)\n",
    "            self.var_box.children = [w.HBox([r[\"name\"], r[\"start\"], r[\"end\"], r[\"step\"], r[\"rm\"]]) for r in self.var_rows_ui]\n",
    "\n",
    "        rm.on_click(_rm)\n",
    "        self.var_rows_ui.append(row_widgets)\n",
    "        self.var_box.children = [w.HBox([r[\"name\"], r[\"start\"], r[\"end\"], r[\"step\"], r[\"rm\"]]) for r in self.var_rows_ui]\n",
    "\n",
    "    def _add_objective_row(self):\n",
    "        name = w.Text(placeholder=\"e.g., yield\", description=\"Name\", layout=w.Layout(width=\"280px\"))\n",
    "        weight = w.FloatText(value=np.nan, description=\"Weight\", layout=w.Layout(width=\"200px\"))\n",
    "        rm = w.Button(description=\"Remove\", icon=\"trash\", layout=w.Layout(width=\"120px\"))\n",
    "\n",
    "        row_widgets = dict(name=name, weight=weight, rm=rm)\n",
    "        if len(self.obj_rows_ui) == 0:\n",
    "            weight.value = 1.0  # first objective default if only one\n",
    "        box = w.HBox([name, weight, rm])\n",
    "\n",
    "        def _rm(_):\n",
    "            self.obj_rows_ui.remove(row_widgets)\n",
    "            self.obj_box.children = [w.HBox([r[\"name\"], r[\"weight\"], r[\"rm\"]]) for r in self.obj_rows_ui]\n",
    "\n",
    "        rm.on_click(_rm)\n",
    "        self.obj_rows_ui.append(row_widgets)\n",
    "        self.obj_box.children = [w.HBox([r[\"name\"], r[\"weight\"], r[\"rm\"]]) for r in self.obj_rows_ui]\n",
    "\n",
    "    def _toggle_existing_data_ui(self, *_):\n",
    "        use_existing = self.use_existing_radio.value == \"Yes\"\n",
    "        self.upload_widget.disabled = not use_existing\n",
    "        self.manual_add_button.disabled = not use_existing\n",
    "        for child in self.manual_rows_box.children:\n",
    "            child.layout.display = \"flex\" if use_existing else \"none\"\n",
    "\n",
    "    def _add_manual_completed_row(self, *_):\n",
    "        if not self.var_rows_ui or not self.obj_rows_ui:\n",
    "            self.status.value = \"<span style='color:red'>Add variables and objectives first</span>\"\n",
    "            return\n",
    "        # Build inputs based on current names\n",
    "        var_names = [f\"var_{r['name'].value.strip().lower()}\" for r in self.var_rows_ui if r[\"name\"].value.strip()]\n",
    "        obj_names = [f\"obj_{r['name'].value.strip().lower()}\" for r in self.obj_rows_ui if r[\"name\"].value.strip()]\n",
    "\n",
    "        row_widgets = {}\n",
    "        row_items = []\n",
    "        for vn in var_names:\n",
    "            row_widgets[vn] = w.FloatText(description=vn, layout=w.Layout(width=\"220px\"))\n",
    "            row_items.append(row_widgets[vn])\n",
    "        for on in obj_names:\n",
    "            row_widgets[on] = w.BoundedFloatText(description=on, min=0.0, max=1.0, layout=w.Layout(width=\"240px\"))\n",
    "            row_items.append(row_widgets[on])\n",
    "        rm = w.Button(description=\"Remove row\", icon=\"trash\")\n",
    "        row_widgets[\"rm\"] = rm\n",
    "        row_box = w.HBox(row_items + [rm])\n",
    "\n",
    "        def _rm(_):\n",
    "            self.existing_entries.remove(row_widgets)\n",
    "            self.manual_rows_box.children = [self._manual_row_to_box(r) for r in self.existing_entries]\n",
    "\n",
    "        rm.on_click(_rm)\n",
    "        self.existing_entries.append(row_widgets)\n",
    "        self.manual_rows_box.children = [self._manual_row_to_box(r) for r in self.existing_entries]\n",
    "\n",
    "    def _manual_row_to_box(self, row_widgets: Dict) -> w.Widget:\n",
    "        row_items = [row_widgets[k] for k in row_widgets if k != \"rm\"]\n",
    "        return w.HBox(row_items + [row_widgets[\"rm\"]])\n",
    "\n",
    "    def _collect_vars(self) -> List[Dict]:\n",
    "        rows = []\n",
    "        for r in self.var_rows_ui:\n",
    "            rows.append(dict(name=r[\"name\"].value, start=r[\"start\"].value, end=r[\"end\"].value, step=r[\"step\"].value))\n",
    "        return rows\n",
    "\n",
    "    def _collect_objs(self) -> List[Dict]:\n",
    "        rows = []\n",
    "        for r in self.obj_rows_ui:\n",
    "            rows.append(dict(name=r[\"name\"].value, weight=r[\"weight\"].value))\n",
    "        return rows\n",
    "\n",
    "    def _finalize_new_experiment(self, *_):\n",
    "        self.progress.value = 0\n",
    "        self.status.value = \"\"\n",
    "\n",
    "        var_rows = self._collect_vars()\n",
    "        ok, msg = validate_variables(var_rows)\n",
    "        if not ok:\n",
    "            self.status.value = f\"<span style='color:red'>{msg}</span>\"\n",
    "            return\n",
    "        self.progress.value = 1\n",
    "\n",
    "        obj_rows = self._collect_objs()\n",
    "        ok, msg, weights = validate_objectives(obj_rows)\n",
    "        if not ok:\n",
    "            self.status.value = f\"<span style='color:red'>{msg}</span>\"\n",
    "            return\n",
    "        self.progress.value = 2\n",
    "\n",
    "        # Grid and size check\n",
    "        grid_df = make_grid_dataframe(var_rows)\n",
    "        claimed_total = int(self.total_points_in.value) if self.total_points_in.value else None\n",
    "        ok, msg, total = validate_grid_size(var_rows, claimed_total if claimed_total else 0)\n",
    "        if not ok:\n",
    "            self.status.value = f\"<span style='color:orange'>{msg}. You can correct the input or leave Total grid points at 0 to accept {total}</span>\"\n",
    "            return\n",
    "        if claimed_total in (None, 0):\n",
    "            self.total_points_in.value = total\n",
    "\n",
    "        # Add objectives and iteration\n",
    "        obj_cols = add_objective_columns(grid_df, obj_rows)\n",
    "        var_cols = [c for c in grid_df.columns if c.startswith(\"var_\")]\n",
    "        grid_df[\"iteration\"] = -1\n",
    "\n",
    "        n_batch = int(self.batch_in.value)\n",
    "        seed = int(self.seed_in.value) if self.seed_in.value else None\n",
    "\n",
    "        # Load existing completed data if provided\n",
    "        completed_df = pd.DataFrame(columns=grid_df.columns)\n",
    "        if self.use_existing_radio.value == \"Yes\":\n",
    "            # Uploaded CSV\n",
    "            if len(self.upload_widget.value):\n",
    "                up = next(iter(self.upload_widget.value.values()))\n",
    "                try:\n",
    "                    df_up = parse_uploaded_csv(up[\"content\"])\n",
    "                    up_var_cols, up_obj_cols, it_col = detect_columns(df_up)\n",
    "                    # Basic column presence check\n",
    "                    missing_vars = [c for c in var_cols if c not in df_up.columns]\n",
    "                    missing_objs = [c for c in obj_cols if c not in df_up.columns]\n",
    "                    if missing_vars or missing_objs or it_col is None:\n",
    "                        self.status.value = \"<span style='color:red'>Uploaded CSV must include matching var_*, obj_* columns and an iteration column</span>\"\n",
    "                        return\n",
    "                    completed_rows = df_up[up_obj_cols].notna().all(axis=1)\n",
    "                    completed_df = df_up.loc[completed_rows, grid_df.columns.intersection(df_up.columns)]\n",
    "                except Exception as e:\n",
    "                    self.status.value = f\"<span style='color:red'>Failed to parse uploaded CSV: {e}</span>\"\n",
    "                    return\n",
    "\n",
    "            # Manual rows\n",
    "            for row in self.existing_entries:\n",
    "                rec = {c: np.nan for c in grid_df.columns}\n",
    "                rec[\"iteration\"] = 1\n",
    "                try:\n",
    "                    for k, v in row.items():\n",
    "                        if k == \"rm\":\n",
    "                            continue\n",
    "                        rec[k] = float(v.value)\n",
    "                except Exception:\n",
    "                    self.status.value = \"<span style='color:red'>Manual row values must be numeric</span>\"\n",
    "                    return\n",
    "                completed_df = pd.concat([completed_df, pd.DataFrame([rec])], ignore_index=True)\n",
    "\n",
    "            # Merge completed into grid_df where coordinates match\n",
    "            if not completed_df.empty:\n",
    "                # Key on all var columns\n",
    "                grid_df = grid_df.merge(\n",
    "                    completed_df[var_cols + obj_cols + [\"iteration\"]],\n",
    "                    on=var_cols, how=\"left\", suffixes=(\"\", \"_comp\")\n",
    "                )\n",
    "                for oc in obj_cols + [\"iteration\"]:\n",
    "                    c_new = f\"{oc}_comp\"\n",
    "                    if c_new in grid_df.columns:\n",
    "                        grid_df[oc] = np.where(grid_df[c_new].notna(), grid_df[c_new], grid_df[oc])\n",
    "                        grid_df.drop(columns=[c_new], inplace=True)\n",
    "\n",
    "        self.progress.value = 3\n",
    "\n",
    "        # Suggest first batch\n",
    "        # Mark suggested rows with iteration = 1 if none completed, else keep completed ones at their iteration and suggest next\n",
    "        has_completed = grid_df[obj_cols].notna().all(axis=1).any()\n",
    "        current_iter = 1\n",
    "        if has_completed:\n",
    "            # set current_iter as 1 + max completed iteration found in data, else 1\n",
    "            if \"iteration\" in grid_df.columns and grid_df[\"iteration\"].max() >= 1:\n",
    "                current_iter = int(grid_df[\"iteration\"].max()) + 1\n",
    "            else:\n",
    "                current_iter = 2\n",
    "\n",
    "        # Prepare weights from objs\n",
    "        if len(obj_rows) == 1:\n",
    "            weights = np.array([1.0])\n",
    "\n",
    "        suggestions = suggest_next(grid_df.copy(), var_cols, obj_cols, n_batch, weights, random_seed=seed)\n",
    "\n",
    "        if suggestions.empty:\n",
    "            self.status.value = \"<span style='color:orange'>No available untested rows to suggest</span>\"\n",
    "        else:\n",
    "            # Mark iteration for suggested rows\n",
    "            sug_idx = suggestions.index\n",
    "            grid_df.loc[sug_idx, \"iteration\"] = current_iter\n",
    "            # Suggested experiments have blank objectives\n",
    "            for oc in obj_cols:\n",
    "                grid_df.loc[sug_idx, oc] = np.nan\n",
    "\n",
    "        # Save CSV\n",
    "        filename = save_experiment_csv(grid_df, self.filename_in.value)\n",
    "        self.progress.value = 4\n",
    "\n",
    "        # Show summary\n",
    "        top = suggestions[var_cols].copy()\n",
    "        top[\"iteration\"] = current_iter\n",
    "        html = f\"<b>Algorithm</b>: Gaussian Process with Expected Improvement<br>\"\n",
    "        html += f\"<b>Suggested experiments (n={len(top)})</b>:\"\n",
    "        self.status.value = html\n",
    "\n",
    "        out = w.Output()\n",
    "        with out:\n",
    "            display(top.reset_index(drop=True))\n",
    "            print(f\"CSV saved: {filename}\")\n",
    "        done = w.HTML(\"<span style='color:green'>Success. You can run experiments, fill objective values, then use Start from Existing Experiment to continue</span>\")\n",
    "        menu_btn = w.Button(description=\"Return to main menu\", icon=\"home\")\n",
    "        menu_btn.on_click(self._back_to_menu)\n",
    "        self._set_content(w.VBox([self.main_menu.children[0], self.main_menu.children[1], self.progress, self.status, out, done, w.HBox([menu_btn])]))\n",
    "\n",
    "    # ---------- Existing experiment workflow ----------\n",
    "\n",
    "    def _existing_experiment_flow(self):\n",
    "        title = w.HTML(\"<h3>Continue from an existing CSV</h3>\")\n",
    "        self.ex_upload = w.FileUpload(accept=\".csv\", multiple=False)\n",
    "        self.override_n = w.BoundedIntText(value=0, min=0, description=\"Next batch size (0 = use previous)\", layout=w.Layout(width=\"320px\"))\n",
    "        self.seed2 = w.BoundedIntText(value=0, min=0, description=\"Random seed (optional)\", layout=w.Layout(width=\"320px\"))\n",
    "        go = w.Button(description=\"Generate next suggestions\", button_style=\"success\", icon=\"cogs\")\n",
    "        back = w.Button(description=\"Back to main menu\", icon=\"home\")\n",
    "        self.status2 = w.HTML(\"\")\n",
    "        self.progress2 = w.IntProgress(value=0, min=0, max=3)\n",
    "\n",
    "        go.on_click(self._continue_from_csv)\n",
    "        back.on_click(self._back_to_menu)\n",
    "\n",
    "        box = w.VBox([title, self.ex_upload, w.HBox([self.override_n, self.seed2]), self.progress2, self.status2, w.HBox([go, back])])\n",
    "        self._set_content(box)\n",
    "\n",
    "    def _continue_from_csv(self, *_):\n",
    "        self.progress2.value = 0\n",
    "        self.status2.value = \"\"\n",
    "        if not len(self.ex_upload.value):\n",
    "            self.status2.value = \"<span style='color:red'>Upload a CSV first</span>\"\n",
    "            return\n",
    "        try:\n",
    "            up = next(iter(self.ex_upload.value.values()))\n",
    "            df = parse_uploaded_csv(up[\"content\"])\n",
    "        except Exception as e:\n",
    "            self.status2.value = f\"<span style='color:red'>Failed to read CSV: {e}</span>\"\n",
    "            return\n",
    "\n",
    "        var_cols, obj_cols, it_col = detect_columns(df)\n",
    "        if it_col is None:\n",
    "            self.status2.value = \"<span style='color:red'>CSV must contain an iteration column</span>\"\n",
    "            return\n",
    "        if not obj_cols:\n",
    "            self.status2.value = \"<span style='color:red'>CSV must contain at least one obj_* column</span>\"\n",
    "            return\n",
    "\n",
    "        self.progress2.value = 1\n",
    "\n",
    "        # Determine latest completed iteration\n",
    "        # Completed means all obj columns filled for that row\n",
    "        df[\"__completed__\"] = df[obj_cols].notna().all(axis=1)\n",
    "        if df[\"__completed__\"].any():\n",
    "            last_completed_iter = int(df.loc[df[\"__completed__\"], \"iteration\"].max())\n",
    "        else:\n",
    "            last_completed_iter = 0\n",
    "\n",
    "        # Warn if latest iteration rows are missing objective values\n",
    "        max_iter = int(df[\"iteration\"].max())\n",
    "        latest_rows = df[\"iteration\"] == max_iter\n",
    "        latest_have_obj = df.loc[latest_rows, obj_cols].notna().all(axis=1).all()\n",
    "        if not latest_have_obj and max_iter > 0:\n",
    "            self.status2.value = \"<span style='color:orange'>Some rows in the latest iteration are missing objective values. Fill them before continuing for best results</span>\"\n",
    "\n",
    "        # Batch size detection\n",
    "        prev_batch = int((df[\"iteration\"] == max_iter).sum()) if max_iter > 0 else 3\n",
    "        n_batch = int(self.override_n.value) if self.override_n.value > 0 else prev_batch\n",
    "\n",
    "        # Weights resolution\n",
    "        # If only one objective, weight = 1.0. Else attempt to infer from a stored comment is out of scope.\n",
    "        wts = np.ones(len(obj_cols), dtype=float)\n",
    "        if len(obj_cols) > 1:\n",
    "            # Ask user to input weights interactively would complicate this step.\n",
    "            # As a practical path, normalize the column-wise max to estimate relative importance if objectives look scaled.\n",
    "            # But the spec assumes normalized 0..1, so default equal weights unless user specified otherwise earlier.\n",
    "            wts = np.ones(len(obj_cols), dtype=float) / len(obj_cols)\n",
    "\n",
    "        # Suggest next\n",
    "        seed = int(self.seed2.value) if self.seed2.value else None\n",
    "        suggestions = suggest_next(df.copy(), var_cols, obj_cols, n_batch, wts, random_seed=seed)\n",
    "\n",
    "        self.progress2.value = 2\n",
    "\n",
    "        if suggestions.empty:\n",
    "            self.status2.value = \"<span style='color:orange'>No untested rows remain for suggestion</span>\"\n",
    "            return\n",
    "\n",
    "        new_iter = int(max_iter) + 1 if max_iter >= 0 else 1\n",
    "        sug_idx = suggestions.index\n",
    "        df.loc[sug_idx, \"iteration\"] = new_iter\n",
    "        for oc in obj_cols:\n",
    "            df.loc[sug_idx, oc] = np.nan\n",
    "\n",
    "        # Save in place with a timestamped backup\n",
    "        base = up[\"metadata\"][\"name\"]\n",
    "        root, ext = os.path.splitext(base)\n",
    "        out_name = f\"{root}_updated_{datetime.now().strftime('%Y%m%d_%H%M%S')}{ext}\"\n",
    "        df.drop(columns=[\"__completed__\"], inplace=True)\n",
    "        df.to_csv(out_name, index=False)\n",
    "\n",
    "        self.progress2.value = 3\n",
    "\n",
    "        html = f\"<b>Suggested next iteration</b>: {new_iter}<br>\"\n",
    "        html += f\"<b>Algorithm</b>: Gaussian Process with Expected Improvement<br>\"\n",
    "        self.status2.value = html\n",
    "\n",
    "        out = w.Output()\n",
    "        with out:\n",
    "            display(suggestions[var_cols].reset_index(drop=True))\n",
    "            print(f\"CSV saved: {out_name}\")\n",
    "        back = w.Button(description=\"Return to main menu\", icon=\"home\")\n",
    "        back.on_click(self._back_to_menu)\n",
    "        self._set_content(w.VBox([w.HTML(\"<h3>Next suggestions</h3>\"), self.progress2, self.status2, out, back]))\n",
    "\n",
    "\n",
    "# Launch UI\n",
    "ChemBOUI()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Optional extensions\n",
    "# =========================\n",
    "\n",
    "def plot_scatter_2d(df: pd.DataFrame, x: str, y: str, color_by: Optional[str] = None):\n",
    "    #Quick 2D scatter with optional color. Works in Colab output cells.\n",
    "    import matplotlib.pyplot as plt\n",
    "    xvals = df[x].values\n",
    "    yvals = df[y].values\n",
    "    plt.figure()\n",
    "    if color_by and color_by in df.columns:\n",
    "        c = df[color_by].values\n",
    "        plt.scatter(xvals, yvals, c=c)\n",
    "        cb = plt.colorbar()\n",
    "        cb.set_label(color_by)\n",
    "    else:\n",
    "        plt.scatter(xvals, yvals)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.title(\"2D scatter\")\n",
    "    plt.show()\n",
    "\n",
    "def export_to_excel(csv_path: str, xlsx_path: Optional[str] = None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if xlsx_path is None:\n",
    "        root, _ = os.path.splitext(csv_path)\n",
    "        xlsx_path = f\"{root}.xlsx\"\n",
    "    with pd.ExcelWriter(xlsx_path) as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=\"experiment\")\n",
    "    return xlsx_path\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "source_map": [
   12,
   32,
   71,
   110,
   146,
   166,
   171,
   181,
   197,
   202,
   218,
   222,
   238,
   260,
   271,
   276,
   290,
   332,
   336,
   352,
   364,
   430,
   437,
   492,
   498,
   536,
   558,
   576,
   586,
   605,
   611,
   629,
   649,
   745,
   750,
   757,
   779,
   1059,
   1062,
   1075,
   1079,
   1145,
   1149,
   1378,
   1394,
   1398,
   1420,
   1424,
   1494,
   1662,
   1666,
   2059,
   2062,
   2206,
   2211
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}