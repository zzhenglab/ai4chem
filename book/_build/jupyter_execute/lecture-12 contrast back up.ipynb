{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "b0e08385",
=======
   "id": "b635746f",
>>>>>>> 6cda9aa (0929-1)
   "metadata": {},
   "source": [
    "# Lecture 12 — Self-supervised Learning\n",
    " \n",
    "## Sections 6 and 7: Data augmentation and contrastive pretraining for MOF synthesis\n",
    " \n",
    "This handout walks through a compact self-supervised pipeline tailored to metal–organic framework (MOF) synthesis records. We construct chemistry-aware and process-aware augmentations, train a small encoder with a contrastive objective, then evaluate with a frozen linear probe for success prediction.\n",
    " \n",
    "**Design choices**\n",
    "- Work on 100 rows so everything runs quickly on CPU. The same code scales to the full dataset by removing the sampling and tuning epochs and batch size.\n",
    "- Two complementary *views* of each experiment: a **process** view and a **chemistry** view. We apply different perturbations that are plausible under lab variability.\n",
    "- Use a SimCLR-style objective (NT-Xent), which is an instance of InfoNCE. It encourages invariances that match our augmentations.\n",
    " \n",
    "**Notation**\n",
    "- A single experiment is a vector $x \\in \\mathbb{R}^d$ composed of process and chemistry features.\n",
    "- An encoder $f_\\theta$ maps input to an embedding $h = f_\\theta(x) \\in \\mathbb{R}^{d_h}$ and a projection head $g_\\phi$ outputs $z = g_\\phi(h) \\in \\mathbb{R}^{d_z}$ for the contrastive loss.\n",
    "- For each row $i$ we sample two stochastic augmentations to create views $x_i^{(1)}, x_i^{(2)}$ and their projected embeddings $z_i^{(1)}, z_i^{(2)}$.\n",
    " \n",
    "---\n",
    " \n",
    "## 6. Data augmentation for MOF synthesis (tabular + molecular)\n",
    " \n",
    "**Goal.** Build label-free augmentations that keep both **chemistry** and **process** context. We pursue two objectives:\n",
    " \n",
    "1) Create paired *views* of the same experiment for contrastive learning.  \n",
    "2) Create simple synthetic variations for robustness when we later train a small classifier.\n",
    " \n",
    "### 6.1 Load and subset the MOF dataset\n",
    " \n",
    "We take a random subset of 100 rows to keep the runtime short. If your environment blocks network access, replace the URL with a local path.\n",
    " \n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "url = \"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/mof_yield_dataset.csv\"\n",
    "df_full = pd.read_csv(url)\n",
    " \n",
    "# Keep only the columns we need and create a success label for a later linear probe\n",
    "need = [\"linker_smiles\", \"temperature\", \"time_h\", \"concentration_M\", \"solvent_DMF\", \"yield\"]\n",
    "df_full = df_full[need].copy()\n",
    "df_full = df_full.rename(columns={\"yield\": \"yield_pct\"})\n",
    " \n",
    "# Binary success target for later probe: success = (yield > 20)\n",
    "df_full[\"success\"] = (df_full[\"yield_pct\"] > 20).astype(int)\n",
    " \n",
    "# Fixed seed for reproducible selection\n",
    "rng = np.random.default_rng(7)\n",
    "idx = rng.choice(len(df_full), size=100, replace=False)\n",
    "df_small = df_full.iloc[idx].reset_index(drop=True)\n",
    "df_small.head()\n",
    "```\n",
    " \n",
    "We save the 100-row subset for reuse.\n",
    " \n",
    "```python\n",
    "small_url_hint = \"mof_yield_dataset_100.csv\"\n",
    "df_small.to_csv(small_url_hint, index=False)\n",
    "small_url_hint\n",
    "```\n",
    " \n",
    "### 6.2 Descriptor and fingerprint features\n",
    " \n",
    "We compute four light descriptors and a 256-bit Morgan fingerprint from the linker SMILES. If RDKit is not present we fall back to NaN descriptors and zero fingerprints so the notebook is still runnable.\n",
    " \n",
    "```python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "# RDKit guarded import\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors, DataStructs\n",
    "    from rdkit.Chem import rdFingerprintGenerator\n",
    "    RD = True\n",
    "except Exception as e:\n",
    "    RD = False\n",
    "    Chem = None\n",
    "    print(\"RDKit not available in this environment. Molecular descriptors will be set to NaN and fingerprints to zeros.\")\n",
    " \n",
    "def calc_desc4(smiles: str):\n",
    "    if not RD or smiles is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\": Crippen.MolLogP(m),\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(m),\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(m),\n",
    "    })\n",
    " \n",
    "def morgan_bits(smiles: str, n_bits: int = 256, radius: int = 2):\n",
    "    if not RD or smiles is None:\n",
    "        return np.zeros(n_bits, dtype=np.int8)\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return np.zeros(n_bits, dtype=np.int8)\n",
    "    gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "    fp = gen.GetFingerprint(m)\n",
    "    arr = np.zeros((n_bits,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    return arr\n",
    " \n",
    "desc = df_small[\"linker_smiles\"].apply(calc_desc4)\n",
    "FP_BITS = 256\n",
    "fps = np.vstack(df_small[\"linker_smiles\"].apply(lambda s: morgan_bits(s, n_bits=FP_BITS, radius=2)).values)\n",
    " \n",
    "feat = pd.concat([df_small.reset_index(drop=True), desc.reset_index(drop=True)], axis=1)\n",
    "for j in range(FP_BITS):\n",
    "    feat[f\"fp_{j}\"] = fps[:, j].astype(np.int8)\n",
    " \n",
    "feat.head(3)\n",
    "```\n",
    " \n",
    "### 6.3 Split into process and chemistry views\n",
    " \n",
    "For augmentation we split features into two views:\n",
    "- **Process view**: `temperature`, `time_h`, `concentration_M`, `solvent_DMF`  \n",
    "- **Chemistry view**: `[MolWt, LogP, TPSA, NumRings]` plus 256-bit fingerprint\n",
    " \n",
    "We standardize the numeric parts only. Bits stay binary.\n",
    " \n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "proc_cols = [\"temperature\", \"time_h\", \"concentration_M\", \"solvent_DMF\"]\n",
    "chem_cols = [\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"] + [f\"fp_{i}\" for i in range(FP_BITS)]\n",
    " \n",
    "X_proc_raw = feat[proc_cols].astype(float).to_numpy()\n",
    "X_chem_raw = feat[chem_cols].astype(float).to_numpy()\n",
    " \n",
    "sc_proc = StandardScaler().fit(X_proc_raw)\n",
    "X_proc = sc_proc.transform(X_proc_raw)\n",
    " \n",
    "# For chemistry we standardize only the descriptors, not the binary bits\n",
    "sc_chem_desc = StandardScaler().fit(feat[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]])\n",
    "X_desc4 = sc_chem_desc.transform(feat[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]])\n",
    "X_bits  = feat[[f\"fp_{i}\" for i in range(FP_BITS)]].to_numpy(dtype=float)\n",
    " \n",
    "X_chem = np.hstack([X_desc4, X_bits])\n",
    "X_proc.shape, X_chem.shape\n",
    "```\n",
    " \n",
    "### 6.4 Why these augmentations\n",
    " \n",
    "We want augmentations that do not change the underlying identity of the experiment yet still add variability. Think of an invariance set $\\mathcal{T}$ of transformations where $t \\in \\mathcal{T}$ should preserve labels while changing nuisance factors. Contrastive learning tries to make the encoder $f_\\theta$ invariant to such $t$ by pulling the two augmented views together.\n",
    " \n",
    "- **Gaussian jitter (process)**: after standardization, add zero-mean noise to simulate setpoint or measurement variance. If $x$ is standardized, $x' = x + \\epsilon$, $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n",
    "- **Mask-and-impute (process)**: randomly set one standardized dimension to zero (mean in original space), which mimics missing logs.\n",
    "- **Descriptor jitter (chemistry)**: add tiny noise to `[MolWt, LogP, TPSA, NumRings]` to model computational variance or wet-lab tolerance.\n",
    "- **Fingerprint dropout (chemistry)**: independently drop a small fraction of bits to represent imperfect perception and force robustness. For bits $b \\in \\{0,1\\}$, with dropout prob $p$, we set $\\tilde{b} = b \\cdot \\mathbb{1}\\{u > p\\}$ where $u \\sim \\text{Uniform}(0,1)$.\n",
    "- **MixUp (for supervised probe)**: for tabular vectors $x_i, x_j$ we synthesize $\\tilde{x} = \\lambda x_i + (1-\\lambda) x_j$, with $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$. This is not used to create positive pairs since it blends identities.\n",
    " \n",
    "```python\n",
    "from dataclasses import dataclass\n",
    " \n",
    "@dataclass\n",
    "class AugmentParams:\n",
    "    proc_noise_std: float = 0.05         # standard dev for Gaussian jitter in z-space\n",
    "    proc_mask_p: float = 0.05            # chance to mask a column and impute\n",
    "    chem_desc_noise_std: float = 0.02    # tiny noise on 4 descriptors\n",
    "    fp_dropout_p: float = 0.02           # independent bit dropout prob\n",
    " \n",
    "P_WEAK = AugmentParams(proc_noise_std=0.03, proc_mask_p=0.02, chem_desc_noise_std=0.01, fp_dropout_p=0.01)\n",
    "P_STRONG = AugmentParams(proc_noise_std=0.10, proc_mask_p=0.10, chem_desc_noise_std=0.05, fp_dropout_p=0.05)\n",
    "P_DEFAULT = P_WEAK\n",
    "```\n",
    " \n",
    "### 6.5 Augmentation operators\n",
    " \n",
    "We provide two operators:\n",
    "- `augment_for_contrastive`: create two correlated views of the same row  \n",
    "- `mixup_rows`: optional synthetic rows for supervised robustness\n",
    " \n",
    "```python\n",
    "def aug_proc_once(x_proc_z, params: AugmentParams, rng: np.random.Generator):\n",
    "    z = x_proc_z.copy()\n",
    "    # Gaussian jitter\n",
    "    z = z + rng.normal(0.0, params.proc_noise_std, size=z.shape)\n",
    "    # Mask-and-impute\n",
    "    if rng.uniform() < params.proc_mask_p:\n",
    "        j = rng.integers(low=0, high=z.shape[0])\n",
    "        z[j] = 0.0   # zero in standardized space equals mean in original space\n",
    "    return z\n",
    " \n",
    "def aug_chem_once(x_chem, params: AugmentParams, rng: np.random.Generator):\n",
    "    x = x_chem.copy()\n",
    "    # first 4 are descriptors in z-space, remainder are 0/1 fingerprint bits\n",
    "    x[:4] = x[:4] + rng.normal(0.0, params.chem_desc_noise_std, size=4)\n",
    "    bits = x[4:].copy()\n",
    "    if params.fp_dropout_p > 0:\n",
    "        mask = rng.uniform(size=bits.shape[0]) < params.fp_dropout_p\n",
    "        bits[mask] = 0.0\n",
    "    x[4:] = bits\n",
    "    return x\n",
    " \n",
    "def augment_for_contrastive(X_proc_z, X_chem_full, params=P_DEFAULT, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = X_proc_z.shape[0]\n",
    "    # produce two views v1 and v2 for each i\n",
    "    v1_proc = np.vstack([aug_proc_once(X_proc_z[i], params, rng) for i in range(n)])\n",
    "    v2_proc = np.vstack([aug_proc_once(X_proc_z[i], params, rng) for i in range(n)])\n",
    "    v1_chem = np.vstack([aug_chem_once(X_chem_full[i], params, rng) for i in range(n)])\n",
    "    v2_chem = np.vstack([aug_chem_once(X_chem_full[i], params, rng) for i in range(n)])\n",
    "    # concatenate process and chemistry views\n",
    "    v1 = np.hstack([v1_proc, v1_chem])\n",
    "    v2 = np.hstack([v2_proc, v2_chem])\n",
    "    return v1, v2\n",
    " \n",
    "def mixup_rows(X, y=None, alpha=0.4, n_new=100, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    m, d = X.shape\n",
    "    rows = []\n",
    "    ys = []\n",
    "    for _ in range(n_new):\n",
    "        i, j = rng.integers(0, m), rng.integers(0, m)\n",
    "        lam = rng.beta(alpha, alpha)\n",
    "        rows.append(lam * X[i] + (1.0 - lam) * X[j])\n",
    "        if y is not None:\n",
    "            ys.append(lam * y[i] + (1.0 - lam) * y[j])\n",
    "    if y is None:\n",
    "        return np.vstack(rows)\n",
    "    return np.vstack(rows), np.array(ys)\n",
    "```\n",
    " \n",
    "### 6.6 Inspect the effect of augmentation\n",
    " \n",
    "A quick histogram check shows that augmented standardized values remain close to the base distribution.\n",
    " \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "v1, v2 = augment_for_contrastive(X_proc, X_chem, params=P_DEFAULT, seed=123)\n",
    " \n",
    "plt.figure()\n",
    "plt.hist(X_proc[:, 0], bins=20, alpha=0.6)\n",
    "plt.hist(v1[:, 0], bins=20, alpha=0.6)\n",
    "plt.title(\"Process feature 0: original vs augmented\")\n",
    "plt.xlabel(\"z value\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "```\n",
    " \n",
    "We also compare PCA (or t-SNE) embeddings of originals vs one augmented view. Colors indicate success vs failure of the original row.\n",
    " \n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    " \n",
    "def embed_2d(X, use_tsne=False, random_state=0):\n",
    "    if use_tsne:\n",
    "        reducer = TSNE(n_components=2, perplexity=20, learning_rate=\"auto\",\n",
    "                       init=\"pca\", random_state=random_state)\n",
    "    else:\n",
    "        reducer = PCA(n_components=2, random_state=random_state)\n",
    "    return reducer.fit_transform(X)\n",
    " \n",
    "# Build a single augmented view for comparison\n",
    "v1, v2 = augment_for_contrastive(X_proc, X_chem, params=P_DEFAULT, seed=123)\n",
    " \n",
    "X_orig = np.hstack([X_proc, X_chem]).astype(float)\n",
    "X_aug  = v1.astype(float)\n",
    " \n",
    "use_tsne = False  # True to use t-SNE instead of PCA\n",
    "Z0 = embed_2d(X_orig, use_tsne=use_tsne, random_state=0)\n",
    "Z1 = embed_2d(X_aug,  use_tsne=use_tsne, random_state=0)\n",
    " \n",
    "# Success/failure masks for originals (same order as df_small)\n",
    "y = feat[\"success\"].to_numpy().astype(int)\n",
    "m_succ = (y == 1)\n",
    "m_fail = (y == 0)\n",
    " \n",
    "plt.figure()\n",
    "# original\n",
    "plt.scatter(Z0[m_succ,0], Z0[m_succ,1], s=18, color= \"lightblue\", alpha=0.8, marker=\"o\", label=\"orig success\")\n",
    "plt.scatter(Z0[m_fail,0], Z0[m_fail,1], s=18, color= \"blue\", alpha=0.6, marker=\"x\", label=\"orig failure\")\n",
    "# augmented single view (labels align with originals)\n",
    "plt.scatter(Z1[m_succ,0], Z1[m_succ,1], s=16, color= \"yellow\", alpha=0.8, marker=\"^\", label=\"aug success\")\n",
    "plt.scatter(Z1[m_fail,0], Z1[m_fail,1], s=16, color= \"red\",alpha=0.6, marker=\"s\", label=\"aug failure\")\n",
    " \n",
    "plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
    "plt.title((\"t-SNE\" if use_tsne else \"PCA\") + \" of original vs augmented (success vs failure)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    " \n",
    "We can make a larger augmented pool per row and visualize its geometry relative to the originals.\n",
    " \n",
    "```python\n",
    "def make_augmented_pool(Xp, Xc, params=P_DEFAULT, n_aug_per_row=3, seed=0):\n",
    "    # Create n_aug_per_row augmented samples for each original row.\n",
    "    # Returns:\n",
    "    #   X_aug_pool: [n * n_aug_per_row, d] augmented features\n",
    "    #   src_idx:    [n * n_aug_per_row] index of the original row for each augmented sample\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = Xp.shape[0]\n",
    "    rows = []\n",
    "    src = []\n",
    "    for i in range(n):\n",
    "        for _ in range(n_aug_per_row):\n",
    "            xpp = aug_proc_once(Xp[i], params, rng)\n",
    "            xcc = aug_chem_once(Xc[i], params, rng)\n",
    "            rows.append(np.hstack([xpp, xcc]))\n",
    "            src.append(i)\n",
    "    return np.vstack(rows).astype(np.float32), np.array(src, dtype=int)\n",
    " \n",
    "# Example: build 5 augmented copies per row and visualize with PCA\n",
    "X_aug5, src5 = make_augmented_pool(X_proc, X_chem, params=P_DEFAULT, n_aug_per_row=5, seed=7)\n",
    "Z5 = embed_2d(X_aug5, use_tsne=False, random_state=0)\n",
    " \n",
    "# Labels for originals and the 5x pool\n",
    "y = feat[\"success\"].to_numpy().astype(int)\n",
    "m_succ = (y == 1)\n",
    "m_fail = (y == 0)\n",
    " \n",
    "y5 = y[src5]              # labels for each augmented sample via its source row\n",
    "m5_succ = (y5 == 1)\n",
    "m5_fail = (y5 == 0)\n",
    " \n",
    "plt.figure()\n",
    "# original\n",
    "plt.scatter(Z0[m_succ,0], Z0[m_succ,1], s=18, color= \"lightblue\", alpha=0.8, marker=\"o\", label=\"orig success\")\n",
    "plt.scatter(Z0[m_fail,0], Z0[m_fail,1], s=18, color= \"blue\",alpha=0.6, marker=\"x\", label=\"orig failure\")\n",
    "# 5× augmented pool\n",
    "plt.scatter(Z5[m5_succ,0], Z5[m5_succ,1], s=10, color= \"yellow\", alpha=0.45, marker=\"^\", label=\"aug x5 success\")\n",
    "plt.scatter(Z5[m5_fail,0], Z5[m5_fail,1], s=10, color= \"red\", alpha=0.45, marker=\"s\", label=\"aug x5 failure\")\n",
    " \n",
    "plt.xlabel(\"dim 1\"); plt.ylabel(\"dim 2\")\n",
    "plt.title(\"PCA of original vs 5× augmented pool (success vs failure)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    " \n",
    "### 6.7 Math for contrastive pairs\n",
    " \n",
    "For each example $x_i$ we sample two views $x_i^{(1)} = t_1(x_i)$ and $x_i^{(2)} = t_2(x_i)$ with $t_1, t_2 \\sim \\mathcal{T}$. A positive pair is $\\big(x_i^{(1)}, x_i^{(2)}\\big)$. In a batch of size $B$ we get $2B$ projected embeddings\n",
    "\\[\n",
    "z_i^{(1)} = g_\\phi\\!\\left(f_\\theta\\!\\big(x_i^{(1)}\\big)\\right), \\quad\n",
    "z_i^{(2)} = g_\\phi\\!\\left(f_\\theta\\!\\big(x_i^{(2)}\\big)\\right).\n",
    "\\]\n",
    "We use cosine similarity\n",
    "\\[\n",
    "\\operatorname{sim}(u, v) \\;=\\; \\frac{u^\\top v}{\\|u\\|\\;\\|v\\|}\n",
    "\\]\n",
    "and the NT-Xent loss (InfoNCE) with temperature $\\tau > 0$:\n",
    "\\[\n",
    "\\ell(u, v) \\;=\\; -\\log \\frac{\\exp\\!\\big(\\operatorname{sim}(u,v)/\\tau\\big)}\n",
    "{\\sum\\limits_{w \\in \\mathcal{A}(u)} \\exp\\!\\big(\\operatorname{sim}(u,w)/\\tau\\big)}.\n",
    "\\]\n",
    "$\\mathcal{A}(u)$ collects all other embeddings in the batch excluding $u$. The symmetric objective sums over anchors in both views. With sufficient negatives, this loss increases a lower bound on mutual information between the two views and encourages $f_\\theta$ to learn invariances matched to $\\mathcal{T}$.\n",
    " \n",
    "---\n",
    " \n",
    "## 7. Contrastive pretraining and a linear probe\n",
    " \n",
    "We train a tiny MLP encoder $f_\\theta$ on augmented pairs using NT-Xent. We then **freeze** the encoder and fit a logistic regression probe on the frozen features $H$ to predict **success** (yield > 20). The probe is a small supervised head that tests whether the representation transfers to the downstream task.\n",
    " \n",
    "### 7.1 Torch setup and data wrappers\n",
    " \n",
    "```python\n",
    "import math, sys, os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# Pack the base matrix that the encoder consumes\n",
    "X_base = np.hstack([X_proc, X_chem]).astype(np.float32)\n",
    "y_success = feat[\"success\"].to_numpy().astype(np.int64)\n",
    "X_base.shape, y_success.shape\n",
    "```\n",
    " \n",
    "```python\n",
    "class ContrastivePairs(Dataset):\n",
    "    def __init__(self, X_proc, X_chem, params: AugmentParams, seed=0):\n",
    "        self.Xp = X_proc.astype(np.float32)\n",
    "        self.Xc = X_chem.astype(np.float32)\n",
    "        self.params = params\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(seed)\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.Xp.shape[0]\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # on-the-fly augmentations for a single index\n",
    "        x1p = aug_proc_once(self.Xp[idx], self.params, self.rng)\n",
    "        x2p = aug_proc_once(self.Xp[idx], self.params, self.rng)\n",
    "        x1c = aug_chem_once(self.Xc[idx], self.params, self.rng)\n",
    "        x2c = aug_chem_once(self.Xc[idx], self.params, self.rng)\n",
    "        x1 = np.hstack([x1p, x1c]).astype(np.float32)\n",
    "        x2 = np.hstack([x2p, x2c]).astype(np.float32)\n",
    "        return torch.from_numpy(x1), torch.from_numpy(x2)\n",
    " \n",
    "params_train = P_DEFAULT\n",
    "train_set = ContrastivePairs(X_proc, X_chem, params_train, seed=13)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True)\n",
    "len(train_set)\n",
    "```\n",
    " \n",
    "### 7.2 Encoder and projection head\n",
    " \n",
    "The encoder maps the concatenated feature vector to a latent space. The projection head is used for the contrastive objective only. For the linear probe we use the encoder output before the head.\n",
    " \n",
    "```python\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hid=256, out=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid, out),\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    " \n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_dim, hid=256, out=128):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hid),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hid, out),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    " \n",
    "in_dim = X_base.shape[1]\n",
    "enc = MLPEncoder(in_dim, hid=256, out=128).to(device)\n",
    "proj = Projector(128, hid=128, out=64).to(device)\n",
    " \n",
    "sum(p.numel() for p in enc.parameters()), sum(p.numel() for p in proj.parameters())\n",
    "```\n",
    " \n",
    "### 7.3 NT-Xent loss\n",
    " \n",
    "```python\n",
    "def nt_xent(z1, z2, temp=0.2, eps=1e-8):\n",
    "    \"\"\"\n",
    "    z1, z2: tensors of shape [B, D]\n",
    "    returns scalar loss\n",
    "    \"\"\"\n",
    "    B, D = z1.shape\n",
    "    # L2 normalize\n",
    "    z1 = z1 / (z1.norm(dim=1, keepdim=True) + eps)\n",
    "    z2 = z2 / (z2.norm(dim=1, keepdim=True) + eps)\n",
    " \n",
    "    reps = torch.cat([z1, z2], dim=0)                              # [2B, D]\n",
    "    sim = torch.mm(reps, reps.t())                                 # cosine since normalized\n",
    "    # mask self-similarity\n",
    "    mask = torch.eye(2*B, dtype=torch.bool, device=reps.device)\n",
    "    sim = sim.masked_fill(mask, -9e15)\n",
    " \n",
    "    # positives: for i in [0..B-1], pos for i is i+B; for i+B, pos is i\n",
    "    pos = torch.cat([torch.arange(B, 2*B), torch.arange(0, B)]).to(reps.device)\n",
    "    logits = sim / temp\n",
    "    labels = pos\n",
    " \n",
    "    # cross entropy over rows\n",
    "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "    return loss\n",
    "```\n",
    " \n",
    "### 7.4 Train the contrastive model\n",
    " \n",
    "We keep epochs small to stay CPU friendly. Watch the training curve for stability.\n",
    " \n",
    "```python\n",
    "opt = torch.optim.Adam(list(enc.parameters()) + list(proj.parameters()), lr=1e-3, weight_decay=1e-5)\n",
    " \n",
    "EPOCHS = 20\n",
    "loss_hist = []\n",
    " \n",
    "enc.train(); proj.train()\n",
    "for ep in range(EPOCHS):\n",
    "    run = []\n",
    "    for (x1, x2) in train_loader:\n",
    "        x1 = x1.to(device); x2 = x2.to(device)\n",
    "        h1 = enc(x1); h2 = enc(x2)\n",
    "        z1 = proj(h1); z2 = proj(h2)\n",
    "        loss = nt_xent(z1, z2, temp=0.2)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        run.append(loss.item())\n",
    "    loss_hist.append(np.mean(run))\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(loss_hist, marker=\"o\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"contrastive loss\")\n",
    "plt.title(\"NT-Xent training loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    " \n",
    "### 7.5 Build representations and visualize\n",
    " \n",
    "We compute embeddings using the encoder output before the projection head. Then we use PCA for a quick 2D visualization.\n",
    " \n",
    "```python\n",
    "enc.eval()\n",
    "with torch.no_grad():\n",
    "    H = enc(torch.from_numpy(X_base).to(device)).cpu().numpy()\n",
    " \n",
    "from sklearn.decomposition import PCA\n",
    "Z = PCA(n_components=2, random_state=0).fit_transform(H)\n",
    " \n",
    "plt.figure()\n",
    "plt.scatter(Z[:,0], Z[:,1], s=20, alpha=0.8)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Encoder representations (PCA 2D)\")\n",
    "plt.show()\n",
    "```\n",
    " \n",
    "### 7.6 Linear probe: success vs failure\n",
    " \n",
    "Freeze the encoder, then train a logistic regression on the frozen features $H$ using 5-fold stratified splits. We report accuracy, F1, and ROC AUC.\n",
    " \n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    " \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "accs, f1s, aucs = [], [], []\n",
    " \n",
    "for tr, te in skf.split(H, y_success):\n",
    "    clf = LogisticRegression(max_iter=200, solver=\"lbfgs\")\n",
    "    clf.fit(H[tr], y_success[tr])\n",
    "    p = clf.predict(H[te])\n",
    "    proba = clf.predict_proba(H[te])[:,1]\n",
    "    accs.append(accuracy_score(y_success[te], p))\n",
    "    f1s.append(f1_score(y_success[te], p))\n",
    "    try:\n",
    "        aucs.append(roc_auc_score(y_success[te], proba))\n",
    "    except Exception:\n",
    "        aucs.append(np.nan)\n",
    " \n",
    "print(\"Linear probe on frozen encoder\")\n",
    "print(\"acc mean:\", round(np.mean(accs), 3), \"±\", round(np.std(accs), 3))\n",
    "print(\"f1  mean:\", round(np.mean(f1s), 3), \"±\", round(np.std(f1s), 3))\n",
    "print(\"auc mean:\", round(np.nanmean(aucs), 3))\n",
    "```\n",
    " \n",
    "### 7.7 Baseline without pretraining\n",
    " \n",
    "We compare against a logistic regression trained directly on the raw standardized features. This clarifies whether contrastive pretraining helps in small data.\n",
    " \n",
    "```python\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "X_raw = X_base.copy()\n",
    " \n",
    "accs0, f1s0, aucs0 = [], [], []\n",
    "for tr, te in skf.split(X_raw, y_success):\n",
    "    base_clf = LogisticRegression(max_iter=200, solver=\"lbfgs\")\n",
    "    base_clf.fit(X_raw[tr], y_success[tr])\n",
    "    p0 = base_clf.predict(X_raw[te])\n",
    "    proba0 = base_clf.predict_proba(X_raw[te])[:,1]\n",
    "    accs0.append(accuracy_score(y_success[te], p0))\n",
    "    f1s0.append(f1_score(y_success[te], p0))\n",
    "    try:\n",
    "        aucs0.append(roc_auc_score(y_success[te], proba0))\n",
    "    except Exception:\n",
    "        aucs0.append(np.nan)\n",
    " \n",
    "print(\"Baseline logistic on raw features\")\n",
    "print(\"acc mean:\", round(np.mean(accs0), 3), \"±\", round(np.std(accs0), 3))\n",
    "print(\"f1  mean:\", round(np.mean(f1s0), 3), \"±\", round(np.std(f1s0), 3))\n",
    "print(\"auc mean:\", round(np.nanmean(aucs0), 3))\n",
    "```\n",
    " \n",
    "### 7.8 Optional: training-time augmentation multiplicity for the probe\n",
    " \n",
    "Sometimes repeating augmented copies of the training rows while fitting the probe improves stability. We keep the encoder frozen and only enlarge the probe training set with encoded augmentations of the training fold.\n",
    " \n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    " \n",
    "def eval_probe_with_augmult(enc, X_proc, X_chem, y, n_aug_per_row=1, params=P_DEFAULT, n_splits=5, seed=0):\n",
    "    \"\"\"\n",
    "    Compare a linear probe trained with augmented training data.\n",
    "    - enc: frozen encoder (torch nn.Module)\n",
    "    - X_proc, X_chem: base inputs used to build augmentations\n",
    "    - y: binary labels (success)\n",
    "    - n_aug_per_row: how many augmented copies per training row (0 means no augmentation)\n",
    "    Returns mean and std of acc, f1, auc.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    A, F, U = [], [], []\n",
    "    with torch.no_grad():\n",
    "        X_base = np.hstack([X_proc, X_chem]).astype(np.float32)\n",
    "        H_all = enc(torch.from_numpy(X_base).to(device)).cpu().numpy()\n",
    " \n",
    "    for tr, te in skf.split(H_all, y):\n",
    "        Htr, ytr = H_all[tr], y[tr]\n",
    "        Hte, yte = H_all[te], y[te]\n",
    " \n",
    "        if n_aug_per_row > 0:\n",
    "            # Build augmented pool *only* from the training rows\n",
    "            Xp_tr = X_proc[tr]\n",
    "            Xc_tr = X_chem[tr]\n",
    "            X_aug_pool, src_idx = make_augmented_pool(Xp_tr, Xc_tr, params=params, n_aug_per_row=n_aug_per_row, seed=seed+7)\n",
    "            # Encode the augmented rows\n",
    "            with torch.no_grad():\n",
    "                H_aug = enc(torch.from_numpy(X_aug_pool).to(device)).cpu().numpy()\n",
    "            y_aug = np.repeat(ytr, n_aug_per_row)\n",
    "            Htr = np.vstack([Htr, H_aug])\n",
    "            ytr = np.concatenate([ytr, y_aug])\n",
    " \n",
    "        clf = LogisticRegression(max_iter=300, solver=\"lbfgs\")\n",
    "        clf.fit(Htr, ytr)\n",
    "        p = clf.predict(Hte)\n",
    "        proba = clf.predict_proba(Hte)[:,1]\n",
    "        A.append(accuracy_score(yte, p))\n",
    "        F.append(f1_score(yte, p))\n",
    "        try:\n",
    "            U.append(roc_auc_score(yte, proba))\n",
    "        except Exception:\n",
    "            U.append(np.nan)\n",
    "    return (np.mean(A), np.std(A)), (np.mean(F), np.std(F)), np.nanmean(U)\n",
    " \n",
    "# Baseline: no augmented rows, then 1x and 5x\n",
    "enc.eval()\n",
    "baseline = eval_probe_with_augmult(enc, X_proc, X_chem, y_success, n_aug_per_row=0, n_splits=5, seed=11)\n",
    "one_x    = eval_probe_with_augmult(enc, X_proc, X_chem, y_success, n_aug_per_row=1, n_splits=5, seed=11)\n",
    "five_x   = eval_probe_with_augmult(enc, X_proc, X_chem, y_success, n_aug_per_row=5, n_splits=5, seed=11)\n",
    " \n",
    "print(\"Linear probe with augmentation multiplicity\")\n",
    "for name, res in [(\"0x (no aug)\", baseline), (\"1x\", one_x), (\"5x\", five_x)]:\n",
    "    (acc_m, acc_s), (f1_m, f1_s), auc_m = res\n",
    "    print(f\"{name:10s} | acc {acc_m:.3f} ± {acc_s:.3f} | f1 {f1_m:.3f} ± {f1_s:.3f} | auc {auc_m:.3f}\")\n",
    "```\n",
    " \n",
    "---\n",
    " \n",
    "## Practical notes and equations at a glance\n",
    " \n",
    "- **Temperature** $\\tau$: controls the softness of the softmax in NT-Xent. Larger $\\tau$ reduces repulsion, smaller $\\tau$ increases it. Try $\\tau \\in \\{0.1, 0.2, 0.5\\}$.\n",
    "- **Batch size** $B$: more negatives generally help NT-Xent since each anchor sees $2B-2$ negatives. With small CPU batches keep expectations modest.\n",
    "- **Mask-and-impute**: in standardized space, setting a coordinate to zero corresponds to imputing the training mean.\n",
    "- **Frozen probe**: we evaluate $f_\\theta$ by training a small linear classifier on $H = f_\\theta(X)$ only. If probe performance beats the baseline on raw features, the representation is more linearly separable for the task.\n",
    "- **Why projection head** $g_\\phi$: empirically helps the encoder avoid collapsing to trivial directions used by the loss. At evaluation we drop $g_\\phi$ and use $h$.\n",
    " \n",
    "---\n",
    " \n",
    "**Summary**\n",
    "- Augmentations create two friendly views per experiment and small supervised variants.  \n",
    "- Contrastive training organizes the space without labels using NT-Xent.  \n",
    "- A tiny probe on the frozen encoder can separate success vs failure with limited data.  \n",
    "- To scale up, remove the 100-row sampling and tune epochs and batch size."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}