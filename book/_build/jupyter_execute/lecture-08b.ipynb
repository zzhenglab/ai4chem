{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96c8080",
   "metadata": {},
   "source": [
    "# Lecture 8 - Neural Networks\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    ":depth: 1\n",
    "```\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "- Build intuition for neurons, layers, activation functions, loss, and optimization.\n",
    "- Create a first neural network for a toy regression, then for chemistry data.\n",
    "- Use PyTorch tensors, `Dataset`, `DataLoader`, and a simple training loop.\n",
    "- Track shapes at each step and visualize learning curves.\n",
    "- Compare to models from Lectures 5-7 and connect ideas like splits and metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Setup\n",
    "\n",
    "We start light, then switch to the C-H dataset you used in earlier lectures.  \n",
    "If RDKit is missing, code will skip molecule drawings but still compute with the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c14b513",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 0. Setup\n",
    "%pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip -q install pandas numpy matplotlib scikit-learn\n",
    "\n",
    "# RDKit is optional for descriptors already stored in the CSV\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors\n",
    "except Exception:\n",
    "    Chem = None\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, math, time, torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509392a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. What is a neural network\n",
    "\n",
    "```{admonition} Picture in words\n",
    "A **neuron** computes `z = w·x + b`, then applies a **nonlinearity** `a = σ(z)`.\n",
    "A **layer** stacks many neurons.\n",
    "A **network** stacks layers.\n",
    "Training finds weights `w, b` that minimize a **loss** on your data.\n",
    "```\n",
    "\n",
    "**Key pieces**\n",
    "\n",
    "- **Activation**: ReLU, Sigmoid, Tanh.  \n",
    "- **Loss**: MSE for regression, Cross-Entropy for classification.  \n",
    "- **Optimizer**: Gradient descent variants (SGD, Adam).  \n",
    "- **Epoch**: one pass over the training set.\n",
    "\n",
    "We will not jump into a large script. We will build the pipeline in tiny steps, check shapes, and talk through each part.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Tensors in PyTorch: a 2-minute tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5146ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalars, vectors, matrices\n",
    "t_scalar = torch.tensor(3.14)\n",
    "t_vec    = torch.tensor([1.0, 2.0, 3.0])\n",
    "t_mat    = torch.tensor([[1., 2.], [3., 4.]])\n",
    "\n",
    "t_scalar.shape, t_vec.shape, t_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c07e0f",
   "metadata": {},
   "source": [
    "```{admonition} Tip\n",
    "Use `.shape` often. In deep learning, many bugs are shape bugs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1fc1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic ops\n",
    "x = torch.tensor([[2., -1.],[0.5, 4.]])\n",
    "y = torch.tensor([[1.,  3.],[2.0, 1.]])\n",
    "x + y, x @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e119e1",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 2.1\n",
    "Create a 3x4 tensor of ones, multiply by 2, then compute its mean.\n",
    "Confirm the shape at each step.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. A first neural net on a toy regression\n",
    "\n",
    "Before touching chemistry, we learn the loop on a simple function: `y = sin(x)` with noise. One input, one output. This shows how a network learns a curve.\n",
    "\n",
    "### 3.1 Make a tiny dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "x_np = np.linspace(-3*np.pi, 3*np.pi, 400).astype(np.float32)\n",
    "y_np = np.sin(x_np) + rng.normal(0, 0.1, size=x_np.shape).astype(np.float32)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.scatter(x_np, y_np, s=10, alpha=0.6)\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"y = sin(x) + noise\"); plt.title(\"Toy regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89719bcf",
   "metadata": {},
   "source": [
    "### 3.2 Convert to tensors and check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(x_np).reshape(-1, 1)  # (N, 1)\n",
    "y = torch.from_numpy(y_np).reshape(-1, 1)  # (N, 1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d39e73",
   "metadata": {},
   "source": [
    "### 3.3 Define a small network\n",
    "\n",
    "We choose a compact multilayer perceptron (MLP): `1 -> 64 -> 64 -> 1` with ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = TinyRegressor()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d695b4a",
   "metadata": {},
   "source": [
    "### 3.4 Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881df65",
   "metadata": {},
   "source": [
    "### 3.5 One manual training step (by hand)\n",
    "\n",
    "We do a single gradient step to see each operation. This is not a full loop yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6263a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "opt.zero_grad()\n",
    "\n",
    "y_pred = model(X)           # forward\n",
    "loss = loss_fn(y_pred, y)   # scalar tensor\n",
    "loss_item_before = loss.item()\n",
    "\n",
    "loss.backward()             # compute gradients\n",
    "opt.step()                  # update weights\n",
    "\n",
    "loss_item_before, loss_fn(model(X), y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370b135",
   "metadata": {},
   "source": [
    "```{admonition} Checkpoint\n",
    "You saw forward, loss, backward, step. This is the basic learning step.\n",
    "Repeat many times over mini-batches to train.\n",
    "```\n",
    "\n",
    "### 3.6 Mini training loop\n",
    "\n",
    "We train for a few epochs and track training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyRegressor()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(400):\n",
    "    opt.zero_grad()\n",
    "    y_hat = model(X)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"train MSE\"); plt.title(\"Training curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30d4f2",
   "metadata": {},
   "source": [
    "### 3.7 Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_fit = model(X).numpy()\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.scatter(X.numpy(), y.numpy(), s=10, alpha=0.3, label=\"data\")\n",
    "plt.plot(X.numpy(), y_fit, lw=2, label=\"model\")\n",
    "plt.legend(); plt.title(\"Fit on toy regression\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c385033",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 3.1\n",
    "Change hidden width from 64 to 16. Retrain and compare the training loss and the curve.\n",
    "Which model underfits or overfits more on this toy?\n",
    "```\n",
    "\n",
    "```{admonition} Exercise 3.2\n",
    "Try `Tanh` instead of `ReLU`. Keep architecture the same.\n",
    "Is convergence slower or faster with the default learning rate?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Neural nets for chemistry: regression on Melting Point\n",
    "\n",
    "We now switch to a familiar table. As in Lectures 5-7, we will use four simple descriptors as features.\n",
    "\n",
    "### 4.1 Load data and build feature matrix\n",
    "\n",
    "```{admonition} Data\n",
    "`MolWt`, `LogP`, `TPSA`, `NumRings` will be our `X`.\n",
    "Target `y` will be `Melting Point`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8444a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/C_H_oxidation_dataset.csv\"\n",
    "df_raw = pd.read_csv(url)\n",
    "\n",
    "# If RDKit available, we can recompute. Otherwise reuse stored numeric columns where present.\n",
    "def descriptors_from_smiles(smiles):\n",
    "    if Chem is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\": Crippen.MolLogP(m),\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(m),\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(m),\n",
    "    })\n",
    "\n",
    "# Compute once if needed\n",
    "desc = df_raw[\"SMILES\"].apply(descriptors_from_smiles)\n",
    "df = pd.concat([df_raw, desc], axis=1)\n",
    "\n",
    "use_cols = [\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Melting Point\"]\n",
    "df_reg = df[use_cols].dropna().reset_index(drop=True)\n",
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc9b9a",
   "metadata": {},
   "source": [
    "### 4.2 Train, validation, test split\n",
    "\n",
    "In Lecture 6, you learned why validation is helpful for picking settings. We will do a 60-20-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baad7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = df_reg[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values.astype(np.float32)\n",
    "y_all = df_reg[\"Melting Point\"].values.astype(np.float32).reshape(-1,1)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "X_train, X_val,  y_train,  y_val  = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42)  # 0.25 of 0.8 = 0.2\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d53fd0",
   "metadata": {},
   "source": [
    "### 4.3 Standardize features\n",
    "\n",
    "Most neural nets train more smoothly when inputs are standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea09e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "Xtr = scaler.transform(X_train).astype(np.float32)\n",
    "Xva = scaler.transform(X_val).astype(np.float32)\n",
    "Xte = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "ytr = y_train.astype(np.float32)\n",
    "yva = y_val.astype(np.float32)\n",
    "yte = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5912491",
   "metadata": {},
   "source": [
    "### 4.4 Wrap tensors and peek at shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_t = torch.from_numpy(Xtr)\n",
    "Xva_t = torch.from_numpy(Xva)\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "ytr_t = torch.from_numpy(ytr)\n",
    "yva_t = torch.from_numpy(yva)\n",
    "yte_t = torch.from_numpy(yte)\n",
    "\n",
    "Xtr_t.shape, ytr_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80978de",
   "metadata": {},
   "source": [
    "### 4.5 A neat `Dataset` and `DataLoader`\n",
    "\n",
    "We will create small batches for stable gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12924e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X) if isinstance(X, np.ndarray) else X\n",
    "        self.y = torch.from_numpy(y) if isinstance(y, np.ndarray) else y\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = ArrayDataset(Xtr, ytr)\n",
    "val_ds   = ArrayDataset(Xva, yva)\n",
    "test_ds  = ArrayDataset(Xte, yte)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6891ae8",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 4.1\n",
    "Change `batch_size` to 16 and 256.\n",
    "Observe the training curve. Which setting is noisier per epoch? Which converges faster in wall time?\n",
    "```\n",
    "\n",
    "### 4.6 Define a small regression MLP and inspect parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPRegressor(nn.Module):\n",
    "    def __init__(self, d_in=4, d_hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "mp_model = MPRegressor(d_in=4, d_hidden=64)\n",
    "sum(p.numel() for p in mp_model.parameters()), mp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c08b6",
   "metadata": {},
   "source": [
    "### 4.7 One forward pass to check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3508091",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    pred = mp_model(xb)\n",
    "pred.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad36e65",
   "metadata": {},
   "source": [
    "### 4.8 Loss, optimizer, and a clear training loop\n",
    "\n",
    "We will compute validation loss each epoch to watch for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(model, train_loader, val_loader, max_epochs=200, lr=1e-3):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    hist = {\"train\": [], \"val\": []}\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        # train\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        hist[\"train\"].append(np.mean(batch_losses))\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            v_losses = []\n",
    "            for xv, yv in val_loader:\n",
    "                pv = model(xv)\n",
    "                v_losses.append(loss_fn(pv, yv).item())\n",
    "        hist[\"val\"].append(np.mean(v_losses))\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"epoch {epoch:3d}  train MSE={hist['train'][-1]:.2f}  val MSE={hist['val'][-1]:.2f}\")\n",
    "    return hist\n",
    "\n",
    "mp_model = MPRegressor()\n",
    "hist = train_regression(mp_model, train_loader, val_loader, max_epochs=200, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829628a5",
   "metadata": {},
   "source": [
    "### 4.9 Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(hist[\"train\"], label=\"train\")\n",
    "plt.plot(hist[\"val\"], label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"MSE\")\n",
    "plt.title(\"Learning curves: Melting Point\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d5015",
   "metadata": {},
   "source": [
    "### 4.10 Evaluate on test and draw parity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d89fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_te = mp_model(Xte_t).numpy().ravel()\n",
    "\n",
    "mse = mean_squared_error(yte, y_pred_te)\n",
    "mae = mean_absolute_error(yte, y_pred_te)\n",
    "r2  = r2_score(yte, y_pred_te)\n",
    "\n",
    "print(f\"Test MSE={mse:.2f}  MAE={mae:.2f}  R2={r2:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(4.2,4))\n",
    "plt.scatter(yte, y_pred_te, alpha=0.6)\n",
    "lims = [min(yte.min(), y_pred_te.min()), max(yte.max(), y_pred_te.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True MP\"); plt.ylabel(\"Predicted MP\"); plt.title(\"Parity: NN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d4b9e",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 4.2\n",
    "Add a **Dropout(p=0.1)** layer after the first ReLU. Retrain with the same settings.\n",
    "Compare validation MSE and test R2. Does Dropout help a little here?\n",
    "```\n",
    "\n",
    "```{admonition} Exercise 4.3\n",
    "Reduce `d_hidden` from 64 to 16. Retrain and compare.\n",
    "Which setting gives better generalization on this target?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Neural nets for chemistry: classification on Toxicity\n",
    "\n",
    "We reuse the same four descriptors. Now the target is binary: toxic vs non_toxic.\n",
    "\n",
    "### 5.1 Prepare labels and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6de32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Toxicity\"]].dropna().reset_index(drop=True)\n",
    "y_txt = df_clf[\"Toxicity\"].str.lower()\n",
    "y_bin = (y_txt == \"toxic\").astype(np.int64).values  # 1 toxic, 0 non_toxic\n",
    "X_all = df_clf[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values.astype(np.float32)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_all, y_bin, test_size=0.2, random_state=7, stratify=y_bin)\n",
    "X_train, X_val,  y_train,  y_val  = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=7, stratify=y_trainval)\n",
    "\n",
    "scaler_c = StandardScaler().fit(X_train)\n",
    "Xtr = scaler_c.transform(X_train).astype(np.float32)\n",
    "Xva = scaler_c.transform(X_val).astype(np.float32)\n",
    "Xte = scaler_c.transform(X_test).astype(np.float32)\n",
    "\n",
    "ytr = y_train.astype(np.int64)\n",
    "yva = y_val.astype(np.int64)\n",
    "yte = y_test.astype(np.int64)\n",
    "\n",
    "train_loader_c = DataLoader(ArrayDataset(Xtr, ytr), batch_size=128, shuffle=True)\n",
    "val_loader_c   = DataLoader(ArrayDataset(Xva, yva), batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819c8e7",
   "metadata": {},
   "source": [
    "### 5.2 Define a small classifier\n",
    "\n",
    "Outputs logits of shape `(batch, 2)`. We will use `nn.CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa039b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicityMLP(nn.Module):\n",
    "    def __init__(self, d_in=4, d_hidden=32, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_hidden, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # logits\n",
    "\n",
    "clf = ToxicityMLP()\n",
    "sum(p.numel() for p in clf.parameters()), clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5f7c6",
   "metadata": {},
   "source": [
    "### 5.3 Single forward pass and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86351209",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_loader_c))\n",
    "logits = clf(xb)   # (B, 2)\n",
    "proba  = torch.softmax(logits, dim=1)  # probabilities\n",
    "logits.shape, proba.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e24bf",
   "metadata": {},
   "source": [
    "### 5.4 Train with Cross-Entropy\n",
    "\n",
    "We also compute validation accuracy per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, max_epochs=100, lr=5e-3):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_hist, val_hist = [], []\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "        train_hist.append(np.mean(losses))\n",
    "\n",
    "        # simple val accuracy\n",
    "        model.eval()\n",
    "        n_correct, n_total, v_losses = 0, 0, []\n",
    "        with torch.no_grad():\n",
    "            for xv, yv in val_loader:\n",
    "                lv = model(xv)\n",
    "                v_losses.append(loss_fn(lv, yv).item())\n",
    "                pred = lv.argmax(dim=1)\n",
    "                n_correct += (pred == yv).sum().item()\n",
    "                n_total   += yv.shape[0]\n",
    "        val_hist.append(np.mean(v_losses))\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"epoch {epoch:3d}  train CE={train_hist[-1]:.3f}  val CE={val_hist[-1]:.3f}  val acc={n_correct/n_total:.3f}\")\n",
    "    return {\"train\": train_hist, \"val\": val_hist}\n",
    "\n",
    "clf = ToxicityMLP()\n",
    "hist_c = train_classifier(clf, train_loader_c, val_loader_c, max_epochs=100, lr=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48195eb1",
   "metadata": {},
   "source": [
    "### 5.5 Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(hist_c[\"train\"], label=\"train CE\")\n",
    "plt.plot(hist_c[\"val\"], label=\"val CE\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"Cross-Entropy\"); plt.legend(); plt.title(\"Classification learning curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c541ec",
   "metadata": {},
   "source": [
    "### 5.6 Test metrics: Accuracy and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bcbf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.eval()\n",
    "with torch.no_grad():\n",
    "    logits_te = clf(torch.from_numpy(Xte))\n",
    "    proba_te  = torch.softmax(logits_te, dim=1)[:,1].numpy()\n",
    "    pred_te   = logits_te.argmax(dim=1).numpy()\n",
    "\n",
    "acc = accuracy_score(yte, pred_te)\n",
    "auc = roc_auc_score(yte, proba_te)\n",
    "print(f\"Test Accuracy={acc:.3f}  AUC={auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c64022",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 5.1\n",
    "Add a second hidden layer with `ReLU`. Keep the same total parameter count by reducing widths.\n",
    "Does AUC change on the test split?\n",
    "```\n",
    "\n",
    "```{admonition} Exercise 5.2\n",
    "Set learning rate to `1e-2` and then `1e-4`. Train for the same epochs.\n",
    "Report validation CE after epoch 100. Which rate is better here?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Practical notes\n",
    "\n",
    "- **Normalization**: Standardize inputs. We reused `StandardScaler` from Lecture 6.\n",
    "- **Initialization**: PyTorch initializes layers reasonably, but very deep nets may need care.\n",
    "- **Learning rate**: If loss does not decrease, lower it. If training is slow, try higher but watch for divergence.\n",
    "- **Batch size**: Small batches add noise that may help generalization.\n",
    "- **Regularization**: Dropout is easy. Weight decay via `Adam(..., weight_decay=1e-4)` can help.\n",
    "\n",
    "```{admonition} Link to earlier lectures\n",
    "Train/val/test split, parity plots, and metrics come from Lectures 5-7. The same diagnostics help with neural nets.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff68dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(mp_model.state_dict(), \"mp_regressor.pt\")\n",
    "\n",
    "# Load into a fresh instance\n",
    "mp2 = MPRegressor()\n",
    "mp2.load_state_dict(torch.load(\"mp_regressor.pt\", map_location=\"cpu\"))\n",
    "mp2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae556dc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Quick reference\n",
    "\n",
    "```{admonition} PyTorch recipe\n",
    "1) Prepare `X, y` as float tensors (long for class labels).\n",
    "2) `Dataset` and `DataLoader` with a batch size.\n",
    "3) Define `nn.Module` with layers and activations.\n",
    "4) Choose loss and optimizer.\n",
    "5) Loop: `zero_grad -> forward -> loss -> backward -> step`.\n",
    "6) Track validation loss and stop if it rises for too long.\n",
    "```\n",
    "\n",
    "```{admonition} Common layers\n",
    "- `nn.Linear(d_in, d_out)`\n",
    "- `nn.ReLU()`, `nn.Tanh()`, `nn.Sigmoid()`\n",
    "- `nn.Dropout(p)`\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Optional: compare to scikit-learn MLP\n",
    "\n",
    "This gives you a quick baseline. It hides some details but is useful for sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0faba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(64,64), activation=\"relu\", random_state=0, max_iter=1000)\n",
    "mlp.fit(Xtr, ytr.ravel())\n",
    "pred = mlp.predict(Xte)\n",
    "print(f\"sklearn MLP  R2={r2_score(yte, pred):.3f}  MAE={mean_absolute_error(yte, pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdb56b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. In-class activity (5 questions)\n",
    "\n",
    "Each task can be done with the code above and small edits.\n",
    "\n",
    "### Q1. Width vs depth on Melting Point\n",
    "\n",
    "Train two regression MLPs:\n",
    "\n",
    "- Model A: `d_hidden=16`, 2 hidden layers\n",
    "- Model B: `d_hidden=64`, 1 hidden layer\n",
    "\n",
    "Use the same split as Section 4. Report **validation MSE** at the end of training and **test R2**.\n",
    "\n",
    "```python\n",
    "# TO DO: build two MPRegressor variants and compare hist[\"val\"][-1] and R2 on test\n",
    "```\n",
    "\n",
    "### Q2. Weight decay on Melting Point\n",
    "\n",
    "Repeat Section 4 with `weight_decay=1e-4` in Adam. Keep everything else the same.\n",
    "Report test **MAE** and compare to the no-decay setting.\n",
    "\n",
    "```python\n",
    "# TO DO: train_regression but create optimizer with weight decay\n",
    "```\n",
    "\n",
    "### Q3. Calibration for Toxicity\n",
    "\n",
    "For the classifier, collect `proba_te`.\n",
    "Compute accuracy at thresholds `0.3, 0.5, 0.7`.\n",
    "Plot the three points on a simple threshold vs accuracy line.\n",
    "\n",
    "```python\n",
    "# TO DO: use proba_te and vary threshold to get predictions, then accuracy_score\n",
    "```\n",
    "\n",
    "### Q4. Swap activation\n",
    "\n",
    "Replace ReLU with Tanh in the regression model and repeat training.\n",
    "Report final validation MSE and test R2.\n",
    "Is convergence slower?\n",
    "\n",
    "```python\n",
    "# TO DO: define MPRegressor with Tanh and train\n",
    "```\n",
    "\n",
    "### Q5. Predict new molecules (Melting Point)\n",
    "\n",
    "Given two descriptor rows:\n",
    "\n",
    "- `[135.0, 2.0, 9.2, 2]`\n",
    "- `[301.0, 0.5, 17.7, 2]`\n",
    "\n",
    "Use the trained regression model and the same `scaler` to predict MP.\n",
    "\n",
    "```python\n",
    "# TO DO: transform with scaler, run model.eval() and predict\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Solutions\n",
    "\n",
    "### 11.1 Solution Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPRegressorA(nn.Module):\n",
    "    def __init__(self, d_in=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class MPRegressorB(nn.Module):\n",
    "    def __init__(self, d_in=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "mA, mB = MPRegressorA(), MPRegressorB()\n",
    "histA = train_regression(mA, train_loader, val_loader, max_epochs=200, lr=1e-3)\n",
    "histB = train_regression(mB, train_loader, val_loader, max_epochs=200, lr=1e-3)\n",
    "\n",
    "def eval_r2(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        yp = model(Xte_t).numpy().ravel()\n",
    "    return r2_score(yte, yp)\n",
    "\n",
    "print(f\"A: val MSE={histA['val'][-1]:.2f}  test R2={eval_r2(mA):.3f}\")\n",
    "print(f\"B: val MSE={histB['val'][-1]:.2f}  test R2={eval_r2(mB):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d880e",
   "metadata": {},
   "source": [
    "### 11.2 Solution Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_wd(model, train_loader, val_loader, max_epochs=200, lr=1e-3, wd=1e-4):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    hist = {\"train\": [], \"val\": []}\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        tloss = []\n",
    "        for xb, yb in train_loader:\n",
    "            opt.zero_grad(); loss = loss_fn(model(xb), yb)\n",
    "            loss.backward(); opt.step(); tloss.append(loss.item())\n",
    "        hist[\"train\"].append(np.mean(tloss))\n",
    "        model.eval(); vloss=[]\n",
    "        with torch.no_grad():\n",
    "            for xv, yv in val_loader:\n",
    "                vloss.append(loss_fn(model(xv), yv).item())\n",
    "        hist[\"val\"].append(np.mean(vloss))\n",
    "    return hist\n",
    "\n",
    "m_wd = MPRegressor()\n",
    "hist_wd = train_regression_wd(m_wd, train_loader, val_loader, max_epochs=200, lr=1e-3, wd=1e-4)\n",
    "m_wd.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_te = m_wd(Xte_t).numpy().ravel()\n",
    "\n",
    "print(f\"With weight decay  MAE={mean_absolute_error(yte, y_pred_te):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46bd23",
   "metadata": {},
   "source": [
    "### 11.3 Solution Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_at_thresh(proba, y_true, thr):\n",
    "    y_hat = (proba >= thr).astype(int)\n",
    "    return accuracy_score(y_true, y_hat)\n",
    "\n",
    "# Reuse proba_te from Section 5.6 after clf is trained\n",
    "thr_list = [0.3, 0.5, 0.7]\n",
    "accs = [acc_at_thresh(proba_te, yte, t) for t in thr_list]\n",
    "for t,a in zip(thr_list, accs):\n",
    "    print(f\"threshold={t:.2f}  acc={a:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(4.2,3))\n",
    "plt.plot(thr_list, accs, marker=\"o\")\n",
    "plt.xlabel(\"threshold\"); plt.ylabel(\"accuracy\"); plt.title(\"Threshold vs accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc427700",
   "metadata": {},
   "source": [
    "### 11.4 Solution Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPRegressorTanh(nn.Module):\n",
    "    def __init__(self, d_in=4, d_hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_hidden, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "m_tanh = MPRegressorTanh()\n",
    "hist_tanh = train_regression(m_tanh, train_loader, val_loader, max_epochs=200, lr=1e-3)\n",
    "\n",
    "m_tanh.eval()\n",
    "with torch.no_grad():\n",
    "    ypt = m_tanh(Xte_t).numpy().ravel()\n",
    "\n",
    "print(f\"Tanh val MSE={hist_tanh['val'][-1]:.2f}  test R2={r2_score(yte, ypt):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850e695",
   "metadata": {},
   "source": [
    "### 11.5 Solution Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_desc = np.array([[135.0, 2.0, 9.2, 2],\n",
    "                     [301.0, 0.5, 17.7, 2]], dtype=np.float32)\n",
    "new_scaled = scaler.transform(new_desc).astype(np.float32)\n",
    "with torch.no_grad():\n",
    "    preds = mp_model(torch.from_numpy(new_scaled)).numpy().ravel()\n",
    "pd.DataFrame({\n",
    "    \"MolWt\":[135.0,301.0],\n",
    "    \"LogP\":[2.0,0.5],\n",
    "    \"TPSA\":[9.2,17.7],\n",
    "    \"NumRings\":[2,2],\n",
    "    \"Pred_MP\":preds\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb763d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Glossary\n",
    "\n",
    "```{glossary}\n",
    "neuron\n",
    "  Computes a weighted sum plus bias and passes it through a nonlinearity.\n",
    "\n",
    "activation\n",
    "  Function applied to a neuron output. ReLU, Tanh, Sigmoid.\n",
    "\n",
    "loss\n",
    "  A number that measures mismatch between predictions and targets.\n",
    "\n",
    "optimizer\n",
    "  An algorithm that updates weights to reduce the loss. Adam, SGD.\n",
    "\n",
    "epoch\n",
    "  One full pass through the training data.\n",
    "\n",
    "batch size\n",
    "  Number of samples per gradient update.\n",
    "\n",
    "Dropout\n",
    "  Randomly zeros hidden units during training to reduce overfitting.\n",
    "\n",
    "weight decay\n",
    "  L2 penalty on weights controlled by the optimizer.\n",
    "\n",
    "Cross-Entropy\n",
    "  Loss used for classification with logits.\n",
    "\n",
    "MSE\n",
    "  Mean squared error, common for regression.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Wrap-up\n",
    "\n",
    "You built two neural networks:\n",
    "\n",
    "- A regressor for Melting Point with 4 simple descriptors.\n",
    "- A classifier for Toxicity with the same inputs.\n",
    "\n",
    "Along the way you tracked shapes, watched loss curves, and reused plots and metrics from earlier lectures. The same habits carry into deeper models and molecular representations later in the course."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "source_map": [
   12,
   36,
   58,
   84,
   91,
   97,
   102,
   117,
   126,
   130,
   134,
   140,
   156,
   160,
   163,
   169,
   181,
   192,
   210,
   214,
   223,
   248,
   273,
   279,
   287,
   293,
   302,
   306,
   315,
   321,
   337,
   346,
   362,
   366,
   371,
   377,
   411,
   415,
   422,
   426,
   443,
   463,
   483,
   489,
   503,
   507,
   512,
   518,
   553,
   557,
   563,
   567,
   577,
   607,
   615,
   642,
   649,
   718,
   750,
   754,
   780,
   784,
   799,
   803,
   824,
   828,
   841
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}