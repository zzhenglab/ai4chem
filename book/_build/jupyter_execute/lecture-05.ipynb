{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b8ed84",
   "metadata": {},
   "source": [
    "# Lecture 5 - Regression and Classification\n",
    "\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    ":depth: 1\n",
    "```\n",
    "## Learning goals\n",
    "\n",
    "- Tell classification from regression by the target type.\n",
    "- Load small chemistry-like datasets with SMILES and simple text features.\n",
    "- Make train, validation, and test splits.\n",
    "- Fit a simple regression model and a simple classification model.\n",
    "- Read and compare common metrics in each case.\n",
    "  [![Colab](https://img.shields.io/badge/Open-Colab-orange)](https://colab.research.google.com/drive/1UYUb5xw7lxDQrZYjJPixfILLrhFCYBlb?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649e4b09",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\52377\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# 0. Setup\n",
    "%pip install scikit-learn pandas matplotlib\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Draw, Descriptors, Crippen, rdMolDescriptors, AllChem\n",
    "except Exception:\n",
    "    try:\n",
    "        %pip install rdkit\n",
    "        from rdkit import Chem\n",
    "        from rdkit.Chem import Draw, Descriptors, Crippen, rdMolDescriptors, AllChem\n",
    "    except Exception as e:\n",
    "        print(\"RDKit is not available in this environment. Drawing and descriptors will be skipped.\")\n",
    "        Chem = None\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3dfe73",
   "metadata": {},
   "source": [
    "## 1. What is supervised learning\n",
    "\n",
    "```{admonition} Definition\n",
    "- **Inputs** `X` are the observed features for each sample.\n",
    "- **Target** `y` is the quantity you want to predict.\n",
    "- **Regression** predicts a number. Example: a boiling point `300 F`.\n",
    "- **Classification** predicts a category. Example: `high` solubility vs `low` solubility.\n",
    "```\n",
    "\n",
    "Rule of thumb:\n",
    "- If **y** is real-valued, use regression.\n",
    "- If **y** is a class label, use classification.\n",
    "\n",
    "\n",
    "## 2. Data preview and descriptor engineering\n",
    "We will read a small CSV from a public repository and compute a handful of molecular descriptors from SMILES. These lightweight features are enough to practice the full workflow.\n",
    "\n",
    "```{admonition} Tip\n",
    "Descriptors such as **molecular weight**, **logP**, **TPSA**, and **ring count** are simple to compute and often serve as a first baseline for structure-property relationships.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek at the two datasets\n",
    "df_oxidation_raw = pd.read_csv(\"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/C_H_oxidation_dataset.csv\")\n",
    "df_oxidation_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ebca7",
   "metadata": {},
   "source": [
    "```{admonition} Think-pair-share\n",
    "⏰\n",
    "**Exercise 1.1**\n",
    "\n",
    "\n",
    "Which column(s) can be target **y**?\n",
    "\n",
    "Which are regression tasks and which are classification tasks?\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Recall from last week that we can use SMILES to introduce additional descriptors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return pd.Series({\n",
    "            \"MolWt\": None,\n",
    "            \"LogP\": None,\n",
    "            \"TPSA\": None,\n",
    "            \"NumRings\": None\n",
    "        })\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(mol),                    # molecular weight\n",
    "        \"LogP\": Crippen.MolLogP(mol),                       # octanol-water logP\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(mol),             # topological polar surface area\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(mol)      # number of rings\n",
    "    })\n",
    "\n",
    "# Apply the function to the SMILES column\n",
    "desc_df = df_oxidation_raw[\"SMILES\"].apply(calc_descriptors)\n",
    "\n",
    "# Concatenate new descriptor columns to original DataFrame\n",
    "df = pd.concat([df_oxidation_raw, desc_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f4a8b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## 3. Regression workflow on melting point\n",
    "\n",
    "We start with a single property, the **Melting Point**, and use four descriptors as features.\n",
    "\n",
    "\n",
    "Now let's first look at regression, we will focus one property prediction at a time. first create a new df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_mp =df[[\"Compound Name\", \"MolWt\", \"LogP\", \"TPSA\", \"NumRings\",\"Melting Point\"]]\n",
    "df_reg_mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417133d",
   "metadata": {},
   "source": [
    "### 3.1 Train and test split\n",
    "\n",
    "```{admonition} Why split?\n",
    "We test on **held-out** data to estimate generalization. A common split is 80 percent train, 20 percent test with a fixed `random_state` for reproducibility.\n",
    "```\n",
    "\n",
    "\n",
    "### 3.2 Splitting the data and train\n",
    "\n",
    "Before training, we need to separate the input features (`X`) from the target (`y`). Then we split into training and test sets to evaluate how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = df_reg_mp[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]]\n",
    "y = df_reg_mp[\"Melting Point\"]\n",
    "\n",
    "# Split into train (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "X_train\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d3c39",
   "metadata": {},
   "source": [
    "```{admonition} Visual check\n",
    "Scatter the training and test points for one descriptor vs the target to see coverage. This is a quick check for weird splits or narrow ranges.\n",
    "```\n",
    "\n",
    "\n",
    "Since we have 575 rows in total, we splited it into 460 train + 115 test and these are shuffled.\n",
    "\n",
    "It’s often useful to check how the training and test sets are distributed. Here we’ll do a scatter plot of one descriptor (say `MolWt`) against the target (`Melting Point`) and color by train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training vs test data\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_train[\"MolWt\"], y_train, color=\"blue\", label=\"Train\", alpha=0.7)\n",
    "plt.scatter(X_test[\"MolWt\"], y_test, color=\"red\", label=\"Test\", alpha=0.7)\n",
    "plt.xlabel(\"Molecular Weight (MolWt)\")\n",
    "plt.ylabel(\"Melting Point\")\n",
    "plt.title(\"Train vs Test Data Split\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b9d1a",
   "metadata": {},
   "source": [
    "Blue points are the training set and red points are the test set. We can see that the split looks balanced and the test set covers a different range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Fit to training data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1f761",
   "metadata": {},
   "source": [
    "Below are metrics, with formulas:\n",
    "\n",
    "- **Mean Squared Error (MSE):**  \n",
    " $\n",
    "  \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "    $\n",
    "  \n",
    "  Squared differences; penalizes large errors heavily.\n",
    "\n",
    "- **Mean Absolute Error (MAE):**  \n",
    "   $\n",
    "  \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "   $   \n",
    "  Easier to interpret; average magnitude of errors.\n",
    "\n",
    "- **R² (Coefficient of Determination):**  \n",
    "   $\n",
    "  R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "   $   \n",
    "  Measures proportion of variance explained by the model.  \n",
    "  -  $ R^2 = 1 $ : perfect prediction  \n",
    "  -  $ R^2 = 0 $ : model is no better than mean  \n",
    "  -  $ R^2 < 0 $ : worse than predicting the average\n",
    "\n",
    "\n",
    "\n",
    "⏰\n",
    "**Exercise 2.1**\n",
    "\n",
    "\n",
    "Change the test_size=0.2 to 0.1 and random_state=42 to 7 to see any difference in resulting MSE, MAE and R2.\n",
    "\n",
    "Now you can use `reg` to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931336e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single new data point with 2 features\n",
    "X_new = np.array([[135, 2, 9.2, 2]]) # [\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]]\n",
    "y_new_pred = reg.predict(X_new)\n",
    "\n",
    "print(\"Predicted value:\", y_new_pred)\n",
    "\n",
    "Xs_new = np.array([[135, 2, 9.2, 2],\n",
    "                  [301, 0.5, 17.7, 2],\n",
    "                  [65, 1.3, 20.0, 1]]) # [\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]]\n",
    "ys_new_pred = reg.predict(Xs_new)\n",
    "\n",
    "print(\"Predicted value:\", ys_new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c8416",
   "metadata": {},
   "source": [
    "```{admonition} Diagnostic plots\n",
    "A residual plot should look centered around zero without obvious patterns. A parity plot compares predicted to true values and should line up near the 45 degree line.\n",
    "```\n",
    "\n",
    "After training, we compare the predicted outputs with the true labels from the test set. \n",
    "\n",
    "This allows us to verify how close the model’s predictions are to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7235ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "resid = y_test - y_pred\n",
    "plt.scatter(y_pred, resid, alpha=0.6)\n",
    "plt.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual plot – Regression\")\n",
    "plt.show()\n",
    "\n",
    "# Parity plot\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True values\")\n",
    "plt.ylabel(\"Predicted values\")\n",
    "plt.title(\"Parity plot – Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93646de",
   "metadata": {},
   "source": [
    "### 3.3 How split choice affects accuracy\n",
    "\n",
    "Besides, we can examine how different splitting strategies influence the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84505444",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sizes = [0.10, 0.20, 0.30]\n",
    "seeds = range(40)  # more seeds = smoother distributions\n",
    "\n",
    "rows = []\n",
    "for t in test_sizes:\n",
    "    for s in seeds:\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=t, random_state=s)\n",
    "        reg = LinearRegression().fit(X_tr, y_tr)\n",
    "        y_hat = reg.predict(X_te)\n",
    "        rows.append({\n",
    "            \"test_size\": t,\n",
    "            \"seed\": s,\n",
    "            \"MSE\": mean_squared_error(y_te, y_hat),\n",
    "            \"MAE\": mean_absolute_error(y_te, y_hat),\n",
    "            \"R2\": r2_score(y_te, y_hat),\n",
    "            \"n_train\": len(X_tr),\n",
    "            \"n_test\": len(X_te),\n",
    "        })\n",
    "\n",
    "df_splits = pd.DataFrame(rows)\n",
    "\n",
    "# Summary table\n",
    "summary = (df_splits\n",
    "           .groupby(\"test_size\")\n",
    "           .agg(MSE_mean=(\"MSE\",\"mean\"), MSE_std=(\"MSE\",\"std\"),\n",
    "                MAE_mean=(\"MAE\",\"mean\"), MAE_std=(\"MAE\",\"std\"),\n",
    "                R2_mean=(\"R2\",\"mean\"),   R2_std=(\"R2\",\"std\"),\n",
    "                n_train_mean=(\"n_train\",\"mean\"), n_test_mean=(\"n_test\",\"mean\"))\n",
    "           .reset_index())\n",
    "print(\"Effect of split on accuracy\")\n",
    "display(summary.round(4))\n",
    "\n",
    "# Simple R2 scatter by test_size to visualize spread\n",
    "plt.figure(figsize=(7,5))\n",
    "for t in test_sizes:\n",
    "    vals = df_splits.loc[df_splits[\"test_size\"]==t, \"R2\"].values\n",
    "    plt.plot([t]*len(vals), vals, \"o\", alpha=0.35, label=f\"test_size={t}\")\n",
    "plt.xlabel(\"test_size\")\n",
    "plt.ylabel(\"R2 on test\")\n",
    "plt.title(\"R2 across many random splits\")\n",
    "plt.show()\n",
    "\n",
    "# One-shot comparison matching your exercise idea\n",
    "for test_size, seed in [(0.2, 15), (0.2, 42), (0.1, 42),(0.1, 7)]:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    reg = LinearRegression().fit(X_tr, y_tr)\n",
    "    y_hat = reg.predict(X_te)\n",
    "    print(f\"test_size={test_size}, seed={seed} -> \"\n",
    "          f\"MSE={mean_squared_error(y_te,y_hat):.3f}, \"\n",
    "          f\"MAE={mean_absolute_error(y_te,y_hat):.3f}, \"\n",
    "          f\"R2={r2_score(y_te,y_hat):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fac654",
   "metadata": {},
   "source": [
    "```{admonition} Reproducibility\n",
    "`random_state` fixes the shuffle used by `train_test_split`. Same seed gives the same split so your metrics are stable from run to run.\n",
    "```\n",
    "\n",
    "### 3.4 Learning curves\n",
    "\n",
    "\n",
    "A random `seed` is simply a number provided to a random number generator to ensure that it produces the same sequence of “random” results each time.\n",
    "\n",
    "For example, functions such as `train_test_split` shuffle the dataset before dividing it into training and testing sets. If you do not specify a `random_state` (the seed), every run may produce a slightly different split. This variation can lead to different accuracy values across runs.\n",
    "\n",
    "**Same seed → same split → same results**\n",
    "\n",
    "**Different seed → different split → possibly different accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165356bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "train_sizes = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "# Storage for test scores\n",
    "test_scores_r2_all = []\n",
    "test_scores_mae_all = []\n",
    "\n",
    "for seed in seeds:\n",
    "    # Fixed train-test split per seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # R²\n",
    "    train_sizes_abs, train_scores_r2, test_scores_r2 = learning_curve(\n",
    "        estimator=LinearRegression(), \n",
    "        X=X_train, y=y_train,\n",
    "        train_sizes=train_sizes, \n",
    "        scoring=\"r2\", \n",
    "        shuffle=False\n",
    "    )\n",
    "    test_scores_r2_all.append(test_scores_r2.mean(axis=1))\n",
    "\n",
    "    # MAE\n",
    "    _, train_scores_mae, test_scores_mae = learning_curve(\n",
    "        estimator=LinearRegression(), \n",
    "        X=X_train, y=y_train,\n",
    "        train_sizes=train_sizes, \n",
    "        scoring=\"neg_mean_absolute_error\", \n",
    "        shuffle=False\n",
    "    )\n",
    "    test_scores_mae_all.append(-test_scores_mae.mean(axis=1))\n",
    "\n",
    "# Convert to arrays\n",
    "test_scores_r2_all = np.array(test_scores_r2_all)\n",
    "test_scores_mae_all = np.array(test_scores_mae_all)\n",
    "\n",
    "# Mean and std across seeds\n",
    "test_mean_r2 = test_scores_r2_all.mean(axis=0)\n",
    "test_std_r2  = test_scores_r2_all.std(axis=0)\n",
    "test_mean_mae = test_scores_mae_all.mean(axis=0)\n",
    "test_std_mae  = test_scores_mae_all.std(axis=0)\n",
    "\n",
    "# Plot R²\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(train_sizes_abs, test_mean_r2, \"o-\", label=\"Test R2\")\n",
    "plt.fill_between(train_sizes_abs, test_mean_r2 - test_std_r2, test_mean_r2 + test_std_r2, alpha=0.2)\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.title(\"Test R2 across seeds\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot MAE\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(train_sizes_abs, test_mean_mae, \"o-\", label=\"Test MAE\")\n",
    "plt.fill_between(train_sizes_abs, test_mean_mae - test_std_mae, test_mean_mae + test_std_mae, alpha=0.2)\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"Test MAE across seeds\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5476247c",
   "metadata": {},
   "source": [
    "### 3.5 Regularization: Lasso and Ridge\n",
    "\n",
    "Now, instead of using **Linear Regression**, we can also experiment with other models such as **Lasso Regression**. These alternatives add regularization, which helps prevent overfitting by penalizing overly complex models.  \n",
    "\n",
    "So, how does `.fit(X, y)` work?  \n",
    "\n",
    "When you call `model.fit(X, y)`, the following steps occur:  \n",
    "\n",
    "1. **Model receives the data**  \n",
    "   - **X**: the feature matrix (input variables).  \n",
    "   - **y**: the target values (labels you want the model to predict).  \n",
    "\n",
    "2. **Optimization process**  \n",
    "   - **Linear Regression**: finds the line, plane, or hyperplane that minimizes the **Mean Squared Error (MSE)** between predictions and true values.  \n",
    "   - **Ridge Regression**: minimizes MSE but adds an **L2 penalty** (squares of the coefficients) to shrink coefficients and control variance.  \n",
    "   - **Lasso Regression**: minimizes MSE but adds an **L1 penalty** (absolute values of the coefficients), which can drive some coefficients exactly to zero, effectively performing **feature selection**.  \n",
    "\n",
    "This optimization is usually solved through iterative algorithms that adjust coefficients until the cost function reaches its minimum.  \n",
    "\n",
    "\n",
    "Now, instead of linear regression, we can also try other, such as lasso regression.\n",
    "\n",
    "\n",
    "```{admonition} Losses\n",
    "- **Linear**  \n",
    "  $\n",
    "  \\hat{y} = w^\\top x + b,\\quad\n",
    "  \\mathrm{Loss} = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2\n",
    "  $\n",
    "\n",
    "- **Lasso**  \n",
    "  $\n",
    "  \\mathrm{Loss} = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 + \\alpha \\sum_j \\lvert w_j \\rvert\n",
    "  $\n",
    "\n",
    "- **Ridge**  \n",
    "  $\n",
    "  \\mathrm{Loss} = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 + \\alpha \\sum_j w_j^2\n",
    "  $\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2681b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model (you can adjust alpha to control regularization strength)\n",
    "reg_lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit to training data\n",
    "reg_lasso .fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = reg_lasso .predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_lasso = mean_squared_error(y_test, y_pred)\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred)\n",
    "r2_lasso = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"For Lasso regression:\")\n",
    "print(\"MSE:\", mse_lasso)\n",
    "print(\"MAE:\", mae_lasso)\n",
    "print(\"R2:\", r2_lasso)\n",
    "print(\"--------------\")\n",
    "print(\"For Linear regression:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f572b",
   "metadata": {},
   "source": [
    "You can see here that, in fact, **Lasso Regression performs slightly better Linear Regression** in this particular example. You can also try changing alpha to 0.1, 1.0, 100 to see any difference.\n",
    "\n",
    "The prediction rule for Linear Regression is:  \n",
    "\n",
    "$$\n",
    "\\hat{y} = w_1x_1 + w_2x_2 + \\dots + w_px_p + b\n",
    "$$\n",
    "\n",
    "We will also look at **Ridge Regression**, which adds an L2 penalty to the loss function:  \n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^p w_j^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99898ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Initialize model (tune alpha for regularization strength)\n",
    "reg_ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit to training data\n",
    "reg_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_ridge = reg_ridge.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"For Ridge regression:\")\n",
    "print(\"MSE:\", mse_ridge)\n",
    "print(\"MAE:\", mae_ridge)\n",
    "print(\"R2:\", r2_ridge)\n",
    "print(\"--------------\")\n",
    "print(\"For Linear regression:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8af52a",
   "metadata": {},
   "source": [
    "We can see here that the models have very similar performance.  \n",
    "\n",
    "Now, what about predicting actual values such as **solubility**?  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 4. Another regression target: solubility in log space\n",
    "\n",
    "Let’s try doing the same process by defining the following molecular descriptors as our input features (**X**):  \n",
    "\n",
    "- `molwt` (molecular weight)  \n",
    "- `logp` (partition coefficient)  \n",
    "- `tpsa` (topological polar surface area)  \n",
    "- `numrings` (number of rings)  \n",
    "\n",
    "Our target (**y**) will be the **solubility** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcbda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select features (X) and target (y)\n",
    "X = df[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]]\n",
    "y = df[\"Solubility_mol_per_L\"]\n",
    "\n",
    "# 2. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Linear Regression\n",
    "reg_linear = LinearRegression()\n",
    "reg_linear.fit(X_train, y_train)\n",
    "y_pred_linear = reg_linear.predict(X_test)\n",
    "\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "print(\"For Linear regression:\")\n",
    "print(\"MSE:\", mse_linear)\n",
    "print(\"MAE:\", mae_linear)\n",
    "print(\"R2:\", r2_linear)\n",
    "print(\"--------------\")\n",
    "\n",
    "# 4. Ridge Regression\n",
    "reg_ridge = Ridge(alpha=1.0)\n",
    "reg_ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = reg_ridge.predict(X_test)\n",
    "\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"For Ridge regression:\")\n",
    "print(\"MSE:\", mse_ridge)\n",
    "print(\"MAE:\", mae_ridge)\n",
    "print(\"R2:\", r2_ridge)\n",
    "print(\"--------------\")\n",
    "\n",
    "# 5. Lasso Regression\n",
    "reg_lasso = Lasso(alpha=0.01, max_iter=10000)  # alpha can be tuned\n",
    "reg_lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = reg_lasso.predict(X_test)\n",
    "\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(\"For Lasso regression:\")\n",
    "print(\"MSE:\", mse_lasso)\n",
    "print(\"MAE:\", mae_lasso)\n",
    "print(\"R2:\", r2_lasso)\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2dd10a",
   "metadata": {},
   "source": [
    "The results here are very poor, with a strongly negative $R^2$ value.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Stabilize with logs\n",
    "Targets like solubility are commonly modeled as `logS`. Taking logs reduces the influence of extreme values and can improve fit quality.\n",
    "```\n",
    "So instead of fitting the solubility values directly, we transform them using:  \n",
    "\n",
    "$$\n",
    "y' = \\log_{10}(\\text{Solubility})\n",
    "$$\n",
    "\n",
    "This way, we predict $y'$ (log-scaled solubility) rather than the raw solubility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c32294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select features (X) and target (y)\n",
    "X = df[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]]\n",
    "\n",
    "######################### ##################### #####################\n",
    "y = np.log10(df[\"Solubility_mol_per_L\"] + 1e-6)  # avoid log(0)\n",
    "######################### all other code stay the same #####################\n",
    "\n",
    "\n",
    "# 2. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Linear Regression\n",
    "reg_linear = LinearRegression()\n",
    "reg_linear.fit(X_train, y_train)\n",
    "y_pred_linear = reg_linear.predict(X_test)\n",
    "\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "print(\"For Linear regression:\")\n",
    "print(\"MSE:\", mse_linear)\n",
    "print(\"MAE:\", mae_linear)\n",
    "print(\"R2:\", r2_linear)\n",
    "print(\"--------------\")\n",
    "\n",
    "# 4. Ridge Regression\n",
    "reg_ridge = Ridge(alpha=1.0)\n",
    "reg_ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = reg_ridge.predict(X_test)\n",
    "\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"For Ridge regression:\")\n",
    "print(\"MSE:\", mse_ridge)\n",
    "print(\"MAE:\", mae_ridge)\n",
    "print(\"R2:\", r2_ridge)\n",
    "print(\"--------------\")\n",
    "\n",
    "# 5. Lasso Regression\n",
    "reg_lasso = Lasso(alpha=0.01, max_iter=10000)  # alpha can be tuned\n",
    "reg_lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = reg_lasso.predict(X_test)\n",
    "\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(\"For Lasso regression:\")\n",
    "print(\"MSE:\", mse_lasso)\n",
    "print(\"MAE:\", mae_lasso)\n",
    "print(\"R2:\", r2_lasso)\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149480d",
   "metadata": {},
   "source": [
    "Now, much better, right? Now, what happen if instead transforming `y`, you actually transforming `x`? Try it by yourself after class.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 5. Binary classification: toxicity\n",
    "\n",
    "We turn to a yes or no outcome, using the same four descriptors. Logistic Regression outputs probabilities. A threshold converts those into class predictions.\n",
    "\n",
    "We will build a binary classifier for **Toxicity** using the pre-built table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084daa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf_tox =df[[\"Compound Name\", \"MolWt\", \"LogP\", \"TPSA\", \"NumRings\",\"Toxicity\"]]\n",
    "df_clf_tox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5718271d",
   "metadata": {},
   "source": [
    "```{admonition} Encoding\n",
    "Map text labels to numbers so that the model can learn from them. Keep an eye on class balance.\n",
    "```\n",
    "\n",
    "We will perform the following steps:  \n",
    "\n",
    "1. **Map labels to numeric values**  \n",
    "   - `toxic` → 1  \n",
    "   - `non_toxic` → 0  \n",
    "\n",
    "2. **Select features for training**  \n",
    "   - `MolWt` (Molecular Weight)  \n",
    "   - `LogP` (Partition Coefficient)  \n",
    "   - `TPSA` (Topological Polar Surface Area)  \n",
    "   - `NumRings` (Number of Rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Label encode\n",
    "label_map = {\"toxic\": 1, \"non_toxic\": 0}\n",
    "y = df_clf_tox[\"Toxicity\"].str.lower().map(label_map).astype(int)\n",
    "\n",
    "# Feature matrix\n",
    "X = df_clf_tox[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]].values\n",
    "\n",
    "# Just to be sure there are no infinities\n",
    "mask_finite = np.isfinite(X).all(axis=1)\n",
    "X = X[mask_finite]\n",
    "y = y[mask_finite]\n",
    "\n",
    "X[:3], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b279ce",
   "metadata": {},
   "source": [
    "When splitting the data into training and test sets, we will use **stratification**.  \n",
    "\n",
    "\n",
    "```{admonition} Why stratification\n",
    "Stratification ensures that the proportion of labels (toxic vs non-toxic) remains approximately the same in both the training and test sets. This prevents issues where one split might have many more examples of one class than the other, which could bias model evaluation.  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba64b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
    "print(\"Train class balance:\", y_train.mean().round(3), \" 1 = toxic\")\n",
    "print(\"Test class balance:\", y_test.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61d8ca",
   "metadata": {},
   "source": [
    "The name **LogisticRegression** can be a bit misleading. Even though it contains the word *regression*, it is **not** used for predicting continuous values.  \n",
    "\n",
    "```{admonition} Difference\n",
    "Logistic Regression → for classification (e.g., spam vs. not spam, toxic vs. not toxic). It outputs probabilities between `0` and `1`. A threshold (commonly `0.5`) is then applied to assign class labels.  \n",
    "\n",
    "Linear Regression → for regression (predicting continuous numbers, like prices, scores, or temperatures).\n",
    "```\n",
    "\n",
    "\n",
    "Remember:\n",
    "Logistic Regression is for classification. It models probability of class 1. Linear Regression is for continuous targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and probabilities\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]  # probability of toxic = 1\n",
    "y_pred,  y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f99b58",
   "metadata": {},
   "source": [
    "```{admonition} Metrics for classification\n",
    "**Accuracy**: fraction of correct predictions.  \n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Precision**: among predicted toxic, how many are truly toxic.  \n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Recall**: among truly toxic, how many we caught.  \n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**F1**: harmonic mean of precision and recall.  \n",
    "\n",
    "$$\n",
    "\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**AUC**: area under the ROC curve. Measures ranking of positives vs negatives over all thresholds.  \n",
    "```\n",
    "\n",
    "Now we can exame metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"Accuracy:  {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall:    {rec:.3f}\")\n",
    "print(f\"F1:        {f1:.3f}\")\n",
    "print(f\"AUC:       {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e9f3f",
   "metadata": {},
   "source": [
    "```{admonition} Confusion matrix and ROC\n",
    "Inspect the mix of true vs predicted labels and visualize how sensitivity and specificity trade off across thresholds.\n",
    "```\n",
    "\n",
    "By default, most classifiers (such as Logistic Regression) use a threshold of **0.5**:  \n",
    "- If predicted probability ≥ 0.5 → class = 1 (toxic)  \n",
    "- If predicted probability < 0.5 → class = 0 (non-toxic)  \n",
    "\n",
    "However, we can **change the threshold** depending on the problem:  \n",
    "- Lowering the threshold (e.g., 0.3) increases sensitivity (recall), catching more positives but with more false positives.  \n",
    "- Raising the threshold (e.g., 0.7) increases precision, reducing false positives but possibly missing some true positives.  \n",
    "\n",
    "This trade-off is important in real-world settings. For example:  \n",
    "- In medical screening, we may prefer higher recall (catch all possible cases).  \n",
    "- In spam filtering, we may prefer higher precision (avoid marking valid emails as spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e190f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thr = roc_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"AUC = {auc:.3f}\")\n",
    "plt.plot([0,1],[0,1], \"k--\", lw=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "threshold = 0.35  # try 0.3, 0.5, 0.7\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "pred = (proba >= threshold).astype(int)\n",
    "\n",
    "print(f\"threshold: {threshold:.3f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, pred):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, pred):.3f}\")\n",
    "print(f\"F1: {f1_score(y_test, pred):.3f}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, proba):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe71502",
   "metadata": {},
   "source": [
    "Depending on how you set the classification threshold, the evaluation metrics will change.  \n",
    "\n",
    "- If you use a **threshold = 0.5**, you will obtain exactly the same results as before (the default behavior).  \n",
    "- Adjusting the threshold upward or downward will shift the balance between **precision** and **recall**, leading to different values for accuracy, F1 score, and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7770d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities from your classifier\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Threshold values\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "# Store results\n",
    "accuracy, precision, recall, f1 = [], [], [], []\n",
    "\n",
    "for t in thresholds:\n",
    "    pred = (proba >= t).astype(int)\n",
    "    accuracy.append(accuracy_score(y_test, pred))\n",
    "    precision.append(precision_score(y_test, pred))\n",
    "    recall.append(recall_score(y_test, pred))\n",
    "    f1.append(f1_score(y_test, pred))\n",
    "\n",
    "# Plot metrics vs thresholds\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, accuracy, marker='o', label='Accuracy')\n",
    "plt.plot(thresholds, precision, marker='o', label='Precision')\n",
    "plt.plot(thresholds, recall, marker='o', label='Recall')\n",
    "plt.plot(thresholds, f1, marker='o', label='F1 Score')\n",
    "\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Metrics at Different Thresholds\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbc421",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. From regression to classes: melting point bins\n",
    "\n",
    "\n",
    "\n",
    "Now, let's think about this:\n",
    "**Can regression question be turning into classificiation?**\n",
    "> Turning `Melting Point` Regression into a 3-Class Classification Task\n",
    "\n",
    "So far we treated melting point (MP) as a continuous variable and built regression models. Another approach is to discretize MP into categories and reframe the task as classification. This can be useful if we only need a decision (e.g., low vs. medium vs. high melting point) rather than an exact temperature.\n",
    "\n",
    "We split melting points into three bins:  \n",
    "> **Class 0 (Low):** MP ≤ 100 °C  \n",
    "> **Class 1 (Medium):** 100 < MP ≤ 200 °C  \n",
    "> **Class 2 (High):** MP > 200 °C  \n",
    "\n",
    "\n",
    "This creates a categorical target suitable for classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: same as before\n",
    "X = df_reg_mp[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]].values\n",
    "\n",
    "# Define categorical target\n",
    "mp = df_reg_mp[\"Melting Point\"].values\n",
    "y3 = pd.cut(\n",
    "    mp,\n",
    "    bins=[-np.inf, 100, 200, np.inf],\n",
    "    labels=[0, 1, 2],\n",
    "    right=True,\n",
    "    include_lowest=True\n",
    ").astype(int)\n",
    "y3\n",
    "\n",
    "# Train/test split with stratification (preserves class proportions)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y3, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train,  y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6305906",
   "metadata": {},
   "source": [
    "```{admonition} Multinomial logistic regression\n",
    "The logistic model extends naturally to more than two classes. Scikit-learn handles this under the hood.\n",
    "```\n",
    "\n",
    "Now we can train a **Logistic Regression** model on the melting point classification task.  \n",
    "\n",
    "Logistic Regression is not limited to binary problems — it can be extended to handle **multiple classes**.  \n",
    "- In the **multinomial** setting, the model learns separate decision boundaries for each class.  \n",
    "- Each class receives its own probability, and the model assigns the label with the highest probability.  \n",
    "\n",
    "This allows us to predict whether a compound falls into **low**, **medium**, or **high** melting point categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538173c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = LogisticRegression(max_iter=1000)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf3.predict(X_test)\n",
    "y_proba = clf3.predict_proba(X_test)  # class probabilities\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average=\"macro\")\n",
    "rec = recall_score(y_test, y_pred, average=\"macro\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Accuracy:  {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall:    {rec:.3f}\")\n",
    "print(f\"F1:        {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b1023",
   "metadata": {},
   "source": [
    "```{admonition} Averaging choices\n",
    "For multiple classes there are different ways to average metrics across classes. **Macro** gives each class equal weight, **micro** aggregates counts, and **weighted** weights by class frequency.\n",
    "```\n",
    "\n",
    "Note: When moving from **binary classification** to **multi-class classification**, metrics like precision, recall, and F1 score cannot be defined in just one way.  \n",
    "You need to decide **how to average** them across multiple classes. This is where strategies such as **macro**, **micro**, and **weighted** averaging come into play.  \n",
    "\n",
    "### Macro Averaging  \n",
    "- Compute the metric (precision, recall, or F1) **for each class separately**.  \n",
    "- Take the **simple, unweighted average** across all classes.  \n",
    "- Every class contributes equally, regardless of how many samples it has.  \n",
    "\n",
    "**Example:**  \n",
    "Suppose we have 3 classes:  \n",
    "- Class 0: 500 samples  \n",
    "- Class 1: 100 samples  \n",
    "- Class 2: 50 samples  \n",
    "\n",
    "If the model performs very well on **Class 0** (the large class) but very poorly on **Class 2** (the small class), **macro averaging** will penalize the model.  \n",
    "This is because each class’s F1 score contributes equally to the final average, even though the class sizes are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619335ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix (counts)\n",
    "labels = [0, 1, 2]  # 0: ≤100, 1: 100–200, 2: >200\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "plt.figure(figsize=(4.8, 4.2))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - MP classes\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "\n",
    "# Tick labels for bins\n",
    "xticks = [\"≤100\", \"100–200\", \">200\"]\n",
    "yticks = [\"≤100\", \"100–200\", \">200\"]\n",
    "plt.xticks(np.arange(len(labels)), xticks, rotation=0)\n",
    "plt.yticks(np.arange(len(labels)), yticks)\n",
    "\n",
    "# Annotations\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: normalized confusion matrix (row-wise)\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(4.8, 4.2))\n",
    "plt.imshow(cm_norm, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "plt.title(\"Confusion Matrix (Normalized)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.xticks(np.arange(len(labels)), xticks, rotation=0)\n",
    "plt.yticks(np.arange(len(labels)), yticks)\n",
    "\n",
    "for i in range(cm_norm.shape[0]):\n",
    "    for j in range(cm_norm.shape[1]):\n",
    "        plt.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ca8d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## 8. Quick reference\n",
    "\n",
    "```{admonition} Model recipes\n",
    "- **LinearRegression**: `fit(X_train, y_train)` then `predict(X_test)`\n",
    "- **Ridge(alpha=1.0)**: same API as LinearRegression\n",
    "- **Lasso(alpha=...)**: same API, can shrink some coefficients to zero\n",
    "- **LogisticRegression(max_iter=...)**: `predict` for labels, `predict_proba` for probabilities\n",
    "```\n",
    "\n",
    "```{admonition} Metrics at a glance\n",
    "- Regression: **MSE**, **MAE**, **R²**\n",
    "- Classification: **Accuracy**, **Precision**, **Recall**, **F1**, **AUC**\n",
    "- Visuals: **residual plot**, **parity plot**, **confusion matrix**, **ROC**\n",
    "```\n",
    "\n",
    "```{admonition} Splits and seeds\n",
    "- `train_test_split(X, y, test_size=0.2, random_state=42)`\n",
    "- Use `stratify=y` for classification to maintain label balance\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Glossary\n",
    "\n",
    "```{glossary}\n",
    "supervised learning\n",
    "  A setup with features `X` and a labeled target `y` for each example.\n",
    "\n",
    "regression\n",
    "  Predicting a continuous number, such as a melting point.\n",
    "\n",
    "classification\n",
    "  Predicting a category, such as toxic vs non_toxic.\n",
    "\n",
    "descriptor\n",
    "  A numeric feature computed from a molecule. Examples: molecular weight, logP, TPSA, ring count.\n",
    "\n",
    "train test split\n",
    "  Partition the data into a part to fit the model and a separate part to estimate performance.\n",
    "\n",
    "regularization\n",
    "  Penalty added to the loss to discourage large weights. Lasso uses L1, Ridge uses L2.\n",
    "\n",
    "residual\n",
    "  The difference `y_true - y_pred` for a sample.\n",
    "\n",
    "ROC AUC\n",
    "  Area under the ROC curve, a threshold independent ranking score for binary classification.\n",
    "\n",
    "macro averaging\n",
    "  Average the metric per class, then take the unweighted mean across classes.\n",
    "\n",
    "parity plot\n",
    "  Scatter of predicted vs true values. Ideal points lie on the diagonal.\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 10. In-class activity\n",
    "\n",
    "\n",
    "### 10.1 Linear Regression with two features\n",
    "\n",
    "Use only `MolWt` and `TPSA` to predict **Melting Point** with Linear Regression. Use a 90/10 split and report **MSE**, **MAE**, and **R²**.\n",
    "\n",
    "```python\n",
    "X = df_reg_mp[[\"MolWt\", \"TPSA\"]]\n",
    "y = df_reg_mp[\"Melting Point\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=..., random_state=...\n",
    ")\n",
    "\n",
    "... #TO DO\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred):.3f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "print(f\"R2:  {r2_score(y_test, y_pred):.3f}\")\n",
    "```\n",
    "\n",
    "### 10.2 Ridge across splits\n",
    "\n",
    "Train a Ridge model (`alpha=1.0`) for **Melting Point** using `MolWt, LogP, TPSA, NumRings`. Compare test **R²** for train sizes 60, 70, 80, 90 percent with `random_state=42`. Plot **R²** vs train percent.\n",
    "\n",
    "```python\n",
    "X = ... #TO DO\n",
    "y = ... #TO DO\n",
    "\n",
    "splits = [...]  # corresponds to 60/40, 70/30, 80/20, 90/10\n",
    "r2_scores = [] # empty, no need to modify\n",
    "\n",
    "for t in splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=t, random_state=... ... #TO DO\n",
    "    )\n",
    "    model = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([60,70,80,90], r2_scores, \"o-\", lw=2)\n",
    "plt.xlabel(\"Train %\")\n",
    "plt.ylabel(\"R² (test)\")\n",
    "plt.title(\"Effect of train/test split on Ridge Regression accuracy\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 10.3 pKa regression two ways\n",
    "\n",
    "Build Ridge regression for **pKa** using the same four descriptors. Report **R²** and **MSE** for each.\n",
    "\n",
    "```python\n",
    "... #TO DO\n",
    "```\n",
    "\n",
    "### 10.4 pKa to classification\n",
    "\n",
    "Turn **pKa** into a binary label and train Logistic Regression with the same descriptors. Report Accuracy, Precision, Recall, F1, and AUC, and draw the ROC. You may pick either rule.\n",
    "\n",
    "- Option A: acidic if pKa ≤ 7  \n",
    "- Option B: median split on pKa\n",
    "\n",
    "```python\n",
    "... #TO DO\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 10. In-class activity\n",
    "\n",
    "\n",
    "### 10.1 Linear Regression with two features\n",
    "\n",
    "Use only `MolWt` and `TPSA` to predict **Melting Point** with Linear Regression. Use a 90/10 split and report **MSE**, **MAE**, and **R²**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb10418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 starter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "X = df_reg_mp[[\"MolWt\", \"TPSA\"]]\n",
    "y = df_reg_mp[\"Melting Point\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=0\n",
    ")\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred):.3f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "print(f\"R2:  {r2_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718af81",
   "metadata": {},
   "source": [
    "### 10.2 Ridge across splits\n",
    "\n",
    "Train a Ridge model (`alpha=1.0`) for **Melting Point** using `MolWt, LogP, TPSA, NumRings`. Compare test **R²** for train sizes 60, 70, 80, 90 percent with `random_state=42`. Plot **R²** vs train percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cf8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reg_mp[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]].values\n",
    "y = df_reg_mp[\"Melting Point\"].values\n",
    "\n",
    "splits = [0.4, 0.3, 0.2, 0.1]  # corresponds to 60/40, 70/30, 80/20, 90/10\n",
    "r2_scores = []\n",
    "\n",
    "for t in splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=t, random_state=42\n",
    "    )\n",
    "    model = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot([60,70,80,90], r2_scores, \"o-\", lw=2)\n",
    "plt.xlabel(\"Train %\")\n",
    "plt.ylabel(\"R² (test)\")\n",
    "plt.title(\"Effect of train/test split on Ridge Regression accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db141b6c",
   "metadata": {},
   "source": [
    "### 10.3 pKa regression two ways\n",
    "\n",
    "Build Ridge regression for **pKa** and for **exp(pKa)** using the same four descriptors. Report **R²** and **MSE** for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Keep rows with a valid pKa\n",
    "df_pka = df[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\", \"pKa\"]].dropna()\n",
    "\n",
    "X = df_pka[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]].values\n",
    "y = df_pka[\"pKa\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "model = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Test R2:  {r2_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, y_pred):.3f}\")\n",
    "\n",
    "# Parity plot\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True pKa\")\n",
    "plt.ylabel(\"Predicted pKa\")\n",
    "plt.title(\"Parity plot for pKa regression (Ridge)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24ad4e",
   "metadata": {},
   "source": [
    "### 10.4 pKa to classification\n",
    "\n",
    "Turn **pKa** into a binary label and train Logistic Regression with the same descriptors. Report Accuracy, Precision, Recall, F1, and AUC, and draw the ROC. You may pick either rule.\n",
    "\n",
    "- Option A: acidic if pKa ≤ 7  \n",
    "- Option B: median split on pKa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean pKa subset\n",
    "df_pka = df[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\", \"pKa\"]].dropna()\n",
    "X = df_pka[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]].values\n",
    "pka_vals = df_pka[\"pKa\"].values\n",
    "\n",
    "# ---- Helper to run classification and plot ----\n",
    "def run_classification(y_cls, rule_name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_cls, test_size=0.20, random_state=42, stratify=y_cls\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc  = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    print(f\"--- {rule_name} ---\")\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1:        {f1:.3f}\")\n",
    "    print(f\"AUC:       {auc:.3f}\")\n",
    "    print()\n",
    "\n",
    "    # ROC plot\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"AUC = {auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1], \"k--\", lw=1)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve for pKa classification ({rule_name})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---- Rule A: acidic if pKa ≤ 7 ----\n",
    "y_cls_A = (pka_vals <= 7.0).astype(int)\n",
    "run_classification(y_cls_A, \"Rule A (pKa ≤ 7 = acidic)\")\n",
    "\n",
    "# ---- Rule B: median split ----\n",
    "median_val = np.median(pka_vals)\n",
    "y_cls_B = (pka_vals <= median_val).astype(int)\n",
    "run_classification(y_cls_B, f\"Rule B (≤ median pKa = acidic, median={median_val:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "source_map": [
   12,
   33,
   68,
   91,
   95,
   114,
   137,
   150,
   153,
   168,
   185,
   197,
   209,
   217,
   235,
   273,
   286,
   297,
   315,
   322,
   374,
   392,
   459,
   506,
   530,
   547,
   574,
   597,
   651,
   668,
   726,
   739,
   742,
   761,
   778,
   790,
   798,
   815,
   825,
   866,
   883,
   902,
   940,
   949,
   979,
   1002,
   1023,
   1038,
   1057,
   1081,
   1125,
   1268,
   1289,
   1295,
   1317,
   1322,
   1354,
   1363
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}