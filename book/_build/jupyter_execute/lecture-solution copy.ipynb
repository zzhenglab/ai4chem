{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ec4b1e",
   "metadata": {},
   "source": [
    "## 8. Solutions\n",
    "\n",
    "\n",
    "### 8.1 Tree vs Forest on log-solubility\n",
    "\n",
    "Goal: predict log-solubility and compare a small tree to a forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01098df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create target: log10(solubility + 1e-6) to avoid log(0)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# If your dataframe already has a numeric solubility column named 'Solubility_mol_per_L', reuse it.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_sol = \u001b[43mdf\u001b[49m.copy()\n\u001b[32m      4\u001b[39m df_sol[\u001b[33m\"\u001b[39m\u001b[33my_log\u001b[39m\u001b[33m\"\u001b[39m] = np.log10(df_sol[\u001b[33m\"\u001b[39m\u001b[33mSolubility_mol_per_L\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1e-6\u001b[39m)\n\u001b[32m      6\u001b[39m Xs = df_sol[[\u001b[33m\"\u001b[39m\u001b[33mMolWt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLogP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTPSA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNumRings\u001b[39m\u001b[33m\"\u001b[39m]].dropna()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Create target: log10(solubility + 1e-6) to avoid log(0)\n",
    "# If your dataframe already has a numeric solubility column named 'Solubility_mol_per_L', reuse it.\n",
    "df_sol = df.copy()\n",
    "df_sol[\"y_log\"] = np.log10(df_sol[\"Solubility_mol_per_L\"] + 1e-6)\n",
    "\n",
    "Xs = df_sol[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]].dropna()\n",
    "ys = df_sol.loc[Xs.index, \"y_log\"]\n",
    "\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(\n",
    "    Xs, ys, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Models\n",
    "tree_sol = DecisionTreeRegressor(max_depth=4, min_samples_leaf=5, random_state=0).fit(Xs_train, ys_train)\n",
    "rf_sol   = RandomForestRegressor(n_estimators=300, min_samples_leaf=5, random_state=0, n_jobs=-1).fit(Xs_train, ys_train)\n",
    "\n",
    "# Scores\n",
    "yhat_tree = tree_sol.predict(Xs_test)\n",
    "yhat_rf   = rf_sol.predict(Xs_test)\n",
    "\n",
    "print(f\"Tree R2:   {r2_score(ys_test, yhat_tree):.3f}\")\n",
    "print(f\"Forest R2: {r2_score(ys_test, yhat_rf):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65903a78",
   "metadata": {},
   "source": [
    "Parity plots for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity for tree\n",
    "plt.scatter(ys_test, yhat_tree, alpha=0.6)\n",
    "lims = [min(ys_test.min(), yhat_tree.min()), max(ys_test.max(), yhat_tree.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Parity plot: Tree on log-solubility\")\n",
    "plt.show()\n",
    "\n",
    "# Parity for forest\n",
    "plt.scatter(ys_test, yhat_rf, alpha=0.6)\n",
    "lims = [min(ys_test.min(), yhat_rf.min()), max(ys_test.max(), yhat_rf.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Parity plot: Forest on log-solubility\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60e451",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.2 Pruning with `min_samples_leaf`\n",
    "\n",
    "Fix `max_depth=None` for a classifier on toxicity and sweep leaf size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896860e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_grid = [1, 2, 3, 5, 8, 12, 20]\n",
    "accs = []\n",
    "\n",
    "for leaf in leaf_grid:\n",
    "    clf = DecisionTreeClassifier(max_depth=None, min_samples_leaf=leaf, random_state=0).fit(X_train, y_train)\n",
    "    accs.append(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "pd.DataFrame({\"min_samples_leaf\": leaf_grid, \"Accuracy\": np.round(accs, 3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceae479",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(leaf_grid, accs, marker=\"o\")\n",
    "plt.xlabel(\"min_samples_leaf\")\n",
    "plt.ylabel(\"Accuracy (test)\")\n",
    "plt.title(\"Pruning with min_samples_leaf\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8514fe4",
   "metadata": {},
   "source": [
    "Hint for interpretation: very small leaves may overfit while very large leaves may underfit.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.3 Toxicity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd50c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 7, 21, 42]\n",
    "rows_oob = []\n",
    "\n",
    "for s in seeds:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=s, stratify=y)\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300, max_features=\"sqrt\", min_samples_leaf=3,\n",
    "        oob_score=True, random_state=s, n_jobs=-1\n",
    "    ).fit(X_tr, y_tr)\n",
    "    acc_test = accuracy_score(y_te, rf.predict(X_te))\n",
    "    rows_oob.append({\"seed\": s, \"OOB\": rf.oob_score_, \"TestAcc\": acc_test})\n",
    "\n",
    "pd.DataFrame(rows_oob).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a92df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oob = pd.DataFrame(rows_oob)\n",
    "plt.plot(df_oob[\"seed\"], df_oob[\"OOB\"], \"o-\", label=\"OOB\")\n",
    "plt.plot(df_oob[\"seed\"], df_oob[\"TestAcc\"], \"o-\", label=\"Test\")\n",
    "plt.xlabel(\"random_state\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"OOB vs Test accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tree = DecisionTreeClassifier(max_depth=2, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plot_tree(small_tree, feature_names=feat_names, class_names=[\"non_toxic\",\"toxic\"], filled=True)\n",
    "plt.title(\"Small Decision Tree (max_depth=2)\")\n",
    "plt.show()\n",
    "\n",
    "# Extract split rules programmatically for the top two levels\n",
    "feat_idx = small_tree.tree_.feature\n",
    "thresh = small_tree.tree_.threshold\n",
    "left = small_tree.tree_.children_left\n",
    "right = small_tree.tree_.children_right\n",
    "\n",
    "def node_rule(node_id):\n",
    "    f = feat_idx[node_id]\n",
    "    t = thresh[node_id]\n",
    "    return f\"{feat_names[f]} <= {t:.3f} ?\"\n",
    "\n",
    "print(\"Root rule:\", node_rule(0))\n",
    "print(\"Left child rule:\", node_rule(left[0]) if left[0] != -1 else \"Left child is a leaf\")\n",
    "print(\"Right child rule:\", node_rule(right[0]) if right[0] != -1 else \"Right child is a leaf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f466f11",
   "metadata": {},
   "source": [
    "Expect OOB to track test accuracy closely. Small differences are normal.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.4 Feature importance agreement on melting point\n",
    "\n",
    "Compare built-in importance to permutation importance for a random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efb665",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_imp = RandomForestRegressor(\n",
    "    n_estimators=400, min_samples_leaf=3, max_features=\"sqrt\",\n",
    "    random_state=0, n_jobs=-1\n",
    ").fit(Xr_train, yr_train)\n",
    "\n",
    "# Built-in importance\n",
    "imp_series = pd.Series(rf_imp.feature_importances_, index=Xr_train.columns).sort_values()\n",
    "\n",
    "# Permutation importance on test\n",
    "perm_r = permutation_importance(\n",
    "    rf_imp, Xr_test, yr_test, scoring=\"r2\", n_repeats=20, random_state=0\n",
    ")\n",
    "perm_series = pd.Series(perm_r.importances_mean, index=Xr_train.columns).sort_values()\n",
    "\n",
    "# Plots\n",
    "imp_series.plot(kind=\"barh\")\n",
    "plt.title(\"Random Forest feature_importances_ (regression)\")\n",
    "plt.show()\n",
    "\n",
    "perm_series.plot(kind=\"barh\")\n",
    "plt.title(\"Permutation importance on test (regression)\")\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame({\"Built_in\": imp_series, \"Permutation\": perm_series})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6218739",
   "metadata": {},
   "source": [
    "Look for agreement on the top features. Disagreements can signal correlation or overfitting in the training trees.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.5 CV on RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = df_clf[[\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]]\n",
    "y = df_clf[\"Toxicity\"].str.lower().map({\"toxic\":1, \"non_toxic\":0}).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=15, stratify=y\n",
    ")\n",
    "\n",
    "# CV setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 400],\n",
    "    \"max_depth\": [None, 6, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 5],\n",
    "    \"max_features\": [\"sqrt\", 0.8],\n",
    "}\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    oob_score=False, random_state=0, n_jobs=-1\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    rf_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(f\"Best CV AUC: {grid.best_score_:.3f}\")\n",
    "\n",
    "# Refit on full training data already done by refit=True\n",
    "rf_best = grid.best_estimator_\n",
    "\n",
    "# Test metrics\n",
    "y_hat = rf_best.predict(X_test)\n",
    "y_proba = rf_best.predict_proba(X_test)[:, 1]\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_hat):.3f}\")\n",
    "print(f\"Test AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "# ROC plot\n",
    "fpr, tpr, thr = roc_curve(y_test, y_proba)\n",
    "plt.plot(fpr, tpr, lw=2)\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC curve - RF with CV-tuned hyperparameters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df455d",
   "metadata": {},
   "source": [
    "Read the rules as binary questions. Samples that satisfy a rule go left. Others go right.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Solutions\n",
    "\n",
    "### Solution Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Melting Point\"]].dropna()\n",
    "X = df_reg[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "y = df_reg[\"Melting Point\"].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPRegressor(hidden_layer_sizes=(32,), activation=\"relu\",\n",
    "                         alpha=1e-3, learning_rate_init=0.01,\n",
    "                         max_iter=1500, random_state=0))\n",
    "]).fit(Xtr, ytr)\n",
    "\n",
    "yhat = pipe.predict(Xte)\n",
    "print(f\"MSE={mean_squared_error(yte,yhat):.2f}  MAE={mean_absolute_error(yte,yhat):.2f}  R2={r2_score(yte,yhat):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(4.5,4))\n",
    "plt.scatter(yte, yhat, alpha=0.65)\n",
    "lims = [min(yte.min(), yhat.min()), max(yte.max(), yhat.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True MP\"); plt.ylabel(\"Pred MP\"); plt.title(\"Q1 parity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f444e0d",
   "metadata": {},
   "source": [
    "### Solution Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542dc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [(16,), (32,), (64,32)]\n",
    "df_sol = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Solubility_mol_per_L\"]].dropna().copy()\n",
    "df_sol[\"logS\"] = np.log10(df_sol[\"Solubility_mol_per_L\"]+1e-6)\n",
    "X = df_sol[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "y = df_sol[\"logS\"].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "\n",
    "r2s, curves = [], []\n",
    "for sz in sizes:\n",
    "    reg = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"mlp\", MLPRegressor(hidden_layer_sizes=sz, activation=\"relu\",\n",
    "                             alpha=1e-3, learning_rate_init=0.01,\n",
    "                             early_stopping=True, validation_fraction=0.15,\n",
    "                             max_iter=3000, random_state=0))\n",
    "    ]).fit(Xtr, ytr)\n",
    "    yhat = reg.predict(Xte)\n",
    "    r2s.append(r2_score(yte, yhat))\n",
    "    curves.append(reg.named_steps[\"mlp\"].loss_curve_)\n",
    "\n",
    "print(pd.DataFrame({\"hidden_sizes\":[str(s) for s in sizes],\"R2\":np.round(r2s,3)}))\n",
    "\n",
    "plt.figure(figsize=(5.5,3.5))\n",
    "for sz, c in zip(sizes, curves):\n",
    "    plt.plot(c, label=str(sz))\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Q2 loss curves\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2488675",
   "metadata": {},
   "source": [
    "### Solution Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c73b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Toxicity\"]].dropna()\n",
    "y = df_clf[\"Toxicity\"].str.lower().map({\"toxic\":1,\"non_toxic\":0}).astype(int).values\n",
    "X = df_clf[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPClassifier(hidden_layer_sizes=(32,), activation=\"relu\",\n",
    "                          alpha=1e-3, learning_rate_init=0.01,\n",
    "                          early_stopping=True, validation_fraction=0.15,\n",
    "                          max_iter=3000, random_state=0))\n",
    "]).fit(Xtr, ytr)\n",
    "\n",
    "proba = clf.predict_proba(Xte)[:,1]\n",
    "for t in [0.3, 0.5, 0.7]:\n",
    "    pred = (proba >= t).astype(int)\n",
    "    print(f\"t={t:.1f}  acc={accuracy_score(yte,pred):.3f}  prec={precision_score(yte,pred):.3f}  rec={recall_score(yte,pred):.3f}  f1={f1_score(yte,pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e1af2",
   "metadata": {},
   "source": [
    "### Solution Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8835ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sol = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Solubility_mol_per_L\"]].dropna().copy()\n",
    "df_sol[\"logS\"] = np.log10(df_sol[\"Solubility_mol_per_L\"]+1e-6)\n",
    "X = df_sol[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "y = df_sol[\"logS\"].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "\n",
    "sc = StandardScaler().fit(Xtr)\n",
    "Xtr_s, Xte_s = sc.transform(Xtr), sc.transform(Xte)\n",
    "\n",
    "lr = LinearRegression().fit(Xtr_s, ytr)\n",
    "yhat_lr = lr.predict(Xte_s)\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(32,), activation=\"relu\",\n",
    "                   alpha=1e-3, learning_rate_init=0.01,\n",
    "                   max_iter=3000, random_state=0).fit(Xtr_s, ytr)\n",
    "yhat_mlp = mlp.predict(Xte_s)\n",
    "\n",
    "print(f\"Linear R2: {r2_score(yte, yhat_lr):.3f}\")\n",
    "print(f\"MLP    R2: {r2_score(yte, yhat_mlp):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "plt.scatter(yte, yhat_lr, alpha=0.6, label=\"Linear\")\n",
    "plt.scatter(yte, yhat_mlp, alpha=0.6, label=\"MLP\")\n",
    "lims = [min(yte.min(), yhat_lr.min(), yhat_mlp.min()), max(yte.max(), yhat_lr.max(), yhat_mlp.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True logS\"); plt.ylabel(\"Predicted\")\n",
    "plt.legend(); plt.title(\"Q4 parity: Linear vs MLP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58486f09",
   "metadata": {},
   "source": [
    "### Solution Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Q5 (full run + metrics + plots)\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Data\n",
    "df_mp = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Melting Point\"]].dropna().copy()\n",
    "\n",
    "X = df_mp[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values.astype(np.float32)\n",
    "y = df_mp[\"Melting Point\"].values.astype(np.float32).reshape(-1,1)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_s = scaler.transform(Xtr).astype(np.float32)\n",
    "Xte_s = scaler.transform(Xte).astype(np.float32)\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(NumpyDataset(Xtr_s, ytr), batch_size=64, shuffle=True)\n",
    "\n",
    "in_dim = Xtr_s.shape[1]\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_dim, 32), nn.ReLU(),\n",
    "    nn.Linear(32, 16),     nn.ReLU(),\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "train_losses = []\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    batch_losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        batch_losses.append(loss.item())\n",
    "    train_losses.append(np.mean(batch_losses))\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat = model(torch.from_numpy(Xte_s)).numpy()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(yte, yhat):.3f}\")\n",
    "print(f\"MAE: {mean_absolute_error(yte, yhat):.3f}\")\n",
    "print(f\"R2:  {r2_score(yte, yhat):.3f}\")\n",
    "\n",
    "# Learning curve\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"train MSE\"); plt.title(\"Training loss (melting point)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Parity plot\n",
    "plt.figure(figsize=(4.6,4))\n",
    "plt.scatter(yte, yhat, alpha=0.65)\n",
    "lims = [min(yte.min(), yhat.min()), max(yte.max(), yhat.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True MP\"); plt.ylabel(\"Pred MP\"); plt.title(\"Parity plot (PyTorch MP)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "source_map": [
   12,
   28,
   51,
   55,
   73,
   81,
   92,
   99,
   109,
   125,
   138,
   162,
   172,
   197,
   207,
   264,
   277,
   299,
   303,
   331,
   335,
   353,
   357,
   386,
   390
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}