{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d724d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 10. Solutions\n",
    "\n",
    "### Solution Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945ac65b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Q1 solution\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mset_seed\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/C_H_oxidation_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m df_oxidation_raw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'set_seed' is not defined"
     ]
    }
   ],
   "source": [
    "# Q1 solution\n",
    "set_seed(0)\n",
    "url = \"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/C_H_oxidation_dataset.csv\"\n",
    "df_oxidation_raw = pd.read_csv(url)\n",
    "def calc_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return pd.Series({\n",
    "            \"MolWt\": None,\n",
    "            \"LogP\": None,\n",
    "            \"TPSA\": None,\n",
    "            \"NumRings\": None\n",
    "        })\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(mol),                    # molecular weight\n",
    "        \"LogP\": Crippen.MolLogP(mol),                       # octanol-water logP\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(mol),             # topological polar surface area\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(mol)      # number of rings\n",
    "    })\n",
    "\n",
    "# Apply the function to the SMILES column\n",
    "desc_df = df_oxidation_raw[\"SMILES\"].apply(calc_descriptors)\n",
    "\n",
    "# Concatenate new descriptor columns to original DataFrame\n",
    "df_clf = pd.concat([df_oxidation_raw, desc_df], axis=1)\n",
    "df_reg = df_clf.copy() \n",
    "\n",
    "# Logistic regression baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X = df_clf[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "y = df_clf[\"Toxicity\"].str.lower().map({\"toxic\":1,\"non_toxic\":0}).values\n",
    "Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
    "baseline = LogisticRegression(max_iter=1000).fit(Xtr,ytr)\n",
    "print(\"Baseline acc:\", accuracy_score(yte, baseline.predict(Xte)))\n",
    "\n",
    "# MLP from scratch\n",
    "class MLP_Q1(nn.Module):\n",
    "    def __init__(self,in_dim,hidden=32,n_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim,hidden)\n",
    "        self.fc2 = nn.Linear(hidden,n_classes)\n",
    "    def forward(self,x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "model = MLP_Q1(4)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    xb = torch.tensor(Xtr, dtype=torch.float32)\n",
    "    yb = torch.tensor(ytr, dtype=torch.long)\n",
    "    opt.zero_grad()\n",
    "    out = model(xb)\n",
    "    loss = loss_fn(out,yb)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "model.eval()\n",
    "ypred = model(torch.tensor(Xte, dtype=torch.float32)).argmax(1).numpy()\n",
    "print(\"MLP acc:\", accuracy_score(yte, ypred))\n",
    "#For this question and the following questions, it is fine if the model performance is worse than the baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa982da0",
   "metadata": {},
   "source": [
    "### Solution Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reg[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values.astype(np.float32)\n",
    "y = df_reg[\"Melting Point\"].values.astype(np.float32).reshape(-1,1)\n",
    "Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "# Baseline\n",
    "rf = RandomForestRegressor(n_estimators=200,random_state=0).fit(Xtr,ytr.ravel())\n",
    "print(\"Baseline RF R2:\", r2_score(yte,rf.predict(Xte)))\n",
    "\n",
    "# PyTorch MLP\n",
    "class MLPreg_Q2(nn.Module):\n",
    "    def __init__(self,in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,64), nn.ReLU(),\n",
    "            nn.Linear(64,32), nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "model = MLPreg_Q2(4)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    xb = torch.tensor(Xtr, dtype=torch.float32)\n",
    "    yb = torch.tensor(ytr, dtype=torch.float32)\n",
    "    opt.zero_grad()\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "    loss.backward(); opt.step()\n",
    "\n",
    "yhat = model(torch.tensor(Xte, dtype=torch.float32)).detach().numpy()\n",
    "print(\"MLP R2:\", r2_score(yte,yhat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db78e6",
   "metadata": {},
   "source": [
    "### Solution Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbec928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build toxicity graphs\n",
    "label_map = {\"toxic\":1, \"non_toxic\":0}\n",
    "df_tox = df[df[\"Toxicity\"].str.lower().isin(label_map.keys())].copy()\n",
    "y_bin = df_tox[\"Toxicity\"].str.lower().map(label_map).astype(int).values\n",
    "\n",
    "graphs_tox = []\n",
    "for smi, yv in zip(df_tox[\"SMILES\"], y_bin):\n",
    "    mol, x_np, ei_np, ea_np = smiles_to_graph(smi)\n",
    "    g = {\n",
    "        \"x\": torch.tensor(x_np, dtype=torch.float32),\n",
    "        \"edge_index\": torch.tensor(ei_np, dtype=torch.long),\n",
    "        \"edge_attr\": torch.tensor(ea_np, dtype=torch.long),\n",
    "        \"y\": torch.tensor(yv, dtype=torch.long)\n",
    "    }\n",
    "    graphs_tox.append(g)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(graphs_tox)),\n",
    "    test_size=0.2, random_state=42, stratify=y_bin\n",
    ")\n",
    "train_graphs = [graphs_tox[i] for i in train_idx]\n",
    "test_graphs  = [graphs_tox[i] for i in test_idx]\n",
    "\n",
    "# Baseline on descriptors: simple MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_desc = df_tox[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "Xtr_d, Xte_d = X_desc[train_idx], X_desc[test_idx]\n",
    "ytr_d, yte_d = y_bin[train_idx], y_bin[test_idx]\n",
    "\n",
    "mlp_base = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", MLPClassifier(hidden_layer_sizes=(16,),\n",
    "                          activation=\"relu\",\n",
    "                          learning_rate_init=0.01,\n",
    "                          max_iter=2000,\n",
    "                          random_state=0))\n",
    "]).fit(Xtr_d, ytr_d)\n",
    "base_acc = accuracy_score(yte_d, mlp_base.predict(Xte_d))\n",
    "\n",
    "# GNN variant: 3 layers, sum pooling, dropout\n",
    "class MPNNClassifierV3(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden=48, n_layers=3, n_classes=2, dropout=0.2, pool=\"sum\"):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        dims = [in_dim] + [hidden]*(n_layers-1) + [hidden]\n",
    "        for a, b in zip(dims[:-1], dims[1:]):\n",
    "            self.layers.append(MPNNLayer(a, b))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden, n_classes)\n",
    "        self.pool = pool\n",
    "\n",
    "    def forward(self, g):\n",
    "        x, edge_index, edge_attr = g[\"x\"], g[\"edge_index\"], g[\"edge_attr\"]\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, edge_index, edge_attr)\n",
    "            h = self.dropout(h)\n",
    "        if self.pool == \"sum\":\n",
    "            h_graph = h.sum(dim=0)\n",
    "        else:\n",
    "            h_graph = h.mean(dim=0)\n",
    "        return self.fc(h_graph)\n",
    "\n",
    "def train_epoch(model, graphs, opt, loss_fn):\n",
    "    model.train(); total=0\n",
    "    for g in graphs:\n",
    "        opt.zero_grad()\n",
    "        out = model(g).unsqueeze(0)\n",
    "        loss = loss_fn(out, g[\"y\"].unsqueeze(0))\n",
    "        loss.backward(); opt.step()\n",
    "        total += float(loss.item())\n",
    "    return total/len(graphs)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(model, graphs):\n",
    "    model.eval(); correct=0\n",
    "    for g in graphs:\n",
    "        pred = model(g).argmax().item()\n",
    "        correct += int(pred == g[\"y\"].item())\n",
    "    return correct/len(graphs)\n",
    "\n",
    "gnn = MPNNClassifierV3(in_dim=4, hidden=48, n_layers=3, dropout=0.2, pool=\"sum\")\n",
    "opt = torch.optim.Adam(gnn.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5): #you should replace with 25. Our class website can't run this long training so I put 5 here.\n",
    "    _ = train_epoch(gnn, train_graphs, opt, loss_fn)\n",
    "\n",
    "gnn_acc = eval_acc(gnn, test_graphs)\n",
    "\n",
    "print(f\"Descriptor MLP baseline acc: {base_acc:.3f}\")\n",
    "print(f\"GNN (3 layers, sum pool, dropout) acc: {gnn_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae9bee",
   "metadata": {},
   "source": [
    "### Solution Q4\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Prepare once\n",
    "y_all = np.array([g[\"y\"].item() for g in graphs_tox])\n",
    "\n",
    "def fit_eval_fold(train_ids, test_ids):\n",
    "    tr = [graphs_tox[i] for i in train_ids]\n",
    "    te = [graphs_tox[i] for i in test_ids]\n",
    "\n",
    "    # Different architecture from Q3 and class: 2 layers, hidden 96, max pooling\n",
    "    class MPNNClassifierCV(nn.Module):\n",
    "        def __init__(self, in_dim=4, hidden=96, n_classes=2):\n",
    "            super().__init__()\n",
    "            self.l1 = MPNNLayer(in_dim, hidden)\n",
    "            self.l2 = MPNNLayer(hidden, hidden)\n",
    "            self.fc = nn.Linear(hidden, n_classes)\n",
    "        def forward(self, g):\n",
    "            x, ei, ea = g[\"x\"], g[\"edge_index\"], g[\"edge_attr\"]\n",
    "            h = self.l1(x, ei, ea)\n",
    "            h = self.l2(h, ei, ea)\n",
    "            h_graph, _ = torch.max(h, dim=0)  # max pooling\n",
    "            return self.fc(h_graph)\n",
    "\n",
    "    model = MPNNClassifierCV()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # quick train\n",
    "    for epoch in range(5): #again, you should replace with 20\n",
    "        model.train()\n",
    "        for g in tr:\n",
    "            opt.zero_grad()\n",
    "            out = model(g).unsqueeze(0)\n",
    "            loss = loss_fn(out, g[\"y\"].unsqueeze(0))\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "    # eval acc and AUC\n",
    "    model.eval()\n",
    "    correct=0; probs=[]; ys=[]\n",
    "    with torch.no_grad():\n",
    "        for g in te:\n",
    "            logits = model(g)\n",
    "            p = torch.softmax(logits, dim=0)[1].item()\n",
    "            pred = int(logits.argmax().item())\n",
    "            probs.append(p); ys.append(int(g[\"y\"].item()))\n",
    "            correct += int(pred == ys[-1])\n",
    "    acc = correct/len(te)\n",
    "    auc = roc_auc_score(ys, probs)\n",
    "    return acc, auc\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "accs, aucs = [], []\n",
    "for tr_ids, te_ids in skf.split(np.arange(len(graphs_tox)), y_all):\n",
    "    acc, auc = fit_eval_fold(tr_ids, te_ids)\n",
    "    accs.append(acc); aucs.append(auc)\n",
    "\n",
    "print(f\"5-fold mean acc: {np.mean(accs):.3f}  ± {np.std(accs):.3f}\")\n",
    "print(f\"5-fold mean AUC: {np.mean(aucs):.3f}  ± {np.std(aucs):.3f}\")\n",
    "\n",
    "```\n",
    "\n",
    "### Solution Q5\n",
    "\n",
    "```python\n",
    "# Reactivity classification with an MLP from scratch on descriptors\n",
    "# Reactivity is −1 or 1. Map to {0,1}.\n",
    "df_rxn = df[[\"SMILES\",\"Reactivity\",\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].dropna().copy()\n",
    "df_rxn = df_rxn[df_rxn[\"Reactivity\"].isin([-1, 1])]\n",
    "y_rxn = (df_rxn[\"Reactivity\"].map({-1:0, 1:1})).astype(int).values\n",
    "X_rxn = df_rxn[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values.astype(np.float32)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_rxn, y_rxn, test_size=0.2, random_state=0, stratify=y_rxn)\n",
    "\n",
    "# Baselines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "logit = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LogisticRegression(max_iter=2000, random_state=0))]).fit(Xtr, ytr)\n",
    "rf    = RandomForestClassifier(n_estimators=300, min_samples_leaf=3, random_state=0, n_jobs=-1).fit(Xtr, ytr)\n",
    "\n",
    "base_acc_log = accuracy_score(yte, logit.predict(Xte))\n",
    "base_auc_log = roc_auc_score(yte, logit.predict_proba(Xte)[:,1])\n",
    "base_acc_rf  = accuracy_score(yte, rf.predict(Xte))\n",
    "base_auc_rf  = roc_auc_score(yte, rf.predict_proba(Xte)[:,1])\n",
    "\n",
    "# PyTorch MLP from scratch\n",
    "class MLPReact(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden=(32,16), n_classes=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden[0]), nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]), nn.ReLU(),\n",
    "            nn.Linear(hidden[1], n_classes)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_s = scaler.transform(Xtr).astype(np.float32)\n",
    "Xte_s = scaler.transform(Xte).astype(np.float32)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "mlp = MLPReact(in_dim=4, hidden=(32,16))\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "xb = torch.tensor(Xtr_s); yb = torch.tensor(ytr, dtype=torch.long)\n",
    "for epoch in range(5): #replace with 60\n",
    "    mlp.train()\n",
    "    opt.zero_grad()\n",
    "    loss = loss_fn(mlp(xb), yb)\n",
    "    loss.backward(); opt.step()\n",
    "\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    logits = mlp(torch.tensor(Xte_s))\n",
    "    pred   = logits.argmax(1).numpy()\n",
    "    proba  = torch.softmax(logits, dim=1)[:,1].numpy()\n",
    "\n",
    "mlp_acc = accuracy_score(yte, pred)\n",
    "mlp_auc = roc_auc_score(yte, proba)\n",
    "\n",
    "print(f\"LogReg baseline  acc={base_acc_log:.3f}  AUC={base_auc_log:.3f}\")\n",
    "print(f\"RF baseline      acc={base_acc_rf:.3f}   AUC={base_auc_rf:.3f}\")\n",
    "print(f\"MLP (scratch)    acc={mlp_acc:.3f}   AUC={mlp_auc:.3f}\")\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "source_map": [
   12,
   25,
   88,
   92,
   127,
   131,
   229
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}