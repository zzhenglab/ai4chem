{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c35ac60",
   "metadata": {},
   "source": [
    "## 11. Solutions\n",
    "\n",
    "### Solution Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f149087",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_reg \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMolWt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPSA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumRings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelting Point\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m df_reg[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMolWt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTPSA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumRings\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m df_reg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelting Point\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_reg = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Melting Point\"]].dropna()\n",
    "X = df_reg[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "y = df_reg[\"Melting Point\"].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPRegressor(hidden_layer_sizes=(32,), activation=\"relu\",\n",
    "                         alpha=1e-3, learning_rate_init=0.01,\n",
    "                         max_iter=1500, random_state=0))\n",
    "]).fit(Xtr, ytr)\n",
    "\n",
    "yhat = pipe.predict(Xte)\n",
    "print(f\"MSE={mean_squared_error(yte,yhat):.2f}  MAE={mean_absolute_error(yte,yhat):.2f}  R2={r2_score(yte,yhat):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(4.5,4))\n",
    "plt.scatter(yte, yhat, alpha=0.65)\n",
    "lims = [min(yte.min(), yhat.min()), max(yte.max(), yhat.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True MP\"); plt.ylabel(\"Pred MP\"); plt.title(\"Q1 parity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8e64c",
   "metadata": {},
   "source": [
    "### Solution Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374edef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [(16,), (32,), (64,32)]\n",
    "df_sol = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Solubility_mol_per_L\"]].dropna().copy()\n",
    "df_sol[\"logS\"] = np.log10(df_sol[\"Solubility_mol_per_L\"]+1e-6)\n",
    "X = df_sol[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "y = df_sol[\"logS\"].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "\n",
    "r2s, curves = [], []\n",
    "for sz in sizes:\n",
    "    reg = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"mlp\", MLPRegressor(hidden_layer_sizes=sz, activation=\"relu\",\n",
    "                             alpha=1e-3, learning_rate_init=0.01,\n",
    "                             early_stopping=True, validation_fraction=0.15,\n",
    "                             max_iter=3000, random_state=0))\n",
    "    ]).fit(Xtr, ytr)\n",
    "    yhat = reg.predict(Xte)\n",
    "    r2s.append(r2_score(yte, yhat))\n",
    "    curves.append(reg.named_steps[\"mlp\"].loss_curve_)\n",
    "\n",
    "print(pd.DataFrame({\"hidden_sizes\":[str(s) for s in sizes],\"R2\":np.round(r2s,3)}))\n",
    "\n",
    "plt.figure(figsize=(5.5,3.5))\n",
    "for sz, c in zip(sizes, curves):\n",
    "    plt.plot(c, label=str(sz))\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Q2 loss curves\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfeebd",
   "metadata": {},
   "source": [
    "### Solution Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Toxicity\"]].dropna()\n",
    "y = df_clf[\"Toxicity\"].str.lower().map({\"toxic\":1,\"non_toxic\":0}).astype(int).values\n",
    "X = df_clf[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPClassifier(hidden_layer_sizes=(32,), activation=\"relu\",\n",
    "                          alpha=1e-3, learning_rate_init=0.01,\n",
    "                          early_stopping=True, validation_fraction=0.15,\n",
    "                          max_iter=3000, random_state=0))\n",
    "]).fit(Xtr, ytr)\n",
    "\n",
    "proba = clf.predict_proba(Xte)[:,1]\n",
    "for t in [0.3, 0.5, 0.7]:\n",
    "    pred = (proba >= t).astype(int)\n",
    "    print(f\"t={t:.1f}  acc={accuracy_score(yte,pred):.3f}  prec={precision_score(yte,pred):.3f}  rec={recall_score(yte,pred):.3f}  f1={f1_score(yte,pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dafffc",
   "metadata": {},
   "source": [
    "### Solution Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sol = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Solubility_mol_per_L\"]].dropna().copy()\n",
    "df_sol[\"logS\"] = np.log10(df_sol[\"Solubility_mol_per_L\"]+1e-6)\n",
    "X = df_sol[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "y = df_sol[\"logS\"].values\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "\n",
    "sc = StandardScaler().fit(Xtr)\n",
    "Xtr_s, Xte_s = sc.transform(Xtr), sc.transform(Xte)\n",
    "\n",
    "lr = LinearRegression().fit(Xtr_s, ytr)\n",
    "yhat_lr = lr.predict(Xte_s)\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(32,), activation=\"relu\",\n",
    "                   alpha=1e-3, learning_rate_init=0.01,\n",
    "                   max_iter=3000, random_state=0).fit(Xtr_s, ytr)\n",
    "yhat_mlp = mlp.predict(Xte_s)\n",
    "\n",
    "print(f\"Linear R2: {r2_score(yte, yhat_lr):.3f}\")\n",
    "print(f\"MLP    R2: {r2_score(yte, yhat_mlp):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "plt.scatter(yte, yhat_lr, alpha=0.6, label=\"Linear\")\n",
    "plt.scatter(yte, yhat_mlp, alpha=0.6, label=\"MLP\")\n",
    "lims = [min(yte.min(), yhat_lr.min(), yhat_mlp.min()), max(yte.max(), yhat_lr.max(), yhat_mlp.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True logS\"); plt.ylabel(\"Predicted\")\n",
    "plt.legend(); plt.title(\"Q4 parity: Linear vs MLP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f536f",
   "metadata": {},
   "source": [
    "### Solution Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af00c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Q5 (full run + metrics + plots)\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Data\n",
    "df_mp = df[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\",\"Melting Point\"]].dropna().copy()\n",
    "\n",
    "X = df_mp[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values.astype(np.float32)\n",
    "y = df_mp[\"Melting Point\"].values.astype(np.float32).reshape(-1,1)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_s = scaler.transform(Xtr).astype(np.float32)\n",
    "Xte_s = scaler.transform(Xte).astype(np.float32)\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(NumpyDataset(Xtr_s, ytr), batch_size=64, shuffle=True)\n",
    "\n",
    "in_dim = Xtr_s.shape[1]\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_dim, 32), nn.ReLU(),\n",
    "    nn.Linear(32, 16),     nn.ReLU(),\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "train_losses = []\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    batch_losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        batch_losses.append(loss.item())\n",
    "    train_losses.append(np.mean(batch_losses))\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat = model(torch.from_numpy(Xte_s)).numpy()\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(yte, yhat):.3f}\")\n",
    "print(f\"MAE: {mean_absolute_error(yte, yhat):.3f}\")\n",
    "print(f\"R2:  {r2_score(yte, yhat):.3f}\")\n",
    "\n",
    "# Learning curve\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"train MSE\"); plt.title(\"Training loss (melting point)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Parity plot\n",
    "plt.figure(figsize=(4.6,4))\n",
    "plt.scatter(yte, yhat, alpha=0.65)\n",
    "lims = [min(yte.min(), yhat.min()), max(yte.max(), yhat.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True MP\"); plt.ylabel(\"Pred MP\"); plt.title(\"Parity plot (PyTorch MP)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2585fab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 10. Solutions\n",
    "\n",
    "### Solution Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 solution\n",
    "set_seed(0)\n",
    "url = \"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/C_H_oxidation_dataset.csv\"\n",
    "df_oxidation_raw = pd.read_csv(url)\n",
    "def calc_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return pd.Series({\n",
    "            \"MolWt\": None,\n",
    "            \"LogP\": None,\n",
    "            \"TPSA\": None,\n",
    "            \"NumRings\": None\n",
    "        })\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(mol),                    # molecular weight\n",
    "        \"LogP\": Crippen.MolLogP(mol),                       # octanol-water logP\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(mol),             # topological polar surface area\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(mol)      # number of rings\n",
    "    })\n",
    "\n",
    "# Apply the function to the SMILES column\n",
    "desc_df = df_oxidation_raw[\"SMILES\"].apply(calc_descriptors)\n",
    "\n",
    "# Concatenate new descriptor columns to original DataFrame\n",
    "df_clf = pd.concat([df_oxidation_raw, desc_df], axis=1)\n",
    "df_reg = df_clf.copy() \n",
    "\n",
    "# Logistic regression baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X = df_clf[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "y = df_clf[\"Toxicity\"].str.lower().map({\"toxic\":1,\"non_toxic\":0}).values\n",
    "Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
    "baseline = LogisticRegression(max_iter=1000).fit(Xtr,ytr)\n",
    "print(\"Baseline acc:\", accuracy_score(yte, baseline.predict(Xte)))\n",
    "\n",
    "# MLP from scratch\n",
    "class MLP_Q1(nn.Module):\n",
    "    def __init__(self,in_dim,hidden=32,n_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim,hidden)\n",
    "        self.fc2 = nn.Linear(hidden,n_classes)\n",
    "    def forward(self,x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "model = MLP_Q1(4)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    xb = torch.tensor(Xtr, dtype=torch.float32)\n",
    "    yb = torch.tensor(ytr, dtype=torch.long)\n",
    "    opt.zero_grad()\n",
    "    out = model(xb)\n",
    "    loss = loss_fn(out,yb)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "model.eval()\n",
    "ypred = model(torch.tensor(Xte, dtype=torch.float32)).argmax(1).numpy()\n",
    "print(\"MLP acc:\", accuracy_score(yte, ypred))\n",
    "#For this question and the following questions, it is fine if the model performance is worse than the baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba02df",
   "metadata": {},
   "source": [
    "### Solution Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reg[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values.astype(np.float32)\n",
    "y = df_reg[\"Melting Point\"].values.astype(np.float32).reshape(-1,1)\n",
    "Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "# Baseline\n",
    "rf = RandomForestRegressor(n_estimators=200,random_state=0).fit(Xtr,ytr.ravel())\n",
    "print(\"Baseline RF R2:\", r2_score(yte,rf.predict(Xte)))\n",
    "\n",
    "# PyTorch MLP\n",
    "class MLPreg_Q2(nn.Module):\n",
    "    def __init__(self,in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,64), nn.ReLU(),\n",
    "            nn.Linear(64,32), nn.ReLU(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "model = MLPreg_Q2(4)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    xb = torch.tensor(Xtr, dtype=torch.float32)\n",
    "    yb = torch.tensor(ytr, dtype=torch.float32)\n",
    "    opt.zero_grad()\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "    loss.backward(); opt.step()\n",
    "\n",
    "yhat = model(torch.tensor(Xte, dtype=torch.float32)).detach().numpy()\n",
    "print(\"MLP R2:\", r2_score(yte,yhat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8fcf8",
   "metadata": {},
   "source": [
    "### Solution Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build toxicity graphs\n",
    "label_map = {\"toxic\":1, \"non_toxic\":0}\n",
    "df_tox = df[df[\"Toxicity\"].str.lower().isin(label_map.keys())].copy()\n",
    "y_bin = df_tox[\"Toxicity\"].str.lower().map(label_map).astype(int).values\n",
    "\n",
    "graphs_tox = []\n",
    "for smi, yv in zip(df_tox[\"SMILES\"], y_bin):\n",
    "    mol, x_np, ei_np, ea_np = smiles_to_graph(smi)\n",
    "    g = {\n",
    "        \"x\": torch.tensor(x_np, dtype=torch.float32),\n",
    "        \"edge_index\": torch.tensor(ei_np, dtype=torch.long),\n",
    "        \"edge_attr\": torch.tensor(ea_np, dtype=torch.long),\n",
    "        \"y\": torch.tensor(yv, dtype=torch.long)\n",
    "    }\n",
    "    graphs_tox.append(g)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(graphs_tox)),\n",
    "    test_size=0.2, random_state=42, stratify=y_bin\n",
    ")\n",
    "train_graphs = [graphs_tox[i] for i in train_idx]\n",
    "test_graphs  = [graphs_tox[i] for i in test_idx]\n",
    "\n",
    "# Baseline on descriptors: simple MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_desc = df_tox[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values\n",
    "Xtr_d, Xte_d = X_desc[train_idx], X_desc[test_idx]\n",
    "ytr_d, yte_d = y_bin[train_idx], y_bin[test_idx]\n",
    "\n",
    "mlp_base = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", MLPClassifier(hidden_layer_sizes=(16,),\n",
    "                          activation=\"relu\",\n",
    "                          learning_rate_init=0.01,\n",
    "                          max_iter=2000,\n",
    "                          random_state=0))\n",
    "]).fit(Xtr_d, ytr_d)\n",
    "base_acc = accuracy_score(yte_d, mlp_base.predict(Xte_d))\n",
    "\n",
    "# GNN variant: 3 layers, sum pooling, dropout\n",
    "class MPNNClassifierV3(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden=48, n_layers=3, n_classes=2, dropout=0.2, pool=\"sum\"):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        dims = [in_dim] + [hidden]*(n_layers-1) + [hidden]\n",
    "        for a, b in zip(dims[:-1], dims[1:]):\n",
    "            self.layers.append(MPNNLayer(a, b))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden, n_classes)\n",
    "        self.pool = pool\n",
    "\n",
    "    def forward(self, g):\n",
    "        x, edge_index, edge_attr = g[\"x\"], g[\"edge_index\"], g[\"edge_attr\"]\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, edge_index, edge_attr)\n",
    "            h = self.dropout(h)\n",
    "        if self.pool == \"sum\":\n",
    "            h_graph = h.sum(dim=0)\n",
    "        else:\n",
    "            h_graph = h.mean(dim=0)\n",
    "        return self.fc(h_graph)\n",
    "\n",
    "def train_epoch(model, graphs, opt, loss_fn):\n",
    "    model.train(); total=0\n",
    "    for g in graphs:\n",
    "        opt.zero_grad()\n",
    "        out = model(g).unsqueeze(0)\n",
    "        loss = loss_fn(out, g[\"y\"].unsqueeze(0))\n",
    "        loss.backward(); opt.step()\n",
    "        total += float(loss.item())\n",
    "    return total/len(graphs)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(model, graphs):\n",
    "    model.eval(); correct=0\n",
    "    for g in graphs:\n",
    "        pred = model(g).argmax().item()\n",
    "        correct += int(pred == g[\"y\"].item())\n",
    "    return correct/len(graphs)\n",
    "\n",
    "gnn = MPNNClassifierV3(in_dim=4, hidden=48, n_layers=3, dropout=0.2, pool=\"sum\")\n",
    "opt = torch.optim.Adam(gnn.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5): #you should replace with 25. Our class website can't run this long training so I put 5 here.\n",
    "    _ = train_epoch(gnn, train_graphs, opt, loss_fn)\n",
    "\n",
    "gnn_acc = eval_acc(gnn, test_graphs)\n",
    "\n",
    "print(f\"Descriptor MLP baseline acc: {base_acc:.3f}\")\n",
    "print(f\"GNN (3 layers, sum pool, dropout) acc: {gnn_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abe68d",
   "metadata": {},
   "source": [
    "### Solution Q4\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Prepare once\n",
    "y_all = np.array([g[\"y\"].item() for g in graphs_tox])\n",
    "\n",
    "def fit_eval_fold(train_ids, test_ids):\n",
    "    tr = [graphs_tox[i] for i in train_ids]\n",
    "    te = [graphs_tox[i] for i in test_ids]\n",
    "\n",
    "    # Different architecture from Q3 and class: 2 layers, hidden 96, max pooling\n",
    "    class MPNNClassifierCV(nn.Module):\n",
    "        def __init__(self, in_dim=4, hidden=96, n_classes=2):\n",
    "            super().__init__()\n",
    "            self.l1 = MPNNLayer(in_dim, hidden)\n",
    "            self.l2 = MPNNLayer(hidden, hidden)\n",
    "            self.fc = nn.Linear(hidden, n_classes)\n",
    "        def forward(self, g):\n",
    "            x, ei, ea = g[\"x\"], g[\"edge_index\"], g[\"edge_attr\"]\n",
    "            h = self.l1(x, ei, ea)\n",
    "            h = self.l2(h, ei, ea)\n",
    "            h_graph, _ = torch.max(h, dim=0)  # max pooling\n",
    "            return self.fc(h_graph)\n",
    "\n",
    "    model = MPNNClassifierCV()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # quick train\n",
    "    for epoch in range(5): #again, you should replace with 20\n",
    "        model.train()\n",
    "        for g in tr:\n",
    "            opt.zero_grad()\n",
    "            out = model(g).unsqueeze(0)\n",
    "            loss = loss_fn(out, g[\"y\"].unsqueeze(0))\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "    # eval acc and AUC\n",
    "    model.eval()\n",
    "    correct=0; probs=[]; ys=[]\n",
    "    with torch.no_grad():\n",
    "        for g in te:\n",
    "            logits = model(g)\n",
    "            p = torch.softmax(logits, dim=0)[1].item()\n",
    "            pred = int(logits.argmax().item())\n",
    "            probs.append(p); ys.append(int(g[\"y\"].item()))\n",
    "            correct += int(pred == ys[-1])\n",
    "    acc = correct/len(te)\n",
    "    auc = roc_auc_score(ys, probs)\n",
    "    return acc, auc\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "accs, aucs = [], []\n",
    "for tr_ids, te_ids in skf.split(np.arange(len(graphs_tox)), y_all):\n",
    "    acc, auc = fit_eval_fold(tr_ids, te_ids)\n",
    "    accs.append(acc); aucs.append(auc)\n",
    "\n",
    "print(f\"5-fold mean acc: {np.mean(accs):.3f}  ± {np.std(accs):.3f}\")\n",
    "print(f\"5-fold mean AUC: {np.mean(aucs):.3f}  ± {np.std(aucs):.3f}\")\n",
    "\n",
    "```\n",
    "\n",
    "### Solution Q5\n",
    "\n",
    "```python\n",
    "# Reactivity classification with an MLP from scratch on descriptors\n",
    "# Reactivity is −1 or 1. Map to {0,1}.\n",
    "df_rxn = df[[\"SMILES\",\"Reactivity\",\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].dropna().copy()\n",
    "df_rxn = df_rxn[df_rxn[\"Reactivity\"].isin([-1, 1])]\n",
    "y_rxn = (df_rxn[\"Reactivity\"].map({-1:0, 1:1})).astype(int).values\n",
    "X_rxn = df_rxn[[\"MolWt\",\"LogP\",\"TPSA\",\"NumRings\"]].values.astype(np.float32)\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_rxn, y_rxn, test_size=0.2, random_state=0, stratify=y_rxn)\n",
    "\n",
    "# Baselines\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "logit = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LogisticRegression(max_iter=2000, random_state=0))]).fit(Xtr, ytr)\n",
    "rf    = RandomForestClassifier(n_estimators=300, min_samples_leaf=3, random_state=0, n_jobs=-1).fit(Xtr, ytr)\n",
    "\n",
    "base_acc_log = accuracy_score(yte, logit.predict(Xte))\n",
    "base_auc_log = roc_auc_score(yte, logit.predict_proba(Xte)[:,1])\n",
    "base_acc_rf  = accuracy_score(yte, rf.predict(Xte))\n",
    "base_auc_rf  = roc_auc_score(yte, rf.predict_proba(Xte)[:,1])\n",
    "\n",
    "# PyTorch MLP from scratch\n",
    "class MLPReact(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden=(32,16), n_classes=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden[0]), nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]), nn.ReLU(),\n",
    "            nn.Linear(hidden[1], n_classes)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_s = scaler.transform(Xtr).astype(np.float32)\n",
    "Xte_s = scaler.transform(Xte).astype(np.float32)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "mlp = MLPReact(in_dim=4, hidden=(32,16))\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "xb = torch.tensor(Xtr_s); yb = torch.tensor(ytr, dtype=torch.long)\n",
    "for epoch in range(5): #replace with 60\n",
    "    mlp.train()\n",
    "    opt.zero_grad()\n",
    "    loss = loss_fn(mlp(xb), yb)\n",
    "    loss.backward(); opt.step()\n",
    "\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    logits = mlp(torch.tensor(Xte_s))\n",
    "    pred   = logits.argmax(1).numpy()\n",
    "    proba  = torch.softmax(logits, dim=1)[:,1].numpy()\n",
    "\n",
    "mlp_acc = accuracy_score(yte, pred)\n",
    "mlp_auc = roc_auc_score(yte, proba)\n",
    "\n",
    "print(f\"LogReg baseline  acc={base_acc_log:.3f}  AUC={base_auc_log:.3f}\")\n",
    "print(f\"RF baseline      acc={base_acc_rf:.3f}   AUC={base_auc_rf:.3f}\")\n",
    "print(f\"MLP (scratch)    acc={mlp_acc:.3f}   AUC={mlp_auc:.3f}\")\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "source_map": [
   12,
   19,
   41,
   45,
   73,
   77,
   95,
   99,
   128,
   132,
   203,
   223,
   286,
   290,
   325,
   329,
   427
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}