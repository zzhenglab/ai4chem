{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc9064a",
   "metadata": {},
   "source": [
    "# Lecture 7 - Decision Trees and Random Forests\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## 1. Setup and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "941f88aa",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cost_complexity_pruning_path' from 'sklearn.tree' (C:\\Users\\52377\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor, RandomForestClassifier\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minspection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m permutation_importance, PartialDependenceDisplay\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cost_complexity_pruning_path\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'cost_complexity_pruning_path' from 'sklearn.tree' (C:\\Users\\52377\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# If you are on Colab, you may need:\n",
    "# %pip install scikit-learn pandas matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names\")\n",
    "\n",
    "# Optional RDKit for descriptors (used in Lectures 5 and 6)\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors, Draw\n",
    "except Exception:\n",
    "    print(\"RDKit not available. Descriptor drawing will be skipped.\")\n",
    "    Chem = None\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.tree import cost_complexity_pruning_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20767c82",
   "metadata": {},
   "source": [
    "We will reuse the Câ€“H oxidation dataset and the same four lightweight descriptors: `MolWt`, `LogP`, `TPSA`, `NumRings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e35394",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/zzhenglab/ai4chem/main/book/_data/C_H_oxidation_dataset.csv\"\n",
    "df_raw = pd.read_csv(url)\n",
    "\n",
    "def calc_desc(smiles):\n",
    "    if Chem is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return pd.Series({\"MolWt\": np.nan, \"LogP\": np.nan, \"TPSA\": np.nan, \"NumRings\": np.nan})\n",
    "    return pd.Series({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\": Crippen.MolLogP(m),\n",
    "        \"TPSA\": rdMolDescriptors.CalcTPSA(m),\n",
    "        \"NumRings\": rdMolDescriptors.CalcNumRings(m)\n",
    "    })\n",
    "\n",
    "desc_df = df_raw[\"SMILES\"].apply(calc_desc)\n",
    "df = pd.concat([df_raw, desc_df], axis=1)\n",
    "\n",
    "feat = [\"MolWt\", \"LogP\", \"TPSA\", \"NumRings\"]\n",
    "X_all = df[feat]\n",
    "print(\"Rows:\", len(df))\n",
    "X_all.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6513ec2",
   "metadata": {},
   "source": [
    "```{admonition} What we will predict\n",
    "- **Regression** target: `Melting Point`  \n",
    "- **Classification** target: `Toxicity` mapped to 1 for toxic and 0 for non_toxic\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Decision trees - intuition and API\n",
    "\n",
    "```{admonition} Idea\n",
    "A decision tree learns a sequence of questions like `MolWt <= 200.5`. Each split aims to make child nodes purer.\n",
    "```\n",
    "\n",
    "- **Regression tree** chooses splits that reduce **MSE** the most. A leaf predicts the **mean** of training `y` within that leaf.\n",
    "- **Classification tree** chooses splits that reduce **Gini** or **entropy**. A leaf predicts the **majority class** and class probabilities.\n",
    "\n",
    "Key hyperparameters you will tune frequently:\n",
    "- `max_depth` - maximum levels of splits\n",
    "- `min_samples_split` - minimum samples required to attempt a split\n",
    "- `min_samples_leaf` - minimum samples allowed in a leaf\n",
    "- `max_features` - number of features to consider when finding the best split\n",
    "\n",
    "Trees handle different feature scales naturally, and they do not require standardization. They can struggle with high noise and very small datasets if left unconstrained.\n",
    "\n",
    "```{admonition} Vocabulary\n",
    "- **Node** is a point where a question is asked.  \n",
    "- **Leaf** holds a simple prediction.  \n",
    "- **Impurity** is a measure of how mixed a node is. Lower is better.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Tree regression on Melting Point\n",
    "\n",
    "### 3.1 Prepare `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf080ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feat]\n",
    "y = df[\"Melting Point\"]\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_tr.shape, X_te.shape, y_tr.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7480ecf",
   "metadata": {},
   "source": [
    "### 3.2 Fit a tiny stump to see one split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd026ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_stump = DecisionTreeRegressor(max_depth=1, random_state=0)\n",
    "tree_stump.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"Train R2:\", r2_score(y_tr, tree_stump.predict(X_tr)).round(3))\n",
    "print(\"Test  R2:\", r2_score(y_te, tree_stump.predict(X_te)).round(3))\n",
    "print(\"Importances:\", dict(zip(feat, np.round(tree_stump.feature_importances_, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c139865",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plot_tree(tree_stump, feature_names=feat, filled=True, rounded=True)\n",
    "plt.title(\"DecisionTreeRegressor depth=1\")\n",
    "plt.show()\n",
    "\n",
    "print(export_text(tree_stump, feature_names=feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a3b6f",
   "metadata": {},
   "source": [
    "### 3.3 Increase depth and watch train vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20956c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(1, 11))\n",
    "r2_tr, r2_te = [], []\n",
    "\n",
    "for d in depths:\n",
    "    m = DecisionTreeRegressor(max_depth=d, random_state=0)\n",
    "    m.fit(X_tr, y_tr)\n",
    "    r2_tr.append(r2_score(y_tr, m.predict(X_tr)))\n",
    "    r2_te.append(r2_score(y_te, m.predict(X_te)))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(depths, r2_tr, \"o-\", label=\"Train R2\")\n",
    "plt.plot(depths, r2_te, \"o-\", label=\"Test R2\")\n",
    "plt.xlabel(\"max_depth\"); plt.ylabel(\"R2\"); plt.title(\"Depth sweep - regression\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa60b2",
   "metadata": {},
   "source": [
    "### 3.4 Choose a reasonable depth and inspect fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a498507",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = 4\n",
    "tree_reg = DecisionTreeRegressor(max_depth=best_depth, random_state=0).fit(X_tr, y_tr)\n",
    "\n",
    "y_hat = tree_reg.predict(X_te)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(y_te, y_hat, alpha=0.6)\n",
    "lims = [min(y_te.min(), y_hat.min()), max(y_te.max(), y_hat.max())]\n",
    "plt.plot(lims, lims, \"k--\")\n",
    "plt.xlabel(\"True MP\"); plt.ylabel(\"Predicted MP\")\n",
    "plt.title(\"Parity plot - tree regression\")\n",
    "plt.show()\n",
    "\n",
    "pd.Series(tree_reg.feature_importances_, index=feat).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8412097",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Tree classification on Toxicity\n",
    "\n",
    "### 4.1 Encode label and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a995c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_map = {\"toxic\": 1, \"non_toxic\": 0}\n",
    "y_cls = df[\"Toxicity\"].str.lower().map(lab_map)\n",
    "\n",
    "mask = y_cls.notna() & X.notna().all(axis=1)\n",
    "Xc = X.loc[mask]; yc = y_cls.loc[mask].astype(int)\n",
    "\n",
    "Xc_tr, Xc_te, yc_tr, yc_te = train_test_split(\n",
    "    Xc, yc, test_size=0.2, random_state=42, stratify=yc\n",
    ")\n",
    "Xc_tr.shape, yc_tr.value_counts(normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f956024",
   "metadata": {},
   "source": [
    "### 4.2 Depth 1 then sweep `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27376406",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_stump = DecisionTreeClassifier(max_depth=1, random_state=0).fit(Xc_tr, yc_tr)\n",
    "proba = clf_stump.predict_proba(Xc_te)[:,1]\n",
    "pred  = (proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(yc_te, pred).round(3))\n",
    "print(\"Precision:\", precision_score(yc_te, pred).round(3))\n",
    "print(\"Recall:\", recall_score(yc_te, pred).round(3))\n",
    "print(\"AUC:\", roc_auc_score(yc_te, proba).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = list(range(1, 11))\n",
    "accs, aucs = [], []\n",
    "for d in depths:\n",
    "    m = DecisionTreeClassifier(max_depth=d, random_state=0).fit(Xc_tr, yc_tr)\n",
    "    pr = m.predict_proba(Xc_te)[:,1]\n",
    "    pd_ = (pr >= 0.5).astype(int)\n",
    "    accs.append(accuracy_score(yc_te, pd_))\n",
    "    aucs.append(roc_auc_score(yc_te, pr))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(depths, accs, \"o-\"); ax[0].set_xlabel(\"max_depth\"); ax[0].set_ylabel(\"Accuracy\"); ax[0].grid(True, alpha=0.3)\n",
    "ax[1].plot(depths, aucs, \"o-\"); ax[1].set_xlabel(\"max_depth\"); ax[1].set_ylabel(\"AUC\"); ax[1].grid(True, alpha=0.3)\n",
    "plt.suptitle(\"Depth sweep - classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pick = 4\n",
    "clf_d = DecisionTreeClassifier(max_depth=d_pick, random_state=0).fit(Xc_tr, yc_tr)\n",
    "cm = confusion_matrix(yc_te, clf_d.predict(Xc_te))\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(cm, cmap=\"Blues\"); plt.title(f\"Confusion matrix - depth={d_pick}\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d506cdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Overfitting and regularization\n",
    "\n",
    "Trees can overfit easily when depth is large or when leaves are tiny. You will see:\n",
    "- **Very high train R2 or train accuracy**\n",
    "- **Noticeably lower test R2 or test accuracy**\n",
    "\n",
    "You can reduce variance by:\n",
    "- Limiting depth with `max_depth`\n",
    "- Requiring more samples in leaves using `min_samples_leaf`\n",
    "- Requiring more samples to split using `min_samples_split`\n",
    "- Cost complexity pruning using `ccp_alpha`\n",
    "\n",
    "### 5.1 Bias vs variance picture using depth curves\n",
    "\n",
    "We already plotted train vs test curves with a depth sweep for regression and classification. The gap between train and test curves grows when the model overfits. Pick a region where the test curve plateaus and the gap is small.\n",
    "\n",
    "### 5.2 Minimum leaf size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_sizes = [1, 2, 5, 10, 20, 40, 80]\n",
    "r2_leaf = []\n",
    "for leaf in leaf_sizes:\n",
    "    m = DecisionTreeRegressor(min_samples_leaf=leaf, random_state=0).fit(X_tr, y_tr)\n",
    "    r2_leaf.append(r2_score(y_te, m.predict(X_te)))\n",
    "\n",
    "pd.DataFrame({\"min_samples_leaf\": leaf_sizes, \"test_R2\": np.round(r2_leaf, 3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8435326",
   "metadata": {},
   "source": [
    "### 5.3 Cost complexity pruning path\n",
    "\n",
    "Scikit-learn can compute a sequence of pruned trees controlled by `ccp_alpha`. Larger `ccp_alpha` means stronger pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from a relatively deep tree\n",
    "deep_tree = DecisionTreeRegressor(random_state=0).fit(X_tr, y_tr)\n",
    "path = cost_complexity_pruning_path(deep_tree, X_tr, y_tr)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# Train along the path\n",
    "r2_te_alpha, r2_tr_alpha = [], []\n",
    "for a in ccp_alphas:\n",
    "    m = DecisionTreeRegressor(random_state=0, ccp_alpha=a).fit(X_tr, y_tr)\n",
    "    r2_tr_alpha.append(r2_score(y_tr, m.predict(X_tr)))\n",
    "    r2_te_alpha.append(r2_score(y_te, m.predict(X_te)))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ccp_alphas, r2_tr_alpha, marker=\"o\", label=\"Train R2\")\n",
    "plt.plot(ccp_alphas, r2_te_alpha, marker=\"o\", label=\"Test R2\")\n",
    "plt.xlabel(\"ccp_alpha\"); plt.ylabel(\"R2\"); plt.title(\"Pruning curve\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fe0bd",
   "metadata": {},
   "source": [
    "```{admonition} Takeaway\n",
    "Trees do not have to be deep to work well. A small amount of pruning or a modest leaf size can improve test performance and stability.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Random Forests\n",
    "\n",
    "A Random Forest builds many trees on bootstrap samples and averages their predictions. Each split considers a random subset of features, which decorrelates trees.\n",
    "\n",
    "### 6.1 Regression with OOB estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=0,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_reg.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"OOB R2:\", getattr(rf_reg, \"oob_score_\", None))\n",
    "print(\"Test R2:\", r2_score(y_te, rf_reg.predict(X_te)).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf97a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators curve\n",
    "ests = [20, 50, 100, 200, 300, 500]\n",
    "r2s = []\n",
    "for n in ests:\n",
    "    m = RandomForestRegressor(n_estimators=n, random_state=0, n_jobs=-1).fit(X_tr, y_tr)\n",
    "    r2s.append(r2_score(y_te, m.predict(X_te)))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ests, r2s, \"o-\")\n",
    "plt.xlabel(\"n_estimators\"); plt.ylabel(\"Test R2\"); plt.title(\"Forest size vs R2\")\n",
    "plt.grid(True, alpha=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e72b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance and permutation importance\n",
    "imp_rf = pd.Series(rf_reg.feature_importances_, index=feat).sort_values(ascending=True)\n",
    "imp_rf.plot(kind=\"barh\", figsize=(5,3)); plt.title(\"RF regression - feature importance\"); plt.show()\n",
    "\n",
    "perm = permutation_importance(rf_reg, X_te, y_te, scoring=\"r2\", n_repeats=20, random_state=0)\n",
    "pd.Series(perm.importances_mean, index=feat).sort_values().plot(kind=\"barh\", figsize=(5,3))\n",
    "plt.title(\"Permutation importance - drop in R2\"); plt.xlabel(\"Mean decrease in R2\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial dependence for one feature\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "PartialDependenceDisplay.from_estimator(rf_reg, X, [\"MolWt\"], ax=plt.gca())\n",
    "plt.title(\"Partial dependence - MolWt (RF regression)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81862685",
   "metadata": {},
   "source": [
    "### 6.2 Classification with ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd09dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    random_state=0,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_clf.fit(Xc_tr, yc_tr)\n",
    "\n",
    "proba = rf_clf.predict_proba(Xc_te)[:,1]\n",
    "pred  = (proba >= 0.5).astype(int)\n",
    "print(\"OOB accuracy:\", getattr(rf_clf, \"oob_score_\", None))\n",
    "print(\"Test accuracy:\", accuracy_score(yc_te, pred).round(3))\n",
    "print(\"Test AUC:\", roc_auc_score(yc_te, proba).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "fpr, tpr, thr = roc_curve(yc_te, proba)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"AUC={roc_auc_score(yc_te, proba):.3f}\")\n",
    "plt.plot([0,1],[0,1], \"k--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC - RF classifier\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification importances\n",
    "pd.Series(rf_clf.feature_importances_, index=feat).sort_values().plot(kind=\"barh\", figsize=(5,3))\n",
    "plt.title(\"RF classification - feature importance\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36bbde",
   "metadata": {},
   "source": [
    "```{admonition} Why forests help\n",
    "A single deep tree can fit noise. Averaging many diverse trees reduces variance and often boosts test performance.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Tuning and validation\n",
    "\n",
    "We will use CV to pick hyperparameters for both tree and forest models. Keep grids compact so the run is quick in class.\n",
    "\n",
    "### 7.1 Decision tree regression grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f023e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dt = {\n",
    "    \"max_depth\": [3, 4, 5, 6, None],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=0)\n",
    "cv = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "grid_dt = GridSearchCV(dt, param_grid_dt, cv=cv, scoring=\"r2\", n_jobs=-1)\n",
    "grid_dt.fit(X_tr, y_tr)\n",
    "\n",
    "best_dt = grid_dt.best_estimator_\n",
    "print(\"Best params:\", grid_dt.best_params_)\n",
    "print(\"CV mean R2:\", grid_dt.best_score_.round(3))\n",
    "print(\"Test R2:\", r2_score(y_te, best_dt.predict(X_te)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38c788",
   "metadata": {},
   "source": [
    "### 7.2 Random forest regression grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    \"n_estimators\": [200, 300],\n",
    "    \"max_depth\": [None, 6, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    "grid_rf = GridSearchCV(rf, param_grid_rf, cv=cv, scoring=\"r2\", n_jobs=-1)\n",
    "grid_rf.fit(X_tr, y_tr)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(\"Best params:\", grid_rf.best_params_)\n",
    "print(\"CV mean R2:\", grid_rf.best_score_.round(3))\n",
    "print(\"Test R2:\", r2_score(y_te, best_rf.predict(X_te)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d386d72",
   "metadata": {},
   "source": [
    "### 7.3 Decision tree classification grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dtc = {\n",
    "    \"max_depth\": [2, 3, 4, 6, None],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "cv_c = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "grid_dtc = GridSearchCV(dtc, param_grid_dtc, cv=cv_c, scoring=\"roc_auc\", n_jobs=-1)\n",
    "grid_dtc.fit(Xc_tr, yc_tr)\n",
    "\n",
    "best_dtc = grid_dtc.best_estimator_\n",
    "print(\"Best params:\", grid_dtc.best_params_)\n",
    "print(\"CV mean AUC:\", grid_dtc.best_score_.round(3))\n",
    "print(\"Test AUC:\", roc_auc_score(yc_te, best_dtc.predict_proba(Xc_te)[:,1]).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13050145",
   "metadata": {},
   "source": [
    "### 7.4 Random forest classification grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360eceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rfc = {\n",
    "    \"n_estimators\": [200, 400],\n",
    "    \"max_depth\": [None, 6, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "grid_rfc = GridSearchCV(rfc, param_grid_rfc, cv=cv_c, scoring=\"roc_auc\", n_jobs=-1)\n",
    "grid_rfc.fit(Xc_tr, yc_tr)\n",
    "\n",
    "best_rfc = grid_rfc.best_estimator_\n",
    "proba_best = best_rfc.predict_proba(Xc_te)[:,1]\n",
    "print(\"Best params:\", grid_rfc.best_params_)\n",
    "print(\"CV mean AUC:\", grid_rfc.best_score_.round(3))\n",
    "print(\"Test AUC:\", roc_auc_score(yc_te, proba_best).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605bfaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Interpretability and diagnostics\n",
    "\n",
    "This section groups useful tools for understanding models and diagnosing issues.\n",
    "\n",
    "### 8.1 Compare a single tree to a forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063912ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_r = DecisionTreeRegressor(max_depth=6, random_state=0).fit(X_tr, y_tr)\n",
    "rf_r   = RandomForestRegressor(n_estimators=300, random_state=0, n_jobs=-1).fit(X_tr, y_tr)\n",
    "\n",
    "print(\"Tree test R2:\", r2_score(y_te, tree_r.predict(X_te)).round(3))\n",
    "print(\"Forest test R2:\", r2_score(y_te, rf_r.predict(X_te)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c45813",
   "metadata": {},
   "source": [
    "### 8.2 Inspect one tree from a forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tree = best_rf.estimators_[0] if 'best_rf' in globals() else rf_r.estimators_[0]\n",
    "plt.figure(figsize=(10,5))\n",
    "plot_tree(one_tree, feature_names=feat, filled=True, rounded=True, max_depth=3)\n",
    "plt.title(\"One tree from the Random Forest - top 3 levels\")\n",
    "plt.show()\n",
    "\n",
    "print(export_text(one_tree, feature_names=feat, max_depth=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f54f4",
   "metadata": {},
   "source": [
    "### 8.3 Importance choices\n",
    "\n",
    "- `feature_importances_` is based on impurity reduction and can prefer variables with many possible splits.\n",
    "- `permutation_importance` measures performance drop when a feature is shuffled. Use it on a held out split.\n",
    "\n",
    "### 8.4 Partial dependence\n",
    "\n",
    "Use `PartialDependenceDisplay.from_estimator` on a fitted forest to show the average effect of a feature while marginalizing others. Good for monotonic trends and rough response shapes.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. In-class activities\n",
    "\n",
    "Each task is short and uses the chapter material. Fill in the `...` lines where shown.\n",
    "\n",
    "### Q1. Tree regression - sweep `min_samples_leaf`\n",
    "\n",
    "Use `DecisionTreeRegressor` on Melting Point with `min_samples_leaf` in `[1, 2, 5, 10, 20, 40]` and `max_depth=None`. Plot test R2 vs `min_samples_leaf` on a log-x scale.\n",
    "\n",
    "```python\n",
    "# Starter\n",
    "# values = [1, 2, 5, 10, 20, 40]\n",
    "# r2s = []\n",
    "# for v in values:\n",
    "#     m = DecisionTreeRegressor(min_samples_leaf=v, random_state=0).fit(X_tr, y_tr)\n",
    "#     r2s.append(r2_score(y_te, m.predict(X_te)))\n",
    "# plt.plot(values, r2s, \"o-\"); plt.xscale(\"log\")\n",
    "# plt.xlabel(\"min_samples_leaf\"); plt.ylabel(\"Test R2\"); plt.title(\"Leaf size sweep\")\n",
    "# plt.show()\n",
    "```\n",
    "\n",
    "### Q2. Tree classification - threshold tuning\n",
    "\n",
    "Train a depth 4 tree on toxicity. Scan thresholds from `0.2` to `0.8` in steps of `0.05`. Find the smallest threshold with **recall â‰¥ 0.80** and report the corresponding **precision** and **F1**.\n",
    "\n",
    "```python\n",
    "# Starter\n",
    "# clf = DecisionTreeClassifier(max_depth=4, random_state=0).fit(Xc_tr, yc_tr)\n",
    "# proba = clf.predict_proba(Xc_te)[:,1]\n",
    "# ths = np.arange(0.20, 0.81, 0.05)\n",
    "# rec_list, prec_list, f1_list = [], [], []\n",
    "# best_t = None\n",
    "# for t in ths:\n",
    "#     pred_t = (proba >= t).astype(int)\n",
    "#     r = recall_score(yc_te, pred_t)\n",
    "#     p = precision_score(yc_te, pred_t, zero_division=0)\n",
    "#     f = f1_score(yc_te, pred_t, zero_division=0)\n",
    "#     rec_list.append(r); prec_list.append(p); f1_list.append(f)\n",
    "#     if best_t is None and r >= 0.80:\n",
    "#         best_t = t\n",
    "# print(\"First threshold with recall >= 0.80:\", best_t)\n",
    "```\n",
    "\n",
    "### Q3. Forest regression - n_estimators curve\n",
    "\n",
    "Train `RandomForestRegressor` with `n_estimators` in `[50, 100, 200, 300, 500]` and record test R2. Plot R2 vs `n_estimators`.\n",
    "\n",
    "```python\n",
    "# Starter\n",
    "# ns = [50, 100, 200, 300, 500]\n",
    "# r2s = []\n",
    "# for n in ns:\n",
    "#     m = RandomForestRegressor(n_estimators=n, random_state=0, n_jobs=-1).fit(X_tr, y_tr)\n",
    "#     r2s.append(r2_score(y_te, m.predict(X_te)))\n",
    "# plt.plot(ns, r2s, \"o-\"); plt.xlabel(\"n_estimators\"); plt.ylabel(\"Test R2\"); plt.title(\"R2 vs forest size\")\n",
    "# plt.show()\n",
    "```\n",
    "\n",
    "### Q4. Forest classification - permutation importance\n",
    "\n",
    "Train `RandomForestClassifier` with `n_estimators=300` on toxicity. Compute permutation importance on the test split with the `roc_auc` scorer and plot.\n",
    "\n",
    "```python\n",
    "# Starter\n",
    "# rfc = RandomForestClassifier(n_estimators=300, random_state=0, n_jobs=-1).fit(Xc_tr, yc_tr)\n",
    "# perm = permutation_importance(rfc, Xc_te, yc_te, scoring=\"roc_auc\", n_repeats=20, random_state=0)\n",
    "# pd.Series(perm.importances_mean, index=feat).sort_values().plot(kind=\"barh\")\n",
    "# plt.title(\"Permutation importance (AUC drop)\")\n",
    "# plt.show()\n",
    "```\n",
    "\n",
    "### Q5. End to end - small forest grid\n",
    "\n",
    "Use GridSearchCV to tune a small forest for Melting Point with:\n",
    "- `n_estimators`: `[200, 300]`\n",
    "- `max_depth`: `[None, 6, 10]`\n",
    "- `min_samples_leaf`: `[1, 2, 5]`\n",
    "- `max_features`: `[\"auto\", \"sqrt\"]`\n",
    "\n",
    "Report best params, CV mean R2, and test R2. Predict for three SMILES of your choice after computing descriptors.\n",
    "\n",
    "```python\n",
    "# Starter\n",
    "# param_grid = {...}\n",
    "# rf = RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    "# grid = GridSearchCV(rf, param_grid, cv=KFold(n_splits=4, shuffle=True, random_state=1), scoring=\"r2\", n_jobs=-1)\n",
    "# grid.fit(X_tr, y_tr)\n",
    "# best_rf = grid.best_estimator_\n",
    "# print(grid.best_params_, grid.best_score_)\n",
    "# print(\"Test R2:\", r2_score(y_te, best_rf.predict(X_te)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Solutions to in-class activities\n",
    "\n",
    "### Solution Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d643f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [1, 2, 5, 10, 20, 40]\n",
    "r2s = []\n",
    "for v in values:\n",
    "    m = DecisionTreeRegressor(min_samples_leaf=v, random_state=0).fit(X_tr, y_tr)\n",
    "    r2s.append(r2_score(y_te, m.predict(X_te)))\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(values, r2s, \"o-\"); plt.xscale(\"log\")\n",
    "plt.xlabel(\"min_samples_leaf\"); plt.ylabel(\"Test R2\"); plt.title(\"Leaf size sweep\")\n",
    "plt.grid(True, alpha=0.3); plt.show()\n",
    "pd.DataFrame({\"min_samples_leaf\": values, \"test_R2\": np.round(r2s,3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db82ba",
   "metadata": {},
   "source": [
    "### Solution Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=4, random_state=0).fit(Xc_tr, yc_tr)\n",
    "proba = clf.predict_proba(Xc_te)[:,1]\n",
    "ths = np.arange(0.20, 0.81, 0.05)\n",
    "rec_list, prec_list, f1_list = [], [], []\n",
    "best_t = None\n",
    "for t in ths:\n",
    "    pred_t = (proba >= t).astype(int)\n",
    "    r = recall_score(yc_te, pred_t)\n",
    "    p = precision_score(yc_te, pred_t, zero_division=0)\n",
    "    f = f1_score(yc_te, pred_t, zero_division=0)\n",
    "    rec_list.append(r); prec_list.append(p); f1_list.append(f)\n",
    "    if best_t is None and r >= 0.80:\n",
    "        best_t = t\n",
    "\n",
    "print(\"First threshold with recall >= 0.80:\", best_t)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(ths, rec_list, marker=\"o\", label=\"Recall\")\n",
    "plt.plot(ths, prec_list, marker=\"o\", label=\"Precision\")\n",
    "plt.plot(ths, f1_list, marker=\"o\", label=\"F1\")\n",
    "plt.xlabel(\"Threshold\"); plt.ylabel(\"Score\"); plt.title(\"Threshold tuning on toxicity (tree)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6a03f",
   "metadata": {},
   "source": [
    "### Solution Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [50, 100, 200, 300, 500]\n",
    "r2s = []\n",
    "for n in ns:\n",
    "    m = RandomForestRegressor(n_estimators=n, random_state=0, n_jobs=-1).fit(X_tr, y_tr)\n",
    "    r2s.append(r2_score(y_te, m.predict(X_te)))\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ns, r2s, \"o-\")\n",
    "plt.xlabel(\"n_estimators\"); plt.ylabel(\"Test R2\"); plt.title(\"R2 vs forest size\")\n",
    "plt.grid(True, alpha=0.3); plt.show()\n",
    "pd.DataFrame({\"n_estimators\": ns, \"test_R2\": np.round(r2s,3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05959bbf",
   "metadata": {},
   "source": [
    "### Solution Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08dacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_sol = RandomForestClassifier(n_estimators=300, random_state=0, n_jobs=-1).fit(Xc_tr, yc_tr)\n",
    "perm = permutation_importance(rfc_sol, Xc_te, yc_te, scoring=\"roc_auc\", n_repeats=20, random_state=0)\n",
    "pd.Series(perm.importances_mean, index=feat).sort_values().plot(kind=\"barh\", figsize=(5,3))\n",
    "plt.title(\"Permutation importance (AUC drop)\"); plt.xlabel(\"Mean decrease in AUC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2a7a1",
   "metadata": {},
   "source": [
    "### Solution Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300],\n",
    "    \"max_depth\": [None, 6, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=0, n_jobs=-1)\n",
    "grid = GridSearchCV(rf, param_grid, cv=KFold(n_splits=4, shuffle=True, random_state=1), scoring=\"r2\", n_jobs=-1)\n",
    "grid.fit(X_tr, y_tr)\n",
    "\n",
    "best_rf_final = grid.best_estimator_\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"CV mean R2:\", grid.best_score_.round(3))\n",
    "print(\"Test R2:\", r2_score(y_te, best_rf_final.predict(X_te)).round(3))\n",
    "\n",
    "# Predict three sample SMILES if RDKit is available\n",
    "smiles_three = [\"C(F)(F)(F)CC=CCO\", \"C1CCCC(COC)C1\", \"CC(CBr)CCl\"]\n",
    "if Chem is not None:\n",
    "    desc = pd.DataFrame([calc_desc(s) for s in smiles_three])[feat]\n",
    "    preds = best_rf_final.predict(desc)\n",
    "    print(pd.DataFrame({\"SMILES\": smiles_three, \"Predicted MP\": preds.round(1)}))\n",
    "else:\n",
    "    print(\"RDKit not available. Skipping SMILES prediction.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "source_map": [
   12,
   23,
   53,
   57,
   81,
   119,
   125,
   129,
   138,
   145,
   149,
   165,
   169,
   183,
   191,
   202,
   206,
   217,
   234,
   246,
   268,
   276,
   282,
   301,
   315,
   328,
   342,
   352,
   358,
   362,
   378,
   387,
   391,
   405,
   421,
   425,
   441,
   445,
   460,
   464,
   481,
   491,
   497,
   501,
   509,
   619,
   630,
   634,
   656,
   660,
   671,
   675,
   681,
   685
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}