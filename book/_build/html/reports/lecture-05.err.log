Traceback (most recent call last):
  File "C:\Users\52377\AppData\Local\Programs\Python\Python313\Lib\site-packages\jupyter_core\utils\__init__.py", line 154, in wrapped
    asyncio.get_running_loop()
    ~~~~~~~~~~~~~~~~~~~~~~~~^^
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\52377\AppData\Local\Programs\Python\Python313\Lib\site-packages\jupyter_cache\executors\utils.py", line 58, in single_nb_execution
    executenb(
    ~~~~~~~~~^
        nb,
        ^^^
    ...<4 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\52377\AppData\Local\Programs\Python\Python313\Lib\site-packages\nbclient\client.py", line 1319, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\52377\AppData\Local\Programs\Python\Python313\Lib\site-packages\jupyter_core\utils\__init__.py", line 158, in wrapped
    return loop.run_until_complete(inner)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
  File "C:\Users\52377\AppData\Local\Programs\Python\Python313\Lib\asyncio\base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "C:\Users\52377\AppData\Local\Programs\Python\Python313\Lib\site-packages\nbclient\client.py", line 709, in async_execute
    await self.async_execute_cell(
        cell, index, execution_count=self.code_cells_executed + 1
    )
  File "C:\Users\52377\AppData\Local\Programs\Python\Python313\Lib\site-packages\nbclient\client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "C:\Users\52377\AppData\Local\Programs\Python\Python313\Lib\site-packages\nbclient\client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# Starter
ths = np.arange(0.20, 0.81, 0.05)
rec_list, prec_list, f1_list = [], [], []
best_t = None

for t in ths:
    pred_t = (y_proba >= t).astype(int)
    r = recall_score(y_test, pred_t)
    p = precision_score(y_test, pred_t, zero_division=0)
    f = f1_score(y_test, pred_t, zero_division=0)
    rec_list.append(r); prec_list.append(p); f1_list.append(f)
    if best_t is None and r >= 0.80:
        best_t = t

print("First threshold with recall >= 0.80:", best_t)

plt.figure(figsize=(7,5))
plt.plot(ths, rec_list, marker="o", label="Recall")
plt.plot(ths, prec_list, marker="o", label="Precision")
plt.plot(ths, f1_list, marker="o", label="F1")
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Threshold tuning on toxicity")
plt.legend()
plt.grid(True)
plt.show()
------------------


[31m---------------------------------------------------------------------------[39m
[31mValueError[39m                                Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[30][39m[32m, line 8[39m
[32m      6[39m [38;5;28;01mfor[39;00m t [38;5;129;01min[39;00m ths:
[32m      7[39m     pred_t = (y_proba >= t).astype([38;5;28mint[39m)
[32m----> [39m[32m8[39m     r = [43mrecall_score[49m[43m([49m[43my_test[49m[43m,[49m[43m [49m[43mpred_t[49m[43m)[49m
[32m      9[39m     p = precision_score(y_test, pred_t, zero_division=[32m0[39m)
[32m     10[39m     f = f1_score(y_test, pred_t, zero_division=[32m0[39m)

[36mFile [39m[32m~\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\utils\_param_validation.py:218[39m, in [36mvalidate_params.<locals>.decorator.<locals>.wrapper[39m[34m(*args, **kwargs)[39m
[32m    212[39m [38;5;28;01mtry[39;00m:
[32m    213[39m     [38;5;28;01mwith[39;00m config_context(
[32m    214[39m         skip_parameter_validation=(
[32m    215[39m             prefer_skip_nested_validation [38;5;129;01mor[39;00m global_skip_validation
[32m    216[39m         )
[32m    217[39m     ):
[32m--> [39m[32m218[39m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m    219[39m [38;5;28;01mexcept[39;00m InvalidParameterError [38;5;28;01mas[39;00m e:
[32m    220[39m     [38;5;66;03m# When the function is just a wrapper around an estimator, we allow[39;00m
[32m    221[39m     [38;5;66;03m# the function to delegate validation to the estimator, but we replace[39;00m
[32m    222[39m     [38;5;66;03m# the name of the estimator by the name of the function in the error[39;00m
[32m    223[39m     [38;5;66;03m# message to avoid confusion.[39;00m
[32m    224[39m     msg = re.sub(
[32m    225[39m         [33mr[39m[33m"[39m[33mparameter of [39m[33m\[39m[33mw+ must be[39m[33m"[39m,
[32m    226[39m         [33mf[39m[33m"[39m[33mparameter of [39m[38;5;132;01m{[39;00mfunc.[34m__qualname__[39m[38;5;132;01m}[39;00m[33m must be[39m[33m"[39m,
[32m    227[39m         [38;5;28mstr[39m(e),
[32m    228[39m     )

[36mFile [39m[32m~\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\metrics\_classification.py:2706[39m, in [36mrecall_score[39m[34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)[39m
[32m   2537[39m [38;5;129m@validate_params[39m(
[32m   2538[39m     {
[32m   2539[39m         [33m"[39m[33my_true[39m[33m"[39m: [[33m"[39m[33marray-like[39m[33m"[39m, [33m"[39m[33msparse matrix[39m[33m"[39m],
[32m   (...)[39m[32m   2564[39m     zero_division=[33m"[39m[33mwarn[39m[33m"[39m,
[32m   2565[39m ):
[32m   2566[39m [38;5;250m    [39m[33;03m"""Compute the recall.[39;00m
[32m   2567[39m 
[32m   2568[39m [33;03m    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of[39;00m
[32m   (...)[39m[32m   2704[39m [33;03m    array([1. , 1. , 0.5])[39;00m
[32m   2705[39m [33;03m    """[39;00m
[32m-> [39m[32m2706[39m     _, r, _, _ = [43mprecision_recall_fscore_support[49m[43m([49m
[32m   2707[39m [43m        [49m[43my_true[49m[43m,[49m
[32m   2708[39m [43m        [49m[43my_pred[49m[43m,[49m
[32m   2709[39m [43m        [49m[43mlabels[49m[43m=[49m[43mlabels[49m[43m,[49m
[32m   2710[39m [43m        [49m[43mpos_label[49m[43m=[49m[43mpos_label[49m[43m,[49m
[32m   2711[39m [43m        [49m[43maverage[49m[43m=[49m[43maverage[49m[43m,[49m
[32m   2712[39m [43m        [49m[43mwarn_for[49m[43m=[49m[43m([49m[33;43m"[39;49m[33;43mrecall[39;49m[33;43m"[39;49m[43m,[49m[43m)[49m[43m,[49m
[32m   2713[39m [43m        [49m[43msample_weight[49m[43m=[49m[43msample_weight[49m[43m,[49m
[32m   2714[39m [43m        [49m[43mzero_division[49m[43m=[49m[43mzero_division[49m[43m,[49m
[32m   2715[39m [43m    [49m[43m)[49m
[32m   2716[39m     [38;5;28;01mreturn[39;00m r

[36mFile [39m[32m~\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\utils\_param_validation.py:191[39m, in [36mvalidate_params.<locals>.decorator.<locals>.wrapper[39m[34m(*args, **kwargs)[39m
[32m    189[39m global_skip_validation = get_config()[[33m"[39m[33mskip_parameter_validation[39m[33m"[39m]
[32m    190[39m [38;5;28;01mif[39;00m global_skip_validation:
[32m--> [39m[32m191[39m     [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
[32m    193[39m func_sig = signature(func)
[32m    195[39m [38;5;66;03m# Map *args/**kwargs to the function signature[39;00m

[36mFile [39m[32m~\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\metrics\_classification.py:1996[39m, in [36mprecision_recall_fscore_support[39m[34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)[39m
[32m   1827[39m [38;5;250m[39m[33;03m"""Compute precision, recall, F-measure and support for each class.[39;00m
[32m   1828[39m 
[32m   1829[39m [33;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of[39;00m
[32m   (...)[39m[32m   1993[39m [33;03m array([2, 2, 2]))[39;00m
[32m   1994[39m [33;03m"""[39;00m
[32m   1995[39m _check_zero_division(zero_division)
[32m-> [39m[32m1996[39m labels = [43m_check_set_wise_labels[49m[43m([49m[43my_true[49m[43m,[49m[43m [49m[43my_pred[49m[43m,[49m[43m [49m[43maverage[49m[43m,[49m[43m [49m[43mlabels[49m[43m,[49m[43m [49m[43mpos_label[49m[43m)[49m
[32m   1998[39m [38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###[39;00m
[32m   1999[39m samplewise = average == [33m"[39m[33msamples[39m[33m"[39m

[36mFile [39m[32m~\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\metrics\_classification.py:1762[39m, in [36m_check_set_wise_labels[39m[34m(y_true, y_pred, average, labels, pos_label)[39m
[32m   1759[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([33m"[39m[33maverage has to be one of [39m[33m"[39m + [38;5;28mstr[39m(average_options))
[32m   1761[39m y_true, y_pred = attach_unique(y_true, y_pred)
[32m-> [39m[32m1762[39m y_type, y_true, y_pred = [43m_check_targets[49m[43m([49m[43my_true[49m[43m,[49m[43m [49m[43my_pred[49m[43m)[49m
[32m   1763[39m [38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str[39;00m
[32m   1764[39m [38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784[39;00m
[32m   1765[39m present_labels = _tolist(unique_labels(y_true, y_pred))

[36mFile [39m[32m~\AppData\Local\Programs\Python\Python313\Lib\site-packages\sklearn\metrics\_classification.py:106[39m, in [36m_check_targets[39m[34m(y_true, y_pred)[39m
[32m    103[39m     y_type = {[33m"[39m[33mmulticlass[39m[33m"[39m}
[32m    105[39m [38;5;28;01mif[39;00m [38;5;28mlen[39m(y_type) > [32m1[39m:
[32m--> [39m[32m106[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[32m    107[39m         [33m"[39m[33mClassification metrics can[39m[33m'[39m[33mt handle a mix of [39m[38;5;132;01m{0}[39;00m[33m and [39m[38;5;132;01m{1}[39;00m[33m targets[39m[33m"[39m.format(
[32m    108[39m             type_true, type_pred
[32m    109[39m         )
[32m    110[39m     )
[32m    112[39m [38;5;66;03m# We can't have more than one value on y_type => The set is no more needed[39;00m
[32m    113[39m y_type = y_type.pop()

[31mValueError[39m: Classification metrics can't handle a mix of continuous and multilabel-indicator targets

